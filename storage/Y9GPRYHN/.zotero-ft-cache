
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:1704.02685

Help | Advanced Search
Search
Computer Science > Computer Vision and Pattern Recognition
(cs)
[Submitted on 10 Apr 2017 ( v1 ), last revised 12 Oct 2019 (this version, v2)]
Title: Learning Important Features Through Propagating Activation Differences
Authors: Avanti Shrikumar , Peyton Greenside , Anshul Kundaje
Download a PDF of the paper titled Learning Important Features Through Propagating Activation Differences, by Avanti Shrikumar and 2 other authors
Download PDF

    Abstract: The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: this http URL , ICML slides: this http URL , ICML talk: this https URL , code: this http URL . 

Comments: 	Updated to include changes present in the ICML camera-ready paper, and other small corrections
Subjects: 	Computer Vision and Pattern Recognition (cs.CV) ; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)
Cite as: 	arXiv:1704.02685 [cs.CV]
  	(or arXiv:1704.02685v2 [cs.CV] for this version)
  	https://doi.org/10.48550/arXiv.1704.02685
Focus to learn more
arXiv-issued DOI via DataCite
Journal reference: 	PMLR 70:3145-3153, 2017
Submission history
From: Avanti Shrikumar [ view email ]
[v1] Mon, 10 Apr 2017 02:23:57 UTC (1,205 KB)
[v2] Sat, 12 Oct 2019 22:13:28 UTC (1,211 KB)
Full-text links:
Access Paper:

    Download a PDF of the paper titled Learning Important Features Through Propagating Activation Differences, by Avanti Shrikumar and 2 other authors
    Download PDF
    PostScript
    Other Formats 

( view license )
Current browse context:
cs.CV
< prev   |   next >
new | recent | 1704
Change to browse by:
cs
cs.LG
cs.NE
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

5 blog links
( what is this? )
DBLP - CS Bibliography
listing | bibtex
Avanti Shrikumar
Peyton Greenside
Anshul Kundaje
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

