1
Scientific Data | (2025) 12:117 | https://doi.org/10.1038/s41597-024-04306-9
www.nature.com/scientificdata
a multi-modal dental dataset for
semi-supervised deep learning
image segmentation
Yaqi Wang 1,2, FanYe3,Yifei Chen4, Chengkai Wang 5, Chengyu Wu6, Feng Xu3, Zhean Ma3,
Yi Liu7,Yifan Zhang8,9,10,11, Mingguo Cao8 ✉& Xiaodiao Chen3 ✉
In response to the increasing prevalence of dental diseases, dental health, a vital aspect of human well-being, warrants greater attention. Panoramic X-ray images (PXI) and Cone Beam Computed Tomography (CBCT) are key tools for dentists in diagnosing and treating dental conditions. Additionally, deep learning for tooth segmentation can focus on relevant treatment information and localize lesions. However, the scarcity of publicly available PXI and CBCT datasets hampers their use in tooth segmentation tasks. Therefore, this paper presents a multimodal dataset for Semi-supervised Tooth Segmentation (STS-Tooth) in dental PXI and CBCT, named STS-2D-Tooth and STS-3D-Tooth. STS-2D-Tooth includes 4,000 images and 900 masks, categorized by age into children and adults. Moreover, we have collected CBCTs providing more detailed and three-dimensional information, resulting in the STS-3D-Tooth dataset comprising 148,400 unlabeled scans and 8,800 masks. To our knowledge, this is the first multimodal dataset combining dental PXI and CBCT, and it is the largest tooth segmentation dataset, a significant step forward for the advancement of tooth segmentation.
Background & Summary
Oral diseases are significant global public health challenges that demand immediate action1. According to the World Health Organization, 3.58 billion people suffer from severe periodontal disease, while dental caries affect
two-thirds of the global population2. As of May 2015, up to 57 systemic diseases have been assessed as potentially
linked to periodontitis, with the most related being cardiovascular diseases, diabetes, and respiratory illnesses3. These conditions severely impact individual quality of life and exert considerable strain on global healthcare systems. Dentists face increasing workloads due to the rising number of patients and affected teeth, challenging current work practices. The traditional method of assessing oral health relies on clinical examinations that focus
on visible signs of current oral diseases and treatment outcomes4. However, relying solely on clinical examinations can be time-consuming and may lead to diagnostic inaccuracies due to observational limitations. Fortunately, the widespread use of Panoramic X-ray Images (PXIs) and Cone Beam Computed Tomography (CBCT) in dentistry has mitigated these issues to some extent. Dental radiological computer-aided detection and diagnosis serve several purposes, such as detecting radiopaque lesions in the maxillary sinus, calcification in the carotid artery, and morphological changes in the mandibular
cortical bone indicative of possible osteoporosis5. PXI offers a comprehensive view of the oral cavity, assisting in detecting issues like impacted teeth, skeletal anomalies, and cysts (see Fig. 1). However, PXI, while providing a complete view of all teeth, also includes irrelevant information, such as the upper and lower jawbones, the
1College of Media Engineering, Communication University of Zhejiang, Hangzhou, 310018, China. 2innovation center for Electronic Design Automation Technology, Hangzhou Dianzi University, Hangzhou, 310018, China. 3School of Computer Science, Hangzhou Dianzi University, Hangzhou 310018, China. 4HDU-ITMO Joint Institute, Hangzhou Dianzi University, Hangzhou, 310018, China. 5School of Management, Hangzhou Dianzi University, Hangzhou, 310018, China. 6Department of Mechanical, electrical and information engineering, Shandong University, Weihai, 264200, China. 7Department of Stomatology, Sichuan Provincial People’s Hospital, University of electronic Science and technology of china, chengdu, china. 8Department of Medicine, Lishui University, Lishui, 323000, China. 9State Key Laboratory of Oral Diseases, National Clinical Research Center for Oral Diseases, West China Hospital of Stomatology, Sichuan University, Chengdu, 610041, China. 10Hangzhou Geriatric Stomatology Hospital, Hangzhou Dental Hospital Group, Hangzhou, china. 11Division of Advanced Prosthetic Dentistry, tohoku University Graduate
School of Dentistry, Sendai, 9808575, Japan. . ✉e-mail: cmg@lsu.edu.cn; xiaodiao@hdu.edu.cn
DATA DeSCrIPTor
oPeN


2
Scientific Data | (2025) 12:117 | https://doi.org/10.1038/s41597-024-04306-9
www.nature.com/scientificdata
www.nature.com/scientificdata/
temporomandibular joint, and parts of the nasal cavity and sinuses. Moreover, as a 2D image, PXI lacks the detail and resolution of 3D CBCT and cannot provide complete three-dimensional information. On the other hand, CBCT provides clear, undistorted 3D images, is highly accurate for complex case treatment planning, and is applicable across various dental specialties (see Figs. 2, 3, showcasing CBCT datasets). Despite CBCT’s superior imaging capabilities, it has higher time costs, radiation exposure, and operational complexity compared to PXI. Therefore, combining both can provide doctors with more detailed and comprehensive patient information from different perspectives. In addition to the wide application of dental radiological CAD, the importance of deep learning methods in the medical field is undeniable. However, current public dental datasets, both PXI and CBCT, are limited in number and annotation information. This is due to constraints in dataset acquisition: (1) The extensive workload in annotation, both in precision and quantity, for example, a CBCT containing 200–500 scans, each requiring an average of 3–8 minutes; (2) Difficulties in accessing medical data; (3) The high cost of hiring dental experts for annotation. Thus, in dental image analysis tasks, semi-supervised learning plays a crucial role, especially when
Fig. 1 Concise display of children and adult dental PXI in the STS-2D-Tooth. It illustrates that the PXI offers a comprehensive view of the oral cavity, yet it also includes an abundance of redundant and interfering information.
Fig. 2 Components of the STS-3D-Tooth. This figure displays axial slices from two categories within the dataset: the extracted Region of Interest (ROI) and the complete scans. One CT scan from each category is selected to showcase their axial slices.


3
Scientific Data | (2025) 12:117 | https://doi.org/10.1038/s41597-024-04306-9
www.nature.com/scientificdata
www.nature.com/scientificdata/
dealing with large amounts of unlabeled data. By leveraging semi-supervised learning approaches, researchers can significantly reduce the dependency on fully annotated datasets, thereby alleviating the burden of manual annotation. This is particularly effective when handling large-scale datasets, such as typical CBCT scans, where the labor-intensive nature of annotation poses a major challenge. Additionally, we note the absence of a benchmark in the dental field to provide baseline tests for various tooth segmentation tasks. Considering all these factors, establishing a multimodal tooth segmentation dataset with pixel-level annotations is meaningful. In this paper, we introduce a Semi-supervised Tooth Segmentation (STS-Tooth) dataset composed of 4,000 PXIs and 148,400 CBCT scans. For 9,700 of these images, we provide pixel-level segmentation annotations. Our main contributions can be summarized as follows:
• Image: The dataset includes 4,000 two-dimensional dental PXIs (STS-2D-Tooth) and 148,400 CBCT scans (STS-3D-Tooth), with 3,500 adult and 500 child dental PXIs in STS-2D-Tooth. • Annotation: We have pixel-level annotated 900 PXIs and 8,800 CBCT scans. The initial work and annotations were completed by 20 trained dental practitioners and verified by 6 experienced dentists, taking a year to complete.
We believe the release of our private STS-Tooth dataset will serve as an important benchmark for tooth image segmentation in the deep learning field, significantly promoting tooth-related research in deep learning and facilitating the transition from technology to clinical application.
Methods
The development of STS-Tooth primarily consisted of five stages: data collection, data preprocessing, data filtering, data annotation, and dataset organization. The main stages in the workflow are depicted in Fig. 4. The detailed descriptions of each stage in the workflow are as follows:
Data collection. For STS-2D-Tooth dataset, a portion of the dataset was sourced from our previous work6. The 2 d dataset was divided into adult teeth PXI (A-PXI) and children’s teeth PXI (C-PXI). For the previous work, we included a total of 2,892 2 d PXIs of children (193 cases) and adults (2692 cases). In this work, we further expanded the number of 2 d PXIs to 4315 cases (including 569 children and 3746 adults). The expanded parts
Fig. 3 Visualization of selected points on axial slices in ITK-SNAP. This figure illustrates the importation of .nii. gz files into ITK-SNAP, focusing on the 99th axial slice from Fig. 2’s Region of Interest (ROI) and the 180th axial slice from Integrity. By altering the viewing perspective, corresponding sagittal planes are derived.


4
Scientific Data | (2025) 12:117 | https://doi.org/10.1038/s41597-024-04306-9
www.nature.com/scientificdata
www.nature.com/scientificdata/
are gathered from Hangzhou Dental Hosptial and Hangzhou Qiantang Dental Hospital, and Sichuan Provincial People’s Hospital collected from January 2020 to December 2022. These images were from 4,192 patients, ranging in age from 4 to 92 years. We classified the data into two categories: adults and children, followed by further processing. In Fig. 5, we showcase A-PXI images with different imaging effects. The grayscale value differences between the background and the teeth in these images vary, which helps to accommodate various imaging effects, thereby enhancing the algorithm’s universality.
For the STS-3D-Tooth dataset, we initially collected a total of 31,380 CBCT scans in our previous work7. In this study, we significantly expanded the dataset to 168,800 CBCT cases. These scans were obtained from 422 patients, aged 10 to 82 years, at Hangzhou Dental Hospital, and Hangzhou Qiantang Dental Hospital, and Sichuan Provincial People’s Hospital. The collection spanned from January 2020 to December 2022, with each scan having an axial resolution of 640 × 640 pixels and a slice thickness ranging from 0.25 mm to 0.3 mm. The PXI and CBCT were obtained using a HiRes3D-Plus device produced by Changzhou Boneng Zhongding Medical Technology, and the data were collected in DICOM format. All PXI and CBCT scans were performed prior to dental surgeries. The data collected from the hospitals were acquired after each participant consented and signed an informed consent form for non-commercial academic communication usage. Participants were informed prior to their treatment and data collection that their de-identified medical data might be used for non-commercial
Fig. 5 Illustration of various image features in the STS-2D-Tooth. The figure demonstrates that the edges of the teeth in the image (d) are more distinct, while images (a–c) show relatively smaller differences in grayscale values between the tooth regions and the background areas.
Fig. 4 Workflow diagram. This diagram provides an overview of the dataset processing procedure, including Data Collection, Data Filtering, Data Annotation, Data Preprocessing, and Dataset Organization.


5
Scientific Data | (2025) 12:117 | https://doi.org/10.1038/s41597-024-04306-9
www.nature.com/scientificdata
www.nature.com/scientificdata/
academic communication, which includes the potential open sharing of anonymized data during such academic exchanges. The ethics committee has thoroughly reviewed the dataset, and the approval for its publication under the license of CC-BY has been acquired. The study adhered to the principles of the Declaration of Helsinki and the data were carefully reviewed and approved by the Medical Ethics Committees of Sichuan Provincial People’s Hospital (Approval ID: 20220484) and Lishui College School of Medicine (Approval ID: 2022YR014). Furthermore, all procedures were conducted strictly in accordance with ethical guidelines to ensure the protection of participants’ privacy and confidentiality throughout the research process.
Data preprocessing. Converting DICOM to PNG format can enhance the compatibility and accessibility of medical images, facilitating viewing and sharing in non-specialized environments while protecting patient privacy. Therefore, we have converted the collected PXI data from DICOM to PNG format to streamline their utilization and broaden their applicability across different domains, such as presentations, educational materials, and collaborative research projects. To preserve the associated three-dimensional information and metadata, CBCT DICOM format data are converted into.nii.gz files. Specifically, a DICOM sequence is first read, with each patient corresponding to one sequence containing 300 to 400.dcm files. Prior to processing, the.dcm files in the folder are carefully checked to ensure they are correctly ordered, as this directly impacts the accuracy of the reconstruction. Once verified, the 2D DICOM slices are combined to create a 3D NIFTI image, which is then compressed and saved as a .nii.gz file for efficient storage and analysis. It is noteworthy that a DICOM file generally consists of a DICOM file header and a DICOM dataset. The DICOM file header contains various privacy information of the patient at the time of imaging, such as the patient’s name, date of birth, phone number, gender, age, etc. Therefore, we removed the file header, retaining only the image data, to ensure that the patient’s privacy information is not disclosed. Additionally, for CBCT, we changed the original axial resolution of 400 × 640 × 640 pixels to 400 × 512 × 512 pixels, and for PXI, we changed the original resolution of 2800 × 1536 pixels to 640 × 320 pixels. For 2D PXI, the key dental structures remain visible even with a lower resolution, while for 3D CBCT, reducing the resolution simplifies processing without losing essential diagnostic information. By modifying the resolution, the efficiency of expert data annotation work has been greatly improved.
Data filtering. For the collected data, we conducted a filtering process to ensure the quality and applicability of the data. The filtering mainly considered the following situations: (1) Invalid or incomplete data; (2) Duplicate data, which might be due to system errors or entry mistakes causing the same information to be recorded multiple times; (3) Outliers, potentially caused by equipment failure, entry errors, or unusual patient conditions; (4) Incomplete elimination of privacy information; (5) Poor quality data. For types (1) to (4) of data, they were directly excluded from our use. Meanwhile, to minimize the bias caused by subjective interpretation in filtering the fifth type of data, we introduced the image quality scoring criteria
(IQSC)8. A total of 8 experts used IQSC8 to subjectively assess data quality. Each image was given a score within the range of 0–4. Here, 0 indicates the lack of required dental structures or unobservable features, 3 indicates that the image quality is acceptable, and 4 indicates that the image quality is above the required level. After screening, 8.53% of the images scored below 3, and this portion of the data was also excluded. Ultimately, we discarded 131 PXI and 11,600 scans for reasons (1) to (4), and 184 PXIs and 8,400 scans for reason (5). The specific changes in the number of data are shown in Table 1.
Data annotation. The dataset labeling work was carried out by 20 dental experts. To improve the efficiency of annotation work, we adopted a mixed annotation method of manual annotation followed by semi-automatic annotation.
• STS-2D-Tooth Annotation: We carried out the annotation in two stages, the first stage being completely manual annotation, and the second stage being iterative semi-automatic annotation. When annotating PXI,
we first manually annotated 300 images using two professional image annotation software, EISeg9 (https://
github.com/PaddleCV-SIG/EISeg) and LabelMe10 (https://github.com/wkentaro/labelme). Then, we used
this data to train a network suitable for dental segmentation tasks on R2 U-Net11 (https://github.com/LeeJunHyun/Image_Segmentation). Afterward, we input the obtained parameters into the network and made predictions on the data collected this time. The prediction results were then manually modified by experts. Specifically, we first limited the maximum pixel value of the original image to 250, then converted the predicted mask into an RGB format, and superimposed it on the original image with RGB [255, 0, 0]. Next, we used CLIP STUDIO PAINT (https://www.clipstudio.net) to modify the images with the overlaid mask. For areas belonging to the dental region but not predicted, we used tools like brushes and paint buckets to assign the area with [255, 0, 0]; for areas not belonging to the dental region, we assigned [0, 0, 0]. After manual
Original number
Filtered number (1) (2) (3) (4) (5) Final number STS-2D-Tooth C-PXI 569 13 16 15 11 36 500
A-PXI 3,746 29 32 19 18 148 3,500 STS-3D-Tooth — 168,800 2,400 3,200 2,800 3,200 8,400 148,400
Table 1. Changes in images quantity during the filtering Process.


6
Scientific Data | (2025) 12:117 | https://doi.org/10.1038/s41597-024-04306-9
www.nature.com/scientificdata
www.nature.com/scientificdata/
modification, we used code to extract the areas where the R channel of RGB is 255 to obtain the corresponding mask and then normalized it. • STS-3D-Tooth Annotation: We also divided the annotation into two stages. In the first stage, CTs were man
ually annotated in the professional 3D annotation software: ITK-SNAP12 (https://github.com/pyushkevich/ itksnap). We first used the software to delineate the dental area layer by layer in the axial view, then manually
fine-tuned the annotations in the coronal and sagittal views7. These data were then used as input for the second stage. In the second stage, the 3D data from the previous annotations were split axially, creating two-dimensional
axial slices, which were then input into R2 U-Net11 for semi-automatic operations similar to STS-2D-Tooth annotation. The difference is that we did not complete training and annotation at once but iteratively. After each round of training, the results were manually supplemented and corrected, and the revised data were continuously added as training material for further training. In the first stage, it took about 60 hours to complete the annotation and review of the dental area of one CT. In the second stage, after the first round of training, it took about 40 hours to complete the annotation of one CT. Thus, this iterative annotation method not only ensures that each CT is manually modified and annotated but also greatly improves the efficiency of the annotation work.
Dataset organization. The quantity and detailed feature information of the dataset are summarized in Table 2. Following the completion of data annotation, accurate and comprehensive masks were obtained for each image. For two-dimensional data, the masks were directly converted into corresponding binary images. For three-dimensional data, multiple two-dimensional axial slices were combined to create three-dimensional data, which were saved in the .nii.gz format. Furthermore, all datasets were organized to ensure they are readily usable by readers without requiring additional conversions or modifications, as illustrated in Fig. 1. The specific actions were as follows: For the STS-2D-Tooth dataset, all images and masks were categorized into adult and child groups, resulting in two sections: A-PXI and C-PXI. Each section included both annotated and unannotated data. Renaming, resolution unification, and channel count standardization were performed for each section to maintain consistency. For the STS-3D-Tooth dataset, Regions of Interest (ROIs) with annotated data were extracted, prioritizing the tooth area while retaining the original complete images for the unannotated parts. These processes are detailed in Figs. 2, 3.
Data records
The STS-Tooth dataset has been uploaded to Zenodo in a compressed file format13. Once decompressed, the folder reveals dental images along with their corresponding masks. The organization of the folder is depicted in Fig. 6. The decompressed files are organized into two main folders, named “STS-2D-Tooth” and “STS-3D-Tooth.” The “STS-2D-Tooth” folder includes two subfolders named “A-PXI” and “C-PXI,” while the “STS-3D-Tooth” folder contains two subfolders named “ROI” and “Integrity,” as shown in Fig. 6(a). These four subfolders adhere to the same naming and arrangement scheme, with the detailed organization viewable in Fig. 6(b). For instance, within the “A-PXI” folder, there are sections titled “Labeled” and “Unlabeled.” The “Labeled” section includes 850 images and 850 corresponding masks, and the “Unlabeled” section comprises 2650 images. The number of images contained within each subfolder can be found in Table 3.
technical Validation
Validation criteria. At present, there is no universally accepted standard for judging the quality of PXI and CBCT images. However, quality is often assessed based on several aspects, including clarity, contrast, exposure
level, noise level, artifacts, and dynamic range14. Therefore, considering the characteristics of PXI and CBCT, we measure image quality from the following three aspects:
• Contrast15: In PXI and CBCT, the grayscale values of the dental area and the background typically differ significantly. Therefore, we believe that images with higher contrast can better differentiate teeth, bone, and soft tissue, which is more conducive to analysis and application.
• Sharpness16: Clarity is crucial for diagnosis. The sharper the image, it is easier to identify details, especially the subtle differences in tooth structure and bone quality.
• Artifacts17: In PXI and CBCT, the presence of other skeletal parts with grayscale values similar to the teeth can create artifacts, potentially interfering with judgment and affecting diagnostic accuracy. Therefore, assessing the presence and severity of artifacts is important.
Number Unlabeled Labeled Total Resolution
STS-2D-Tooth
C-PXI 450 50 500 A-PXI 2,650 850 3,500 640 × 320 Total 3,100 900 4,000 STS-3D-Tooth — 135,600 12,800 148,400 400 × 512 × 512
Table 2. Quantitative Overview of the Dataset. This table details the dataset’s composition, with the STS-2DTooth categorized into ‘Children’ and ‘Adult’ based on different age. Additionally, the dataset encompasses both annotated and unannotated data.


7
Scientific Data | (2025) 12:117 | https://doi.org/10.1038/s41597-024-04306-9
www.nature.com/scientificdata
www.nature.com/scientificdata/
In addition to the images, we have also provided annotations for 900 PXIs and 8800 scans. We have evaluated the quality of these annotations, primarily using Pixel Accuracy and Boundary Accuracy to measure the
accuracy of the annotated results18. Pixel Accuracy broadly examines the accuracy of annotations, focusing on whether the dental areas in the images are completely segmented and whether there are segmentation errors. Boundary Accuracy looks at the accuracy of annotations from the perspective of edge details, checking whether the edges of the teeth are segmented with pixel-level precision. We refer to the above standards for assessing image and annotation quality as STS evaluation. This com
prehensive evaluation system for the dataset’s overall quality comprises five categories19, each worth 20 points, totaling a score out of 100, calculated on a percentage basis, as shown in Table 4.
Validation results. We conducted evaluations on both the STS-2D-Tooth and STS-3D-Tooth datasets using the STS-Evaluation system. We categorized all the datasets into three groups based on type and age: A-PXI and C-PXI under STS-2D-Tooth, and STS-3D-Tooth. To comprehensively assess the quality of the datasets, we considered randomly sampling data from each category for scoring. Specifically, 30% of the data was randomly selected from each category. This data was then scored by five dental experts, knowledgeable in image processing and specially trained for this task. The average of the total scores from all five experts constituted the score for that dataset category, as shown in Table 5. These scores demonstrate that the quality of the images we collected has been recognized by experts. Furthermore, it validates that our dataset is of high quality and suitable for further research applications.
Fig. 6 Organization of the STS-Tooth. This diagram illustrates the specific arrangement of the two phases of the dataset within its storage folders.
Unlabeled Labeled Total 1 2,650 850 3,500 2 450 50 500 Total 3,100 900 4,000 3 92,400 4,000 96,400 4 43,200 8,800 52,000 Total 135,600 12,800 148,400
Table 3. This table presents the distribution of the number of images within each blue box in part Fig. 6(a).
Score
Image Quality Assessment Annotation Quality Assessment Contrast Sharpness Artifacts Pixel Accuracy Boundary Accuracy 0–5 Unclear Unclear Many Unclear Unclear 6–10 Slightly Clear Slightly Clear Some Slightly Clear Slightly Clear 11–15 Mostly Clear Mostly Clear Few Mostly Clear Mostly Clear 16–20 Clear Clear None Clear Clear
Table 4. Assessment table of image and annotation quality scores.


8
Scientific Data | (2025) 12:117 | https://doi.org/10.1038/s41597-024-04306-9
www.nature.com/scientificdata
www.nature.com/scientificdata/
Code availability
There were no additional special codes used for the creation of this dataset; all tools and codes utilized are openly available as open-source. We ensures reproducibility for other researchers while adhering to the terms of the open-source licenses.
Received: 23 May 2024; Accepted: 16 December 2024; Published: xx xx xxxx
references
1. Winkelmann, J., Listl, S., van Ginneken, E., Vassallo, P. & Benzian, H. Universal health coverage cannot be universal without oral health. The Lancet Public Health 8, e8–e10 (2023). 2. Jain, N., Dutt, U., Radenkov, I. & Jain, S. Who’s global oral health status report 2022: Actions, discussion and implementation (2023). 3. Herrera, D., Serrano, J., Roldán, S., Alonso, B. & Sanz, M. Oral and systemic health: is there a “new” link with covid-19? Clinical Oral Investigations 27, 3–13, https://doi.org/10.1007/s00784-023-04950-2 (2023). 4. Yu, X., Chen, Y., Li, Y., Hong, J. & Hua, F. A bibliometric mapping study of the literature on oral health-related quality of life. Journal of Evidence-Based Dental Practice 23, 101780 (2023).
5. Katsumata, A. Deep learning and artificial intelligence in dental diagnostic imaging. Japanese Dental Science Review 59, 329–333, https://doi.org/10.1016/j.jdsr.2023.09.004 (2023). 6. Zhang, Y. et al. Children’s dental panoramic radiographs dataset for caries segmentation and dental disease detection. FigShare https://doi.org/10.6084/m9.figshare.c.6317013.v1 (2023). 7. Cui, W. et al. Ctooth+: A large-scale dental cone beam computed tomography dataset and benchmark for tooth volume segmentation. In MICCAI Workshop on Data Augmentation, Labelling, and Imperfections, 64–73 (Springer, 2022).
8. Padole, A. M. et al. Development and validation of image quality scoring criteria (iqsc) for pediatric ct: a preliminary study. Insights into Imaging 10, 95, https://doi.org/10.1186/s13244-019-0769-8 (2019). 9. Liu, Y. et al. Paddleseg: A high-efficient development toolkit for image segmentation (2021). 10. Russell, B. C., Torralba, A., Murphy, K. P. & Freeman, W. T. Labelme: A database and web-based tool for image annotation. International Journal of Computer Vision 77, 157–173, https://doi.org/10.1007/s11263-007-0090-8 (2008). 11. Alom, M. Z., Hasan, M., Yakopcic, C., Taha, T. M. & Asari, V. K. Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation. arXiv preprint arXiv:1802.06955 (2018). 12. Yushkevich, P. A. et al. User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability. Neuroimage 31, 1116–1128 (2006). 13. Wang, Y. et al. STS-Tooth: A multi-modal dental dataset for semi-supervised deep learning image segmentation. Zenodo https://doi.org/10.5281/zenodo.10597292 (2024). 14. Chandler, D. M. Seven challenges in image quality assessment: past, present, and future research. International Scholarly Research Notices 2013 (2013).
15. Gu, K., Zhai, G., Lin, W. & Liu, M. The analysis of image contrast: From quality assessment to automatic enhancement. IEEE transactions on cybernetics 46, 284–297 (2015).
16. Zhu, M., Yu, L., Wang, Z., Ke, Z. & Zhi, C. A survey on objective evaluation of image sharpness. Applied Sciences 13, 2652 (2023). 17. Schuler, A. Image artifacts and pitfalls. In Chest sonography, 187–196 (Springer, 2022). 18. Clinton, N. et al. Accuracy assessment measures for object-based image segmentation goodness. Photogramm. Eng. Remote Sens 76, 289–299 (2010). 19. Chow, L. S. & Paramesran, R. Review of medical image quality assessment. Biomedical signal processing and control 27, 145–154 (2016).
Acknowledgements
This research is supported in part by the National Science Fund for Distinguished Young Scholars(No. 62206242) and National Natural Science Foundation of China (No. 61972120).
author contributions
Yaqi Wang collected and processed the data from the institution, in addition to generally coordinating the work of all parties and proofreading and revising the article. Fan Ye was responsible for creating the dataset as well as organizing the language and writing the text part of the article. Yifei Chen, Chengkai Wang, and Chengyu Wu were responsible for revising the final version of the article. Zhe’an Ma and Feng Xu were responsible for designing and drawing the graphs and charts of the article. Yi Liu and Yifan Zhang were responsible for providing patient data as the hospital. Xiaodiao Chen provided important guidance for writing the article. Finally, all authors reviewed the manuscript.
Competing interests
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
additional information
Correspondence and requests for materials should be addressed to M.C. or X.C.
Image Quality Assessment Annotation Quality Assessment Contrast Sharpness Artifacts Pixel Accuracy Boundary Accuracy STS-2D-Tooth A-PXI 18.23 18.02 18.42 18.41 18.94
C-PXI 18.67 18.62 17.58 17.92 18.18 STS-3D-Tooth — 18.09 18.45 17.95 17.88 19.01
Table 5. Quality assessment score table for each category of dataset.


9
Scientific Data | (2025) 12:117 | https://doi.org/10.1038/s41597-024-04306-9
www.nature.com/scientificdata
www.nature.com/scientificdata/
Reprints and permissions information is available at www.nature.com/reprints.
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Open Access This article is licensed under a Creative Commons Attribution-NonCommercialNoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.
© The Author(s) 2025