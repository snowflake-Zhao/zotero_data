GLOBAL EDITION
Digital Image Processing
FOURTH EDITION
Rafael C. Gonzalez • Richard E. Woods
www.EBooksWorld.ir


Support Package for Digital
Image Processing
Your new textbook provides access to support packages that may include reviews in areas like probability and vectors, tutorials on topics relevant to the material in the book, an image database, and more. Refer to the Preface in the textbook for a detailed list of resources.
Follow the instructions below to register for the Companion Website for Rafael C. Gonzalez and Richard E. Woods’ Digital Image Processing, Fourth Edition, Global Edition.
1. Go to www.ImageProcessingPlace.com 2. Find the title of your textbook. 3. Click Support Materials and follow the on-screen instructions to create a login name and password.
Use the login name and password you created during registration to start using the digital resources that accompany your textbook.
IMPORTANT:
This serial code can only be used once. This subscription is not transferrable.
www.EBooksWorld.ir


Processing
igital Image
4
D
FOURTH EDITION
Rafael C. Gonzalez
University of Tennessee
Richard E. Woods
Interapptics
330 Hudson Street, New York, NY 10013
Global Edition
www.EBooksWorld.ir


Senior Vice President Courseware Portfolio Management: Marcia J. Horton Director, Portfolio Management: Engineering, Computer Science & Global Editions: Julian Partridge Portfolio Manager: Julie Bai Field Marketing Manager: Demetrius Hall Product Marketing Manager: Yvonne Vannatta Marketing Assistant: Jon Bryant Content Managing Producer, ECS and Math: Scott Disanno Content Producer: Michelle Bayman Project Manager: Rose Kernan Assistant Project Editor, Global Editions: Vikash Tiwari Operations Specialist: Maura Zaldivar-Garcia Manager, Rights and Permissions: Ben Ferrini Senior Manufacturing Controller, Global Editions: Trudy Kimber Media Production Manager, Global Editions: Vikram Kumar Cover Designer: Lumina Datamatics Cover Photo: CT image—© zhuravliki.123rf.com/Pearson Asset Library; Gram-negative bacteria—© royaltystockphoto.com/ Shutterstock.com; Orion Nebula—© creativemarc/Shutterstock.com; Fingerprints—© Larysa Ray/Shutterstock.com; Cancer cells—© Greenshoots Communications/Alamy Stock Photo
MATLAB is a registered trademark of The MathWorks, Inc., 1 Apple Hill Drive, Natick, MA 01760-2098.
Pearson Education Limited Edinburgh Gate Harlow Essex CM20 2JE England
and Associated Companies throughout the world
Visit us on the World Wide Web at: www.pearsonglobaleditions.com
© Pearson Education Limited 2018
The rights of Rafael C. Gonzalez and Richard E. Woods to be identified as the authors of this work have been asserted by them in accordance with the Copyright, Designs and Patents Act 1988.
Authorized adaptation from the United States edition, entitled Digital Image Processing, Fourth Edition, ISBN 978-0-13-335672-4, by Rafael C. Gonzalez and Richard E. Woods, published by Pearson Education © 2018.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, without either the prior written permission of the publisher or a license permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, Saffron House, 6–10 Kirby Street, London EC1N 8TS.
All trademarks used herein are the property of their respective owners. The use of any trademark in this text does not vest in the author or publisher any trademark ownership rights in such trademarks, nor does the use of such trademarks imply any affiliation with or endorsement of this book by such owners.
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library
10 9 8 7 6 5 4 3 2 1
ISBN 10: 1-292-22304-9 ISBN 13: 978-1-292-22304-9
Typeset by Richard E. Woods
Printed and bound in Malaysia
www.EBooksWorld.ir


To Connie, Ralph, and Rob and To Janice, David, and Jonathan
www.EBooksWorld.ir


This page intentionally left blank
www.EBooksWorld.ir


Contents
Preface 9
Acknowledgments 12
The Book Website 13
The DIP4E Support Packages 13
About the Authors 14
1 Introduction 17
What is Digital Image Processing? 18 The Origins of Digital Image Processing 19 Examples of Fields that Use Digital Image Processing 23 Fundamental Steps in Digital Image Processing 41 Components of an Image Processing System 44
2 Digital Image Fundamentals 47
Elements of Visual Perception 48 Light and the Electromagnetic Spectrum 54 Image Sensing and Acquisition 57 Image Sampling and Quantization 63 Some Basic Relationships Between Pixels 79 Introduction to the Basic Mathematical Tools Used in Digital Image Processing 83
3 Intensity Transformations and Spatial
Filtering 119
Background 120 Some Basic Intensity Transformation Functions 122 Histogram Processing 133 Fundamentals of Spatial Filtering 153 Smoothing (Lowpass) Spatial Filters 164 Sharpening (Highpass) Spatial Filters 175 Highpass, Bandreject, and Bandpass Filters from Lowpass Filters 188 Combining Spatial Enhancement Methods 191
www.EBooksWorld.ir


6 Contents
4 Filtering in the Frequency
Domain 203
Background 204 Preliminary Concepts 207 Sampling and the Fourier Transform of Sampled Functions 215 The Discrete Fourier Transform of One Variable 225 Extensions to Functions of Two Variables 230 Some Properties of the 2-D DFT and IDFT 240 The Basics of Filtering in the Frequency Domain 260 Image Smoothing Using Lowpass Frequency Domain Filters 272 Image Sharpening Using Highpass Filters 284 Selective Filtering 296 The Fast Fourier Transform 303
5 Image Restoration
and Reconstruction 317
A Model of the Image Degradation/Restoration process 318 Noise Models 318 Restoration in the Presence of Noise Only—Spatial Filtering 327 Periodic Noise Reduction Using Frequency Domain Filtering 340 Linear, Position-Invariant Degradations 348 Estimating the Degradation Function 352 Inverse Filtering 356 Minimum Mean Square Error (Wiener) Filtering 358 Constrained Least Squares Filtering 363 Geometric Mean Filter 367 Image Reconstruction from Projections 368
6 Color Image Processing 399
Color Fundamentals 400 Color Models 405 Pseudocolor Image Processing 420 Basics of Full-Color Image Processing 429 Color Transformations 430
www.EBooksWorld.ir


Contents 7
Color Image Smoothing and Sharpening 442 Using Color in Image Segmentation 445 Noise in Color Images 452 Color Image Compression 455
7 Wavelet and Other Image Transforms 463
Preliminaries 464 Matrix-based Transforms 466 Correlation 478 Basis Functions in the Time-Frequency Plane 479 Basis Images 483 Fourier-Related Transforms 484 Walsh-Hadamard Transforms 496 Slant Transform 500 Haar Transform 502 Wavelet Transforms 504
8 Image Compression and
Watermarking 539
Fundamentals 540 Huffman Coding 553 Golomb Coding 556 Arithmetic Coding 561 LZW Coding 564 Run-length Coding 566 Symbol-based Coding 572 Bit-plane Coding 575 Block Transform Coding 576 Predictive Coding 594 Wavelet Coding 614 Digital Image Watermarking 624
9 Morphological Image Processing 635
Preliminaries 636 Erosion and Dilation 638 Opening and Closing 644 The Hit-or-Miss Transform 648
www.EBooksWorld.ir


8 Contents
Some Basic Morphological Algorithms 652 Morphological Reconstruction 667 Summary of Morphological Operations on Binary Images 673 Grayscale Morphology 674
10 Image Segmentation 699
Fundamentals 700 Point, Line, and Edge Detection 701 Thresholding 742 Segmentation by Region Growing and by Region Splitting and Merging 764 Region Segmentation Using Clustering and Superpixels 770
Region Segmentation Using Graph Cuts 777 Segmentation Using Morphological Watersheds 786 The Use of Motion in Segmentation 796
11 Feature Extraction 811
Background 812 Boundary Preprocessing 814 Boundary Feature Descriptors 831 Region Feature Descriptors 840 Principal Components as Feature Descriptors 859 Whole-Image Features 868 Scale-Invariant Feature Transform (SIFT) 881
12 Image Pattern Classification 903
Background 904 Patterns and Pattern Classes 906 Pattern Classification by Prototype Matching 910 Optimum (Bayes) Statistical Classifiers 923 Neural Networks and Deep Learning 931 Deep Convolutional Neural Networks 964 Some Additional Details of Implementation 987
Bibliography 995
Index 1009
www.EBooksWorld.ir


Preface
When something can be read without effort, great effort has gone into its writing. Enrique Jardiel Poncela
This edition of Digital Image Processing is a major revision of the book. As in the 1977 and 1987 editions by Gonzalez and Wintz, and the 1992, 2002, and 2008 editions by Gonzalez and Woods, this sixth-generation edition was prepared with students and instructors in mind. The principal objectives of the book continue to be to provide an introduction to basic concepts and methodologies applicable to digital image processing, and to develop a foundation that can be used as the basis for further study and research in this field. To achieve these objectives, we focused again on material that we believe is fundamental and whose scope of application is not limited to the solution of specialized problems. The mathematical complexity of the book remains at a level well within the grasp of college seniors and first-year graduate students who have introductory preparation in mathematical analysis, vectors, matrices, probability, statistics, linear systems, and computer programming. The book website provides tutorials to support readers needing a review of this background material. One of the principal reasons this book has been the world leader in its field for 40 years is the level of attention we pay to the changing educational needs of our readers. The present edition is based on an extensive survey that involved faculty, students, and independent readers of the book in 150 institutions from 30 countries. The survey revealed a need for coverage of new material that has matured since the last edition of the book. The principal findings of the survey indicated a need for:
• Expanded coverage of the fundamentals of spatial filtering. • A more comprehensive and cohesive coverage of image transforms. • A more complete presentation of finite differences, with a focus on edge detection. • A discussion of clustering, superpixels, and their use in region segmentation. • Coverage of maximally stable extremal regions. • Expanded coverage of feature extraction to include the Scale Invariant Feature Transform (SIFT). • Expanded coverage of neural networks to include deep neural networks, backpropagation, deep learning, and, especially, deep convolutional neural networks. • More homework exercises at the end of the chapters.
The new and reorganized material that resulted in the present edition is our attempt at providing a reasonable balance between rigor, clarity of presentation, and the findings of the survey. In addition to new material, earlier portions of the text were updated and clarified. This edition contains 241 new images, 72 new drawings, and 135 new exercises.
www.EBooksWorld.ir


10 Preface
New to This Edition
The highlights of this edition are as follows.
Chapter 1: Some figures were updated, and parts of the text were rewritten to correspond to changes in later chapters.
Chapter 2: Many of the sections and examples were rewritten for clarity. We added 14 new exercises.
Chapter 3: Fundamental concepts of spatial filtering were rewritten to include a discussion on separable filter kernels, expanded coverage of the properties of lowpass Gaussian kernels, and expanded coverage of highpass, bandreject, and bandpass filters, including numerous new examples that illustrate their use. In addition to revisions in the text, including 6 new examples, the chapter has 59 new images, 2 new line drawings, and 15 new exercises.
Chapter 4: Several of the sections of this chapter were revised to improve the clarity of presentation. We replaced dated graphical material with 35 new images and 4 new line drawings. We added 21 new exercises.
Chapter 5: Revisions to this chapter were limited to clarifications and a few corrections in notation. We added 6 new images and 14 new exercises,
Chapter 6: Several sections were clarified, and the explanation of the CMY and CMYK color models was expanded, including 2 new images.
Chapter 7: This is a new chapter that brings together wavelets, several new transforms, and many of the image transforms that were scattered throughout the book. The emphasis of this new chapter is on the presentation of these transforms from a unified point of view. We added 24 new images, 20 new drawings, and 25 new exercises.
Chapter 8: The material was revised with numerous clarifications and several improvements to the presentation.
Chapter 9: Revisions of this chapter included a complete rewrite of several sections, including redrafting of several line drawings. We added 16 new exercises
Chapter 10: Several of the sections were rewritten for clarity. We updated the chapter by adding coverage of finite differences, K-means clustering, superpixels, and graph cuts. The new topics are illustrated with 4 new examples. In total, we added 29 new images, 3 new drawings, and 6 new exercises.
Chapter 11: The chapter was updated with numerous topics, beginning with a more detailed classification of feature types and their uses. In addition to improvements in the clarity of presentation, we added coverage of slope change codes, expanded the explanation of skeletons, medial axes, and the distance transform, and added several new basic descriptors of compactness, circularity, and eccentricity. New material includes coverage of the Harris-Stephens corner detector, and a presentation of maximally stable extremal regions. A major addition to the chapter is a comprehensive discussion dealing with the Scale-Invariant Feature Transform (SIFT). The new material is complemented by 65 new images, 15 new drawings, and 12 new exercises.
www.EBooksWorld.ir


Preface 11
Chapter 12: This chapter underwent a major revision to include an extensive rewrite of neural networks and deep learning, an area that has grown significantly since the last edition of the book. We added a comprehensive discussion on fully connected, deep neural networks that includes derivation of backpropagation starting from basic principles. The equations of backpropagation were expressed in “traditional” scalar terms, and then generalized into a compact set of matrix equations ideally suited for implementation of deep neural nets. The effectiveness of fully connected networks was demonstrated with several examples that included a comparison with the Bayes classifier. One of the most-requested topics in the survey was coverage of deep convolutional neural networks. We added an extensive section on this, following the same blueprint we used for deep, fully connected nets. That is, we derived the equations of backpropagation for convolutional nets, and showed how they are different from “traditional” backpropagation. We then illustrated the use of convolutional networks with simple images, and applied them to large image databases of numerals and natural scenes. The written material is complemented by 23 new images, 28 new drawings, and 12 new exercises. Also for the first time, we have created student and faculty support packages that can be downloaded from the book website. The Student Support Package contains many of the original images in the book and answers to selected exercises The Faculty Support Package contains solutions to all exercises, teaching suggestions, and all the art in the book in the form of modifiable PowerPoint slides. One support package is made available with every new book, free of charge. The book website, established during the launch of the 2002 edition, continues to be a success, attracting more than 25,000 visitors each month. The site was upgraded for the launch of this edition. For more details on site features and content, see The Book Website, following the Acknowledgments section.
This edition of Digital Image Processing is a reflection of how the educational needs of our readers have changed since 2008. As is usual in an endeavor such as this, progress in the field continues after work on the manuscript stops. One of the reasons why this book has been so well accepted since it first appeared in 1977 is its continued emphasis on fundamental concepts that retain their relevance over time. This approach, among other things, attempts to provide a measure of stability in a rapidly evolving body of knowledge. We have tried to follow the same principle in preparing this edition of the book.
R.C.G. R.E.W.
www.EBooksWorld.ir


12 Acknowledgments
Acknowledgments
We are indebted to a number of individuals in academic circles, industry, and government who have contributed to this edition of the book. In particular, we wish to extend our appreciation to Hairong Qi and her students, Zhifei Zhang and Chengcheng Li, for their valuable review of the material on neural networks, and for their help in generating examples for that material. We also want to thank Ernesto Bribiesca Correa for providing and reviewing material on slope chain codes, and Dirk Padfield for his many suggestions and review of several chapters in the book. We appreciate Michel Kocher’s many thoughtful comments and suggestions over the years on how to improve the book. Thanks also to Steve Eddins for his suggestions on MATLAB and related software issues. Numerous individuals have contributed to material carried over from the previous to the current edition of the book.Their contributions have been important in so many different ways that we find it difficult to acknowledge them in any other way but alphabetically. We thank Mongi A. Abidi, Yongmin Kim, Bryan Morse, Andrew Oldroyd, Ali M. Reza, Edgardo Felipe Riveron, Jose Ruiz Shulcloper, and Cameron H.G. Wright for their many suggestions on how to improve the presentation and/or the scope of coverage in the book. We are also indebted to Naomi Fernandes at the MathWorks for providing us with MATLAB software and support that were important in our ability to create many of the examples and experimental results included in this edition of the book. A significant percentage of the new images used in this edition (and in some cases their history and interpretation) were obtained through the efforts of individuals whose contributions are sincerely appreciated. In particular, we wish to acknowledge the efforts of Serge Beucher, Uwe Boos, Michael E. Casey, Michael W. Davidson, Susan L. Forsburg, Thomas R. Gest, Daniel A. Hammer, Zhong He, Roger Heady, Juan A. Herrera, John M. Hudak, Michael Hurwitz, Chris J. Johannsen, Rhonda Knighton, Don P. Mitchell, A. Morris, Curtis C. Ober, David. R. Pickens, Michael Robinson, Michael Shaffer, Pete Sites, Sally Stowe, Craig Watson, David K. Wehe, and Robert A. West. We also wish to acknowledge other individuals and organizations cited in the captions of numerous figures throughout the book for their permission to use that material. We also thank Scott Disanno, Michelle Bayman, Rose Kernan, and Julie Bai for their support and significant patience during the production of the book.
R.C.G. R.E.W.
www.EBooksWorld.ir


The Book Website
www.ImageProcessingPlace.com
Digital Image Processing is a completely self-contained book. However, the companion website offers additional support in a number of important areas.
For the Student or Independent Reader the site contains
• Reviews in areas such as probability, statistics, vectors, and matrices. • A Tutorials section containing dozens of tutorials on topics relevant to the material in the book. • An image database containing all the images in the book, as well as many other image databases.
For the Instructor the site contains
• An Instructor’s Manual with complete solutions to all the problems. • Classroom presentation materials in modifiable PowerPoint format. • Material removed from previous editions, downloadable in convenient PDF format. • Numerous links to other educational resources.
For the Practitioner the site contains additional specialized topics such as • Links to commercial sites. • Selected new references. • Links to commercial image databases.
The website is an ideal tool for keeping the book current between editions by including new topics, digital images, and other relevant material that has appeared after the book was published. Although considerable care was taken in the production of the book, the website is also a convenient repository for any errors discovered between printings.
The DIP4E Support Packages
In this edition, we created support packages for students and faculty to organize all the classroom support materials available for the new edition of the book into one easy download. The Student Support Package contains many of the original images in the book, and answers to selected exercises, The Faculty Support Package contains solutions to all exercises, teaching suggestions, and all the art in the book in modifiable PowerPoint slides. One support package is made available with every new book, free of charge. Applications for the support packages are submitted at the book website.
www.EBooksWorld.ir


About the Authors
RAFAEL C. GONZALEZ
R. C. Gonzalez received the B.S.E.E. degree from the University of Miami in 1965 and the M.E. and Ph.D. degrees in electrical engineering from the University of Florida, Gainesville, in 1967 and 1970, respectively. He joined the Electrical and Computer Science Department at the University of Tennessee, Knoxville (UTK) in 1970, where he became Associate Professor in 1973, Professor in 1978, and Distinguished Service Professor in 1984. He served as Chairman of the department from 1994 through 1997. He is currently a Professor Emeritus at UTK. Gonzalez is the founder of the Image & Pattern Analysis Laboratory and the Robotics & Computer Vision Laboratory at the University of Tennessee. He also founded Perceptics Corporation in 1982 and was its president until 1992. The last three years of this period were spent under a full-time employment contract with Westinghouse Corporation, who acquired the company in 1989. Under his direction, Perceptics became highly successful in image processing, computer vision, and laser disk storage technology. In its initial ten years, Perceptics introduced a series of innovative products, including: The world’s first commercially available computer vision system for automatically reading license plates on moving vehicles; a series of large-scale image processing and archiving systems used by the U.S. Navy at six different manufacturing sites throughout the country to inspect the rocket motors of missiles in the Trident II Submarine Program; the market-leading family of imaging boards for advanced Macintosh computers; and a line of trillionbyte laser disk products. He is a frequent consultant to industry and government in the areas of pattern recognition, image processing, and machine learning. His academic honors for work in these fields include the 1977 UTK College of Engineering Faculty Achievement Award; the 1978 UTK Chancellor’s Research Scholar Award; the 1980 Magnavox Engineering Professor Award; and the 1980 M.E. Brooks Distinguished Professor Award. In 1981 he became an IBM Professor at the University of Tennessee and in 1984 he was named a Distinguished Service Professor there. He was awarded a Distinguished Alumnus Award by the University of Miami in 1985, the Phi Kappa Phi Scholar Award in 1986, and the University of Tennessee’s Nathan W. Dougherty Award for Excellence in Engineering in 1992. Honors for industrial accomplishment include the 1987 IEEE Outstanding Engineer Award for Commercial Development in Tennessee; the 1988 Albert Rose National Award for Excellence in Commercial Image Processing; the 1989 B. Otto Wheeley Award for Excellence in Technology Transfer; the 1989 Coopers and Lybrand Entrepreneur of the Year Award; the 1992 IEEE Region 3 Outstanding Engineer Award; and the 1993 Automated Imaging Association National Award for Technology Development. Gonzalez is author or co-author of over 100 technical articles, two edited books, and four textbooks in the fields of pattern recognition, image processing, and robotics. His books are used in over 1000 universities and research institutions throughout
www.EBooksWorld.ir


the world. He is listed in the prestigious Marquis Who’s Who in America, Marquis Who’s Who in Engineering, Marquis Who’s Who in the World, and in 10 other national and international biographical citations. He is the co-holder of two U.S. Patents, and has been an associate editor of the IEEE Transactions on Systems, Man and Cybernetics, and the International Journal of Computer and Information Sciences. He is a member of numerous professional and honorary societies, including Tau Beta Pi, Phi Kappa Phi, Eta Kappa Nu, and Sigma Xi. He is a Fellow of the IEEE.
RICHARD E. WOODS
R. E. Woods earned his B.S., M.S., and Ph.D. degrees in Electrical Engineering from the University of Tennessee, Knoxville in 1975, 1977, and 1980, respectively. He became an Assistant Professor of Electrical Engineering and Computer Science in 1981 and was recognized as a Distinguished Engineering Alumnus in 1986. A veteran hardware and software developer, Dr. Woods has been involved in the founding of several high-technology startups, including Perceptics Corporation, where he was responsible for the development of the company’s quantitative image analysis and autonomous decision-making products; MedData Interactive, a hightechnology company specializing in the development of handheld computer systems for medical applications; and Interapptics, an internet-based company that designs desktop and handheld computer applications. Dr. Woods currently serves on several nonprofit educational and media-related boards, including Johnson University, and was recently a summer English instructor at the Beijing Institute of Technology. He is the holder of a U.S. Patent in the area of digital image processing and has published two textbooks, as well as numerous articles related to digital signal processing. Dr. Woods is a member of several professional societies, including Tau Beta Pi, Phi Kappa Phi, and the IEEE.
www.EBooksWorld.ir


This page intentionally left blank
www.EBooksWorld.ir


17
1
Introduction
One picture is worth more than ten thousand words. Anonymous
Preview
Interest in digital image processing methods stems from two principal application areas: improvement of pictorial information for human interpretation, and processing of image data for tasks such as storage, transmission, and extraction of pictorial information. This chapter has several objectives: (1) to define the scope of the field that we call image processing; (2) to give a historical perspective of the origins of this field; (3) to present an overview of the state of the art in image processing by examining some of the principal areas in which it is applied; (4) to discuss briefly the principal approaches used in digital image processing; (5) to give an overview of the components contained in a typical, general-purpose image processing system; and (6) to provide direction to the literature where image processing work is reported. The material in this chapter is extensively illustrated with a range of images that are representative of the images we will be using throughout the book.
Upon completion of this chapter, readers should:
Understand the concept of a digital image.
Have a broad overview of the historical underpinnings of the field of digital image processing.
Understand the definition and scope of digital image processing.
Know the fundamentals of the electromagnetic spectrum and its relationship to image generation.
Be aware of the different fields in which digital image processing methods are applied.
Be familiar with the basic processes involved in image processing.
Be familiar with the components that make up a general-purpose digital image processing system.
Be familiar with the scope of the literature where image processing work is reported.
www.EBooksWorld.ir


18 Chapter 1 Introduction
1.1 WHAT IS DIGITAL IMAGE PROCESSING?
An image may be defined as a two-dimensional function, f (x, y), where x and y are spatial (plane) coordinates, and the amplitude of f at any pair of coordinates (x, y) is called the intensity or gray level of the image at that point. When x, y, and the intensity values of f are all finite, discrete quantities, we call the image a digital image. The field of digital image processing refers to processing digital images by means of a digital computer. Note that a digital image is composed of a finite number of elements, each of which has a particular location and value. These elements are called picture elements, image elements, pels, and pixels. Pixel is the term used most widely to denote the elements of a digital image. We will consider these definitions in more formal terms in Chapter 2. Vision is the most advanced of our senses, so it is not surprising that images play the single most important role in human perception. However, unlike humans, who are limited to the visual band of the electromagnetic (EM) spectrum, imaging machines cover almost the entire EM spectrum, ranging from gamma to radio waves. They can operate on images generated by sources that humans are not accustomed to associating with images. These include ultrasound, electron microscopy, and computer-generated images. Thus, digital image processing encompasses a wide and varied field of applications. There is no general agreement among authors regarding where image processing stops and other related areas, such as image analysis and computer vision, start. Sometimes, a distinction is made by defining image processing as a discipline in which both the input and output of a process are images. We believe this to be a limiting and somewhat artificial boundary. For example, under this definition, even the trivial task of computing the average intensity of an image (which yields a single number) would not be considered an image processing operation. On the other hand, there are fields such as computer vision whose ultimate goal is to use computers to emulate human vision, including learning and being able to make inferences and take actions based on visual inputs. This area itself is a branch of artificial intelligence (AI) whose objective is to emulate human intelligence.The field of AI is in its earliest stages of infancy in terms of development, with progress having been much slower than originally anticipated. The area of image analysis (also called image understanding) is in between image processing and computer vision. There are no clear-cut boundaries in the continuum from image processing at one end to computer vision at the other. However, one useful paradigm is to consider three types of computerized processes in this continuum: low-, mid-, and highlevel processes. Low-level processes involve primitive operations such as image preprocessing to reduce noise, contrast enhancement, and image sharpening. A lowlevel process is characterized by the fact that both its inputs and outputs are images. Mid-level processing of images involves tasks such as segmentation (partitioning an image into regions or objects), description of those objects to reduce them to a form suitable for computer processing, and classification (recognition) of individual objects. A mid-level process is characterized by the fact that its inputs generally are images, but its outputs are attributes extracted from those images (e.g., edges, contours, and the identity of individual objects). Finally, higher-level processing
1.1
www.EBooksWorld.ir


1.2 The Origins of Digital Image Processing 19
involves “making sense” of an ensemble of recognized objects, as in image analysis, and, at the far end of the continuum, performing the cognitive functions normally associated with human vision. Based on the preceding comments, we see that a logical place of overlap between image processing and image analysis is the area of recognition of individual regions or objects in an image.Thus, what we call in this book digital image processing encompasses processes whose inputs and outputs are images and, in addition, includes processes that extract attributes from images up to, and including, the recognition of individual objects. As an illustration to clarify these concepts, consider the area of automated analysis of text. The processes of acquiring an image of the area containing the text, preprocessing that image, extracting (segmenting) the individual characters, describing the characters in a form suitable for computer processing, and recognizing those individual characters are in the scope of what we call digital image processing in this book. Making sense of the content of the page may be viewed as being in the domain of image analysis and even computer vision, depending on the level of complexity implied by the statement “making sense of.” As will become evident shortly, digital image processing, as we have defined it, is used routinely in a broad range of areas of exceptional social and economic value. The concepts developed in the following chapters are the foundation for the methods used in those application areas.
1.2 THE ORIGINS OF DIGITAL IMAGE PROCESSING
One of the earliest applications of digital images was in the newspaper industry, when pictures were first sent by submarine cable between London and New York. Introduction of the Bartlane cable picture transmission system in the early 1920s reduced the time required to transport a picture across the Atlantic from more than a week to less than three hours. Specialized printing equipment coded pictures for cable transmission, then reconstructed them at the receiving end. Figure 1.1 was transmitted in this way and reproduced on a telegraph printer fitted with typefaces simulating a halftone pattern. Some of the initial problems in improving the visual quality of these early digital pictures were related to the selection of printing procedures and the distribution of
1.2
FIGURE 1.1 A digital picture produced in 1921 from a coded tape by a telegraph printer with special typefaces. (McFarlane.) [References in the bibliography at the end of the book are listed in alphabetical order by authors’ last names.]
www.EBooksWorld.ir


20 Chapter 1 Introduction
intensity levels. The printing method used to obtain Fig. 1.1 was abandoned toward the end of 1921 in favor of a technique based on photographic reproduction made from tapes perforated at the telegraph receiving terminal. Figure 1.2 shows an image obtained using this method. The improvements over Fig. 1.1 are evident, both in tonal quality and in resolution. The early Bartlane systems were capable of coding images in five distinct levels of gray. This capability was increased to 15 levels in 1929. Figure 1.3 is typical of the type of images that could be obtained using the 15-tone equipment. During this period, introduction of a system for developing a film plate via light beams that were modulated by the coded picture tape improved the reproduction process considerably. Although the examples just cited involve digital images, they are not considered digital image processing results in the context of our definition, because digital computers were not used in their creation. Thus, the history of digital image processing is intimately tied to the development of the digital computer. In fact, digital images require so much storage and computational power that progress in the field of digital image processing has been dependent on the development of digital computers and of supporting technologies that include data storage, display, and transmission.
FIGURE 1.2
A digital picture made in 1922 from a tape punched after the signals had crossed the Atlantic twice. (McFarlane.)
FIGURE 1.3
Unretouched cable picture of Generals Pershing (right) and Foch, transmitted in 1929 from London to New York by 15-tone equipment. (McFarlane.)
www.EBooksWorld.ir


1.2 The Origins of Digital Image Processing 21
The concept of a computer dates back to the invention of the abacus in Asia Minor, more than 5000 years ago. More recently, there have been developments in the past two centuries that are the foundation of what we call a computer today. However, the basis for what we call a modern digital computer dates back to only the 1940s, with the introduction by John von Neumann of two key concepts: (1) a memory to hold a stored program and data, and (2) conditional branching. These two ideas are the foundation of a central processing unit (CPU), which is at the heart of computers today. Starting with von Neumann, there were a series of key advances that led to computers powerful enough to be used for digital image processing. Briefly, these advances may be summarized as follows: (1) the invention of the transistor at Bell Laboratories in 1948; (2) the development in the 1950s and 1960s of the high-level programming languages COBOL (Common Business-Oriented Language) and FORTRAN (Formula Translator); (3) the invention of the integrated circuit (IC) at Texas Instruments in 1958; (4) the development of operating systems in the early 1960s; (5) the development of the microprocessor (a single chip consisting of a CPU, memory, and input and output controls) by Intel in the early 1970s; (6) the introduction by IBM of the personal computer in 1981; and (7) progressive miniaturization of components, starting with large-scale integration (LI) in the late 1970s, then very-large-scale integration (VLSI) in the 1980s, to the present use of ultra-large-scale integration (ULSI) and experimental nonotechnologies. Concurrent with these advances were developments in the areas of mass storage and display systems, both of which are fundamental requirements for digital image processing. The first computers powerful enough to carry out meaningful image processing tasks appeared in the early 1960s. The birth of what we call digital image processing today can be traced to the availability of those machines, and to the onset of the space program during that period. It took the combination of those two developments to bring into focus the potential of digital image processing for solving problems of practical significance. Work on using computer techniques for improving images from a space probe began at the Jet Propulsion Laboratory (Pasadena, California) in 1964, when pictures of the moon transmitted by Ranger 7 were processed by a computer to correct various types of image distortion inherent in the on-board television camera. Figure 1.4 shows the first image of the moon taken by Ranger 7 on July 31, 1964 at 9:09 A.M. Eastern Daylight Time (EDT), about 17 minutes before impacting the lunar surface (the markers, called reseau marks, are used for geometric corrections, as discussed in Chapter 2).This also is the first image of the moon taken by a U.S. spacecraft. The imaging lessons learned with Ranger 7 served as the basis for improved methods used to enhance and restore images from the Surveyor missions to the moon, the Mariner series of flyby missions to Mars, the Apollo manned flights to the moon, and others. In parallel with space applications, digital image processing techniques began in the late 1960s and early 1970s to be used in medical imaging, remote Earth resources observations, and astronomy. The invention in the early 1970s of computerized axial tomography (CAT), also called computerized tomography (CT) for short, is one of the most important events in the application of image processing in medical diagnosis. Computerized axial tomography is a process in which a ring of detectors
www.EBooksWorld.ir


22 Chapter 1 Introduction
encircles an object (or patient) and an X-ray source, concentric with the detector ring, rotates about the object. The X-rays pass through the object and are collected at the opposite end by the corresponding detectors in the ring. This procedure is repeated the source rotates. Tomography consists of algorithms that use the sensed data to construct an image that represents a “slice” through the object. Motion of the object in a direction perpendicular to the ring of detectors produces a set of such slices, which constitute a three-dimensional (3-D) rendition of the inside of the object. Tomography was invented independently by Sir Godfrey N. Hounsfield and Professor Allan M. Cormack, who shared the 1979 Nobel Prize in Medicine for their invention. It is interesting to note that X-rays were discovered in 1895 by Wilhelm Conrad Roentgen, for which he received the 1901 Nobel Prize for Physics.These two inventions, nearly 100 years apart, led to some of the most important applications of image processing today. From the 1960s until the present, the field of image processing has grown vigorously. In addition to applications in medicine and the space program, digital image processing techniques are now used in a broad range of applications. Computer procedures are used to enhance the contrast or code the intensity levels into color for easier interpretation of X-rays and other images used in industry, medicine, and the biological sciences. Geographers use the same or similar techniques to study pollution patterns from aerial and satellite imagery. Image enhancement and restoration procedures are used to process degraded images of unrecoverable objects, or experimental results too expensive to duplicate. In archeology, image processing methods have successfully restored blurred pictures that were the only available records of rare artifacts lost or damaged after being photographed. In physics and related fields, computer techniques routinely enhance images of experiments in areas such as high-energy plasmas and electron microscopy. Similarly successful applications of image processing concepts can be found in astronomy, biology, nuclear medicine, law enforcement, defense, and industry.
FIGURE 1.4
The first picture of the moon by a U.S. spacecraft. Ranger 7 took this image on July 31, 1964 at 9:09 A.M. EDT, about 17 minutes before impacting the lunar surface. (Courtesy of NASA.)
www.EBooksWorld.ir


1.3 Examples of Fields that Use Digital Image Processing 23
These examples illustrate processing results intended for human interpretation. The second major area of application of digital image processing techniques mentioned at the beginning of this chapter is in solving problems dealing with machine perception. In this case, interest is on procedures for extracting information from an image, in a form suitable for computer processing. Often, this information bears little resemblance to visual features that humans use in interpreting the content of an image. Examples of the type of information used in machine perception are statistical moments, Fourier transform coefficients, and multidimensional distance measures. Typical problems in machine perception that routinely utilize image processing techniques are automatic character recognition, industrial machine vision for product assembly and inspection, military recognizance, automatic processing of fingerprints, screening of X-rays and blood samples, and machine processing of aerial and satellite imagery for weather prediction and environmental assessment. The continuing decline in the ratio of computer price to performance, and the expansion of networking and communication bandwidth via the internet, have created unprecedented opportunities for continued growth of digital image processing. Some of these application areas will be illustrated in the following section.
1.3 EXAMPLES OF FIELDS THAT USE DIGITAL IMAGE PROCESSING
Today, there is almost no area of technical endeavor that is not impacted in some way by digital image processing. We can cover only a few of these applications in the context and space of the current discussion. However, limited as it is, the material presented in this section will leave no doubt in your mind regarding the breadth and importance of digital image processing. We show in this section numerous areas of application, each of which routinely utilizes the digital image processing techniques developed in the following chapters. Many of the images shown in this section are used later in one or more of the examples given in the book. Most images shown are digital images. The areas of application of digital image processing are so varied that some form of organization is desirable in attempting to capture the breadth of this field. One of the simplest ways to develop a basic understanding of the extent of image processing applications is to categorize images according to their source (e.g., X-ray, visual, infrared, and so on).The principal energy source for images in use today is the electromagnetic energy spectrum. Other important sources of energy include acoustic, ultrasonic, and electronic (in the form of electron beams used in electron microscopy). Synthetic images, used for modeling and visualization, are generated by computer. In this section we will discuss briefly how images are generated in these various categories, and the areas in which they are applied. Methods for converting images into digital form will be discussed in the next chapter. Images based on radiation from the EM spectrum are the most familiar, especially images in the X-ray and visual bands of the spectrum. Electromagnetic waves can be conceptualized as propagating sinusoidal waves of varying wavelengths, or they can be thought of as a stream of massless particles, each traveling in a wavelike pattern and moving at the speed of light. Each massless particle contains a certain amount (or bundle) of energy. Each bundle of energy is called a photon. If spectral
1.3
www.EBooksWorld.ir


24 Chapter 1 Introduction
bands are grouped according to energy per photon, we obtain the spectrum shown in Fig. 1.5, ranging from gamma rays (highest energy) at one end to radio waves (lowest energy) at the other. The bands are shown shaded to convey the fact that bands of the EM spectrum are not distinct, but rather transition smoothly from one to the other.
GAMMA-RAY IMAGING
Major uses of imaging based on gamma rays include nuclear medicine and astronomical observations. In nuclear medicine, the approach is to inject a patient with a radioactive isotope that emits gamma rays as it decays. Images are produced from the emissions collected by gamma-ray detectors. Figure 1.6(a) shows an image of a complete bone scan obtained by using gamma-ray imaging. Images of this sort are used to locate sites of bone pathology, such as infections or tumors. Figure 1.6(b) shows another major modality of nuclear imaging called positron emission tomography (PET). The principle is the same as with X-ray tomography, mentioned briefly in Section 1.2. However, instead of using an external source of X-ray energy, the patient is given a radioactive isotope that emits positrons as it decays. When a positron meets an electron, both are annihilated and two gamma rays are given off. These are detected and a tomographic image is created using the basic principles of tomography. The image shown in Fig. 1.6(b) is one sample of a sequence that constitutes a 3-D rendition of the patient. This image shows a tumor in the brain and another in the lung, easily visible as small white masses. A star in the constellation of Cygnus exploded about 15,000 years ago, generating a superheated, stationary gas cloud (known as the Cygnus Loop) that glows in a spectacular array of colors. Figure 1.6(c) shows an image of the Cygnus Loop in the gamma-ray band. Unlike the two examples in Figs. 1.6(a) and (b), this image was obtained using the natural radiation of the object being imaged. Finally, Fig. 1.6(d) shows an image of gamma radiation from a valve in a nuclear reactor. An area of strong radiation is seen in the lower left side of the image.
X-RAY IMAGING
X-rays are among the oldest sources of EM radiation used for imaging. The best known use of X-rays is medical diagnostics, but they are also used extensively in industry and other areas, such as astronomy. X-rays for medical and industrial imaging are generated using an X-ray tube, which is a vacuum tube with a cathode and anode. The cathode is heated, causing free electrons to be released. These electrons flow at high speed to the positively charged anode. When the electrons strike a
10 9
10 8
10 7
10 6
10 5
10 4
10 3
10 2
100 10 1
101
102
103
104
105
106
Energy of one photon (electron volts)
Gamma rays X-rays Ultraviolet Visible Infrared Microwaves Radio waves
FIGURE 1.5 The electromagnetic spectrum arranged according to energy per photon.
www.EBooksWorld.ir


1.3 Examples of Fields that Use Digital Image Processing 25
nucleus, energy is released in the form of X-ray radiation. The energy (penetrating power) of X-rays is controlled by a voltage applied across the anode, and by a current applied to the filament in the cathode. Figure 1.7(a) shows a familiar chest X-ray generated simply by placing the patient between an X-ray source and a film sensitive to X-ray energy. The intensity of the X-rays is modified by absorption as they pass through the patient, and the resulting energy falling on the film develops it, much in the same way that light develops photographic film. In digital radiography,
ab cd
FIGURE 1.6
Examples of gamma-ray imaging. (a) Bone scan. (b) PET image. (c) Cygnus Loop. (d) Gamma radiation (bright spot) from a reactor valve. (Images courtesy of (a) G.E. Medical Systems; (b) Dr. Michael E. Casey, CTI PET Systems; (c) NASA; (d) Professors Zhong He and David K. Wehe, University of Michigan.)
www.EBooksWorld.ir


26 Chapter 1 Introduction
digital images are obtained by one of two methods: (1) by digitizing X-ray films; or; (2) by having the X-rays that pass through the patient fall directly onto devices (such as a phosphor screen) that convert X-rays to light.The light signal in turn is captured by a light-sensitive digitizing system. We will discuss digitization in more detail in Chapters 2 and 4.
b
ca d
e
FIGURE 1.7
Examples of X-ray imaging. (a) Chest X-ray. (b) Aortic angiogram. (c) Head CT. (d) Circuit boards. (e) Cygnus Loop. (Images courtesy of (a) and (c) Dr. David R. Pickens, Dept. of Radiology & Radiological Sciences, Vanderbilt University Medical Center; (b) Dr. Thomas R. Gest, Division of Anatomical Sciences, Univ. of Michigan Medical School; (d) Mr. Joseph E. Pascente, Lixi, Inc.; and (e) NASA.)
www.EBooksWorld.ir


1.3 Examples of Fields that Use Digital Image Processing 27
Angiography is another major application in an area called contrast enhancement radiography. This procedure is used to obtain images of blood vessels, called angiograms. A catheter (a small, flexible, hollow tube) is inserted, for example, into an artery or vein in the groin. The catheter is threaded into the blood vessel and guided to the area to be studied. When the catheter reaches the site under investigation, an X-ray contrast medium is injected through the tube. This enhances the contrast of the blood vessels and enables a radiologist to see any irregularities or blockages. Figure 1.7(b) shows an example of an aortic angiogram. The catheter can be seen being inserted into the large blood vessel on the lower left of the picture. Note the high contrast of the large vessel as the contrast medium flows up in the direction of the kidneys, which are also visible in the image. As we will discuss further in Chapter 2, angiography is a major area of digital image processing, where image subtraction is used to further enhance the blood vessels being studied. Another important use of X-rays in medical imaging is computerized axial tomography (CAT). Due to their resolution and 3-D capabilities, CAT scans revolutionized medicine from the moment they first became available in the early 1970s. As noted in Section 1.2, each CAT image is a “slice” taken perpendicularly through the patient. Numerous slices are generated as the patient is moved in a longitudinal direction. The ensemble of such images constitutes a 3-D rendition of the inside of the body, with the longitudinal resolution being proportional to the number of slice images taken. Figure 1.7(c) shows a typical CAT slice image of a human head. Techniques similar to the ones just discussed, but generally involving higher energy X-rays, are applicable in industrial processes. Figure 1.7(d) shows an X-ray image of an electronic circuit board. Such images, representative of literally hundreds of industrial applications of X-rays, are used to examine circuit boards for flaws in manufacturing, such as missing components or broken traces. Industrial CAT scans are useful when the parts can be penetrated by X-rays, such as in plastic assemblies, and even large bodies, such as solid-propellant rocket motors. Figure 1.7(e) shows an example of X-ray imaging in astronomy. This image is the Cygnus Loop of Fig. 1.6(c), but imaged in the X-ray band.
IMAGING IN THE ULTRAVIOLET BAND
Applications of ultraviolet “light” are varied. They include lithography, industrial inspection, microscopy, lasers, biological imaging, and astronomical observations. We illustrate imaging in this band with examples from microscopy and astronomy. Ultraviolet light is used in fluorescence microscopy, one of the fastest growing areas of microscopy. Fluorescence is a phenomenon discovered in the middle of the nineteenth century, when it was first observed that the mineral fluorspar fluoresces when ultraviolet light is directed upon it. The ultraviolet light itself is not visible, but when a photon of ultraviolet radiation collides with an electron in an atom of a fluorescent material, it elevates the electron to a higher energy level. Subsequently, the excited electron relaxes to a lower level and emits light in the form of a lower-energy photon in the visible (red) light region. Important tasks performed with a fluorescence microscope are to use an excitation light to irradiate a prepared specimen, and then to separate the much weaker radiating fluorescent light from the brighter
www.EBooksWorld.ir


28 Chapter 1 Introduction
excitation light. Thus, only the emission light reaches the eye or other detector. The resulting fluorescing areas shine against a dark background with sufficient contrast to permit detection. The darker the background of the nonfluorescing material, the more efficient the instrument. Fluorescence microscopy is an excellent method for studying materials that can be made to fluoresce, either in their natural form (primary fluorescence) or when treated with chemicals capable of fluorescing (secondary fluorescence). Figures 1.8(a) and (b) show results typical of the capability of fluorescence microscopy. Figure 1.8(a) shows a fluorescence microscope image of normal corn, and Fig. 1.8(b) shows corn infected by “smut,” a disease of cereals, corn, grasses, onions, and sorghum that can be caused by any one of more than 700 species of parasitic fungi. Corn smut is particularly harmful because corn is one of the principal food sources in the world. As another illustration, Fig. 1.8(c) shows the Cygnus Loop imaged in the high-energy region of the ultraviolet band.
IMAGING IN THE VISIBLE AND INFRARED BANDS
Considering that the visual band of the electromagnetic spectrum is the most familiar in all our activities, it is not surprising that imaging in this band outweighs by far all the others in terms of breadth of application. The infrared band often is used in conjunction with visual imaging, so we have grouped the visible and infrared bands in this section for the purpose of illustration. We consider in the following discussion applications in light microscopy, astronomy, remote sensing, industry, and law enforcement. Figure 1.9 shows several examples of images obtained with a light microscope. The examples range from pharmaceuticals and microinspection to materials characterization. Even in microscopy alone, the application areas are too numerous to detail here. It is not difficult to conceptualize the types of processes one might apply to these images, ranging from enhancement to measurements.
abc
FIGURE 1.8 Examples of ultraviolet imaging. (a) Normal corn. (b) Corn infected by smut. (c) Cygnus Loop. (Images (a) and (b) courtesy of Dr. Michael W. Davidson, Florida State University, (c) NASA.)
www.EBooksWorld.ir


1.3 Examples of Fields that Use Digital Image Processing 29
Another major area of visual processing is remote sensing, which usually includes several bands in the visual and infrared regions of the spectrum. Table 1.1 shows the so-called thematic bands in NASA’s LANDSAT satellites. The primary function of LANDSAT is to obtain and transmit images of the Earth from space, for purposes of monitoring environmental conditions on the planet. The bands are expressed in terms of wavelength, with 1 mm being equal to 10−6 m (we will discuss the wavelength regions of the electromagnetic spectrum in more detail in Chapter 2). Note the characteristics and uses of each band in Table 1.1. In order to develop a basic appreciation for the power of this type of multispectral imaging, consider Fig. 1.10, which shows one image for each of the spectral bands in Table 1.1. The area imaged is Washington D.C., which includes features such as buildings, roads, vegetation, and a major river (the Potomac) going though the city.
abc de f
FIGURE 1.9
Examples of light microscopy images. (a) Taxol (anticancer agent), magnified 250 ×. (b) Cholesterol40 ×. (c) Microprocessor—60 ×. (d) Nickel oxide thin film—600 ×. (e) Surface of audio CD—1750 ×. (f) Organic superconductor— 450 ×. (Images courtesy of Dr. Michael W. Davidson, Florida State University.)
www.EBooksWorld.ir


30 Chapter 1 Introduction
Images of population centers are used over time to assess population growth and shift patterns, pollution, and other factors affecting the environment. The differences between visual and infrared image features are quite noticeable in these images. Observe, for example, how well defined the river is from its surroundings in Bands 4 and 5. Weather observation and prediction also are major applications of multispectral imaging from satellites. For example, Fig. 1.11 is an image of Hurricane Katrina, one of the most devastating storms in recent memory in the Western Hemisphere. This image was taken by a National Oceanographic and Atmospheric Administration (NOAA) satellite using sensors in the visible and infrared bands. The eye of the hurricane is clearly visible in this image.
Band No. Name Wavelength
(Mm) Characteristics and Uses
1 Visible blue 0.45– 0.52 Maximum water penetration
2 Visible green 0.53– 0.61 Measures plant vigor
3 Visible red 0.63– 0.69 Vegetation discrimination
4 Near infrared 0.78– 0.90 Biomass and shoreline mapping
5 Middle infrared 1.55–1.75 Moisture content: soil/vegetation
6 Thermal infrared 10.4–12.5 Soil moisture; thermal mapping
7 Short-wave infrared 2.09–2.35 Mineral mapping
TABLE 1.1
Thematic bands of NASA’s LANDSAT satellite.
123
4567
FIGURE 1.10 LANDSAT satellite images of the Washington, D.C. area. The numbers refer to the thematic bands in Table 1.1. (Images courtesy of NASA.)
www.EBooksWorld.ir


1.3 Examples of Fields that Use Digital Image Processing 31
Figures 1.12 and 1.13 show an application of infrared imaging. These images are part of the Nighttime Lights of the World data set, which provides a global inventory of human settlements. The images were generated by an infrared imaging system mounted on a NOAA/DMSP (Defense Meteorological Satellite Program) satellite. The infrared system operates in the band 10.0 to 13.4 mm, and has the unique capability to observe faint sources of visible, near infrared emissions present on the Earth’s surface, including cities, towns, villages, gas flares, and fires. Even without formal training in image processing, it is not difficult to imagine writing a computer program that would use these images to estimate the relative percent of total electrical energy used by various regions of the world. A major area of imaging in the visible spectrum is in automated visual inspection of manufactured goods. Figure 1.14 shows some examples. Figure 1.14(a) is a controller board for a CD-ROM drive. A typical image processing task with products such as this is to inspect them for missing parts (the black square on the top, right quadrant of the image is an example of a missing component). Figure 1.14(b) is an imaged pill container. The objective here is to have a machine look for missing, incomplete, or deformed pills. Figure 1.14(c) shows an application in which image processing is used to look for bottles that are not filled up to an acceptable level. Figure 1.14(d) shows a clear plastic part with an unacceptable number of air pockets in it. Detecting anomalies like these is a major theme of industrial inspection that includes other products, such as wood and cloth. Figure 1.14(e) shows a batch of cereal during inspection for color and the presence of anomalies such as burned flakes. Finally, Fig. 1.14(f) shows an image of an intraocular implant (replacement lens for the human eye). A “structured light” illumination technique was used to highlight deformations toward the center of the lens, and other imperfections. For example, the markings at 1 o’clock and 5 o’clock are tweezer damage. Most of the other small speckle detail is debris. The objective in this type of inspection is to find damaged or incorrectly manufactured implants automatically, prior to packaging.
FIGURE 1.11
Satellite image of Hurricane Katrina taken on August 29, 2005. (Courtesy of NOAA.)
www.EBooksWorld.ir


32 Chapter 1 Introduction
Figure 1.15 illustrates some additional examples of image processing in the visible spectrum. Figure 1.15(a) shows a thumb print. Images of fingerprints are routinely processed by computer, either to enhance them or to find features that aid in the automated search of a database for potential matches. Figure 1.15(b) shows an image of paper currency. Applications of digital image processing in this area
FIGURE 1.12 Infrared satellite images of the Americas. The small shaded map is provided for reference. (Courtesy of NOAA.)
www.EBooksWorld.ir


1.3 Examples of Fields that Use Digital Image Processing 33
include automated counting and, in law enforcement, the reading of the serial number for the purpose of tracking and identifying currency bills.The two vehicle images shown in Figs. 1.15(c) and (d) are examples of automated license plate reading. The light rectangles indicate the area in which the imaging system detected the plate. The black rectangles show the results of automatically reading the plate content by the system. License plate and other applications of character recognition are used extensively for traffic monitoring and surveillance.
IMAGING IN THE MICROWAVE BAND
The principal application of imaging in the microwave band is radar. The unique feature of imaging radar is its ability to collect data over virtually any region at any time, regardless of weather or ambient lighting conditions. Some radar waves can penetrate clouds, and under certain conditions, can also see through vegetation, ice, and dry sand. In many cases, radar is the only way to explore inaccessible regions of the Earth’s surface. An imaging radar works like a flash camera in that it provides its own illumination (microwave pulses) to illuminate an area on the ground and
FIGURE 1.13 Infrared satellite images of the remaining populated parts of the world. The small shaded map is provided for reference. (Courtesy of NOAA.)
www.EBooksWorld.ir


34 Chapter 1 Introduction
take a snapshot image. Instead of a camera lens, a radar uses an antenna and digital computer processing to record its images. In a radar image, one can see only the microwave energy that was reflected back toward the radar antenna. Figure 1.16 shows a spaceborne radar image covering a rugged mountainous area of southeast Tibet, about 90 km east of the city of Lhasa. In the lower right corner is a wide valley of the Lhasa River, which is populated by Tibetan farmers and yak herders, and includes the village of Menba. Mountains in this area reach about 5800 m (19,000 ft) above sea level, while the valley floors lie about 4300 m (14,000 ft) above sea level. Note the clarity and detail of the image, unencumbered by clouds or other atmospheric conditions that normally interfere with images in the visual band.
IMAGING IN THE RADIO BAND
As in the case of imaging at the other end of the spectrum (gamma rays), the major applications of imaging in the radio band are in medicine and astronomy. In medicine, radio waves are used in magnetic resonance imaging (MRI). This technique places a
abc de f
FIGURE 1.14 Some examples of manufactured goods checked using digital image processing. (a) Circuit board controller. (b) Packaged pills. (c) Bottles. (d) Air bubbles in a clear plastic product. (e) Cereal. (f) Image of intraocular implant. (Figure (f) courtesy of Mr. Pete Sites, Perceptics Corporation.)
www.EBooksWorld.ir


1.3 Examples of Fields that Use Digital Image Processing 35
patient in a powerful magnet and passes radio waves through the individual’s body in short pulses. Each pulse causes a responding pulse of radio waves to be emitted by the patient’s tissues. The location from which these signals originate and their strength are determined by a computer, which produces a two-dimensional image of a section of the patient. MRI can produce images in any plane. Figure 1.17 shows MRI images of a human knee and spine. The rightmost image in Fig. 1.18 is an image of the Crab Pulsar in the radio band. Also shown for an interesting comparison are images of the same region, but taken in most of the bands discussed earlier. Observe that each image gives a totally different “view” of the pulsar.
OTHER IMAGING MODALITIES
Although imaging in the electromagnetic spectrum is dominant by far, there are a number of other imaging modalities that are also important. Specifically, we discuss
ab
d
c
FIGURE 1.15
Some additional examples of imaging in the visible spectrum. (a) Thumb print. (b) Paper currency. (c) and (d) Automated license plate reading. (Figure (a) courtesy of the National Institute of Standards and Technology. Figures (c) and (d) courtesy of Dr. Juan Herrera, Perceptics Corporation.)
www.EBooksWorld.ir


36 Chapter 1 Introduction
in this section acoustic imaging, electron microscopy, and synthetic (computer-generated) imaging. Imaging using “sound” finds application in geological exploration, industry, and medicine. Geological applications use sound in the low end of the sound spectrum (hundreds of Hz) while imaging in other areas use ultrasound (millions of Hz). The most important commercial applications of image processing in geology are in mineral and oil exploration. For image acquisition over land, one of the main approaches is to use a large truck and a large flat steel plate.The plate is pressed on the ground by
FIGURE 1.16
Spaceborne radar image of mountainous region in southeast Tibet. (Courtesy of NASA.)
ab
FIGURE 1.17 MRI images of a human (a) knee, and (b) spine. (Figure (a) courtesy of Dr. Thomas R. Gest, Division of Anatomical Sciences, University of Michigan Medical School, and (b) courtesy of Dr. David R. Pickens, Department of Radiology and Radiological Sciences, Vanderbilt University Medical Center.)
www.EBooksWorld.ir


1.3 Examples of Fields that Use Digital Image Processing 37
the truck, and the truck is vibrated through a frequency spectrum up to 100 Hz. The strength and speed of the returning sound waves are determined by the composition of the Earth below the surface. These are analyzed by computer, and images are generated from the resulting analysis. For marine image acquisition, the energy source consists usually of two air guns towed behind a ship. Returning sound waves are detected by hydrophones placed in cables that are either towed behind the ship, laid on the bottom of the ocean, or hung from buoys (vertical cables). The two air guns are alternately pressurized to ~2000 psi and then set off. The constant motion of the ship provides a transversal direction of motion that, together with the returning sound waves, is used to generate a 3-D map of the composition of the Earth below the bottom of the ocean. Figure 1.19 shows a cross-sectional image of a well-known 3-D model against which the performance of seismic imaging algorithms is tested.The arrow points to a hydrocarbon (oil and/or gas) trap. This target is brighter than the surrounding layers because the change in density in the target region is larger. Seismic interpreters look for these “bright spots” to find oil and gas. The layers above also are bright, but their brightness does not vary as strongly across the layers. Many seismic reconstruction algorithms have difficulty imaging this target because of the faults above it. Although ultrasound imaging is used routinely in manufacturing, the best known applications of this technique are in medicine, especially in obstetrics, where fetuses are imaged to determine the health of their development. A byproduct of this
Gamma X-ray Optical Infrared Radio
FIGURE 1.18 Images of the Crab Pulsar (in the center of each image) covering the electromagnetic spectrum. (Courtesy of NASA.)
FIGURE 1.19
Cross-sectional image of a seismic model. The arrow points to a hydrocarbon (oil and/or gas) trap. (Courtesy of Dr. Curtis Ober, Sandia National Laboratories.)
www.EBooksWorld.ir


38 Chapter 1 Introduction
examination is determining the sex of the baby. Ultrasound images are generated using the following basic procedure:
1. The ultrasound system (a computer, ultrasound probe consisting of a source, a receiver, and a display) transmits high-frequency (1 to 5 MHz) sound pulses into the body.
2. The sound waves travel into the body and hit a boundary between tissues (e.g., between fluid and soft tissue, soft tissue and bone). Some of the sound waves are reflected back to the probe, while some travel on further until they reach another boundary and are reflected. 3. The reflected waves are picked up by the probe and relayed to the computer. 4. The machine calculates the distance from the probe to the tissue or organ boundaries using the speed of sound in tissue (1540 m/s) and the time of each echo’s return.
5. The system displays the distances and intensities of the echoes on the screen, forming a two-dimensional image.
In a typical ultrasound image, millions of pulses and echoes are sent and received each second. The probe can be moved along the surface of the body and angled to obtain various views. Figure 1.20 shows several examples of medical uses of ultrasound. We continue the discussion on imaging modalities with some examples of electron microscopy. Electron microscopes function as their optical counterparts, except
ab cd
FIGURE 1.20 Examples of ultrasound imaging. (a) A fetus. (b) Another view of the fetus. (c) Thyroids. (d) Muscle layers showing lesion. (Courtesy of Siemens Medical Systems, Inc., Ultrasound Group.)
www.EBooksWorld.ir


1.3 Examples of Fields that Use Digital Image Processing 39
that they use a focused beam of electrons instead of light to image a specimen. The operation of electron microscopes involves the following basic steps: A stream of electrons is produced by an electron source and accelerated toward the specimen using a positive electrical potential. This stream is confined and focused using metal apertures and magnetic lenses into a thin, monochromatic beam. This beam is focused onto the sample using a magnetic lens. Interactions occur inside the irradiated sample, affecting the electron beam. These interactions and effects are detected and transformed into an image, much in the same way that light is reflected from, or absorbed by, objects in a scene. These basic steps are carried out in all electron microscopes. A transmission electron microscope (TEM) works much like a slide projector. A projector transmits a beam of light through a slide; as the light passes through the slide, it is modulated by the contents of the slide. This transmitted beam is then projected onto the viewing screen, forming an enlarged image of the slide. TEMs work in the same way, except that they shine a beam of electrons through a specimen (analogous to the slide). The fraction of the beam transmitted through the specimen is projected onto a phosphor screen. The interaction of the electrons with the phosphor produces light and, therefore, a viewable image. A scanning electron microscope (SEM), on the other hand, actually scans the electron beam and records the interaction of beam and sample at each location. This produces one dot on a phosphor screen. A complete image is formed by a raster scan of the beam through the sample, much like a TV camera. The electrons interact with a phosphor screen and produce light. SEMs are suitable for “bulky” samples, while TEMs require very thin samples. Electron microscopes are capable of very high magnification. While light microscopy is limited to magnifications on the order of 1000 ×, electron microscopes can achieve magnification of 10, 000 × or more. Figure 1.21 shows two SEM images of specimen failures due to thermal overload. We conclude the discussion of imaging modalities by looking briefly at images that are not obtained from physical objects. Instead, they are generated by computer. Fractals are striking examples of computer-generated images. Basically, a fractal is nothing more than an iterative reproduction of a basic pattern according to some mathematical rules. For instance, tiling is one of the simplest ways to generate a fractal image.A square can be subdivided into four square subregions, each of which can be further subdivided into four smaller square regions, and so on. Depending on the complexity of the rules for filling each subsquare, some beautiful tile images can be generated using this method. Of course, the geometry can be arbitrary. For instance, the fractal image could be grown radially out of a center point. Figure 1.22(a) shows a fractal grown in this way. Figure 1.22(b) shows another fractal (a “moonscape”) that provides an interesting analogy to the images of space used as illustrations in some of the preceding sections. A more structured approach to image generation by computer lies in 3-D modeling. This is an area that provides an important intersection between image processing and computer graphics, and is the basis for many 3-D visualization systems (e.g., flight simulators). Figures 1.22(c) and (d) show examples of computer-generated images. Because the original object is created in 3-D, images can be generated in any
www.EBooksWorld.ir


40 Chapter 1 Introduction
perspective from plane projections of the 3-D volume. Images of this type can be used for medical training and for a host of other applications, such as criminal forensics and special effects.
ab
FIGURE 1.21 (a) 250 × SEM image of a tungsten filament following thermal failure (note the shattered pieces on the lower left). (b) 2500 × SEM image of a damaged integrated circuit. The white fibers are oxides resulting from thermal destruction. (Figure (a) courtesy of Mr. Michael Shaffer, Department of Geological Sciences, University of Oregon, Eugene; (b) courtesy of Dr. J. M. Hudak, McMaster University, Hamilton, Ontario, Canada.)
ab cd
FIGURE 1.22
(a) and (b) Fractal images. (c) and (d) Images generated from 3-D computer models of the objects shown. (Figures (a) and (b) courtesy of Ms. Melissa D. Binde, Swarthmore College; (c) and (d) courtesy of NASA.)
www.EBooksWorld.ir


1.4 Fundamental Steps in Digital Image Processing 41
1.4 FUNDAMENTAL STEPS IN DIGITAL IMAGE PROCESSING
It is helpful to divide the material covered in the following chapters into the two broad categories defined in Section 1.1: methods whose input and output are images, and methods whose inputs may be images, but whose outputs are attributes extracted from those images. This organization is summarized in Fig. 1.23. The diagram does not imply that every process is applied to an image. Rather, the intention is to convey an idea of all the methodologies that can be applied to images for different purposes, and possibly with different objectives. The discussion in this section may be viewed as a brief overview of the material in the remainder of the book. Image acquisition is the first process in Fig. 1.23. The discussion in Section 1.3 gave some hints regarding the origin of digital images. This topic will be considered in much more detail in Chapter 2, where we also introduce a number of basic digital image concepts that are used throughout the book.Acquisition could be as simple as being given an image that is already in digital form. Generally, the image acquisition stage involves preprocessing, such as scaling. Image enhancement is the process of manipulating an image so the result is more suitable than the original for a specific application. The word specific is important here, because it establishes at the outset that enhancement techniques are problem oriented. Thus, for example, a method that is quite useful for enhancing X-ray images may not be the best approach for enhancing satellite images taken in the infrared band of the electromagnetic spectrum. There is no general “theory” of image enhancement. When an image is processed for visual interpretation, the viewer is the ultimate judge of how well a particular
1.4
Knowledge base
CHAPTER 7
Wavelets and other image transforms
Outputs of these processes generally are images
CHAPTER 5
Image restoration
CHAPTERS 3 & 4
Image filtering and enhancement
Problem domain
Outputs of these processes generally are image attributes
CHAPTER 8
Compression and watermarking
CHAPTER 2
Image acquisition
CHAPTER 9
Morphological processing
CHAPTERS 10
Segmentation
CHAPTER 11
Feature extraction
CHAPTER 12 Image pattern classification
Wavelets and multiresolution processing
Color Image Processing
CHAPTER 6
FIGURE 1.23
Fundamental steps in digital image processing. The chapter(s) indicated in the boxes is where the material described in the box is discussed.
www.EBooksWorld.ir


42 Chapter 1 Introduction
method works. Enhancement techniques are so varied, and use so many different image processing approaches, that it is difficult to assemble a meaningful body of techniques suitable for enhancement in one chapter without extensive background development. For this reason, and also because beginners in the field of image processing generally find enhancement applications visually appealing, interesting, and relatively simple to understand, we will use image enhancement as examples when introducing new concepts in parts of Chapter 2 and in Chapters 3 and 4. The material in the latter two chapters span many of the methods used traditionally for image enhancement.Therefore, using examples from image enhancement to introduce new image processing methods developed in these early chapters not only saves having an extra chapter in the book dealing with image enhancement but, more importantly, is an effective approach for introducing newcomers to the details of processing techniques early in the book. However, as you will see in progressing through the rest of the book, the material developed in Chapters 3 and 4 is applicable to a much broader class of problems than just image enhancement. Image restoration is an area that also deals with improving the appearance of an image. However, unlike enhancement, which is subjective, image restoration is objective, in the sense that restoration techniques tend to be based on mathematical or probabilistic models of image degradation. Enhancement, on the other hand, is based on human subjective preferences regarding what constitutes a “good” enhancement result. Color image processing is an area that has been gaining in importance because of the significant increase in the use of digital images over the internet. Chapter 6 covers a number of fundamental concepts in color models and basic color processing in a digital domain. Color is used also as the basis for extracting features of interest in an image. Wavelets are the foundation for representing images in various degrees of resolution. In particular, this material is used in the book for image data compression and for pyramidal representation, in which images are subdivided successively into smaller regions. The material in Chapters 4 and 5 is based mostly on the Fourier transform. In addition to wavelets, we will also discuss in Chapter 7 a number of other transforms that are used routinely in image processing. Compression, as the name implies, deals with techniques for reducing the storage required to save an image, or the bandwidth required to transmit it. Although storage technology has improved significantly over the past decade, the same cannot be said for transmission capacity. This is true particularly in uses of the internet, which are characterized by significant pictorial content. Image compression is familiar (perhaps inadvertently) to most users of computers in the form of image file extensions, such as the jpg file extension used in the JPEG (Joint Photographic Experts Group) image compression standard. Morphological processing deals with tools for extracting image components that are useful in the representation and description of shape. The material in this chapter begins a transition from processes that output images to processes that output image attributes, as indicated in Section 1.1. Segmentation partitions an image into its constituent parts or objects. In general, autonomous segmentation is one of the most difficult tasks in digital image
www.EBooksWorld.ir


1.4 Fundamental Steps in Digital Image Processing 43
processing. A rugged segmentation procedure brings the process a long way toward successful solution of imaging problems that require objects to be identified individually. On the other hand, weak or erratic segmentation algorithms almost always guarantee eventual failure. In general, the more accurate the segmentation, the more likely automated object classification is to succeed. Feature extraction almost always follows the output of a segmentation stage, which usually is raw pixel data, constituting either the boundary of a region (i.e., the set of pixels separating one image region from another) or all the points in the region itself. Feature extraction consists of feature detection and feature description. Feature detection refers to finding the features in an image, region, or boundary. Feature description assigns quantitative attributes to the detected features. For example, we might detect corners in a region, and describe those corners by their orientation and location; both of these descriptors are quantitative attributes. Feature processing methods discussed in this chapter are subdivided into three principal categories, depending on whether they are applicable to boundaries, regions, or whole images. Some features are applicable to more than one category. Feature descriptors should be as insensitive as possible to variations in parameters such as scale, translation, rotation, illumination, and viewpoint. Image pattern classification is the process that assigns a label (e.g.,“vehicle”) to an object based on its feature descriptors. In the last chapter of the book, we will discuss methods of image pattern classification ranging from “classical” approaches such as minimum-distance, correlation, and Bayes classifiers, to more modern approaches implemented using deep neural networks. In particular, we will discuss in detail deep convolutional neural networks, which are ideally suited for image processing work. So far, we have said nothing about the need for prior knowledge or about the interaction between the knowledge base and the processing modules in Fig. 1.23. Knowledge about a problem domain is coded into an image processing system in the form of a knowledge database.This knowledge may be as simple as detailing regions of an image where the information of interest is known to be located, thus limiting the search that has to be conducted in seeking that information.The knowledge base can also be quite complex, such as an interrelated list of all major possible defects in a materials inspection problem, or an image database containing high-resolution satellite images of a region in connection with change-detection applications. In addition to guiding the operation of each processing module, the knowledge base also controls the interaction between modules. This distinction is made in Fig. 1.23 by the use of double-headed arrows between the processing modules and the knowledge base, as opposed to single-headed arrows linking the processing modules. Although we do not discuss image display explicitly at this point, it is important to keep in mind that viewing the results of image processing can take place at the output of any stage in Fig. 1.23. We also note that not all image processing applications require the complexity of interactions implied by Fig. 1.23. In fact, not even all those modules are needed in many cases. For example, image enhancement for human visual interpretation seldom requires use of any of the other stages in Fig. 1.23. In general, however, as the complexity of an image processing task increases, so does the number of processes required to solve the problem.
www.EBooksWorld.ir


44 Chapter 1 Introduction
1.5 COMPONENTS OF AN IMAGE PROCESSING SYSTEM
As recently as the mid-1980s, numerous models of image processing systems being sold throughout the world were rather substantial peripheral devices that attached to equally substantial host computers. Late in the 1980s and early in the 1990s, the market shifted to image processing hardware in the form of single boards designed to be compatible with industry standard buses and to fit into engineering workstation cabinets and personal computers. In the late 1990s and early 2000s, a new class of add-on boards, called graphics processing units (GPUs) were introduced for work on 3-D applications, such as games and other 3-D graphics applications. It was not long before GPUs found their way into image processing applications involving large-scale matrix implementations, such as training deep convolutional networks. In addition to lowering costs, the market shift from substantial peripheral devices to add-on processing boards also served as a catalyst for a significant number of new companies specializing in the development of software written specifically for image processing. The trend continues toward miniaturizing and blending of general-purpose small computers with specialized image processing hardware and software. Figure 1.24 shows the basic components comprising a typical general-purpose system used for digital image processing. The function of each component will be discussed in the following paragraphs, starting with image sensing. Two subsystems are required to acquire digital images. The first is a physical sensor that responds to the energy radiated by the object we wish to image. The second, called a digitizer, is a device for converting the output of the physical sensing device into digital form. For instance, in a digital video camera, the sensors (CCD chips) produce an electrical output proportional to light intensity. The digitizer converts these outputs to digital data. These topics will be covered in Chapter 2. Specialized image processing hardware usually consists of the digitizer just mentioned, plus hardware that performs other primitive operations, such as an arithmetic logic unit (ALU), that performs arithmetic and logical operations in parallel on entire images. One example of how an ALU is used is in averaging images as quickly as they are digitized, for the purpose of noise reduction. This type of hardware sometimes is called a front-end subsystem, and its most distinguishing characteristic is speed. In other words, this unit performs functions that require fast data throughputs (e.g., digitizing and averaging video images at 30 frames/s) that the typical main computer cannot handle. One or more GPUs (see above) also are common in image processing systems that perform intensive matrix operations. The computer in an image processing system is a general-purpose computer and can range from a PC to a supercomputer. In dedicated applications, sometimes custom computers are used to achieve a required level of performance, but our interest here is on general-purpose image processing systems. In these systems, almost any well-equipped PC-type machine is suitable for off-line image processing tasks. Software for image processing consists of specialized modules that perform specific tasks. A well-designed package also includes the capability for the user to write code that, as a minimum, utilizes the specialized modules. More sophisticated
1.5
www.EBooksWorld.ir


1.5 Components of an Image Processing System 45
software packages allow the integration of those modules and general-purpose software commands from at least one computer language. Commercially available image processing software, such as the well-known MATLAB® Image Processing Toolbox, is also common in a well-equipped image processing system. Mass storage is a must in image processing applications.An image of size 1024 × 1024 pixels, in which the intensity of each pixel is an 8-bit quantity, requires one megabyte of storage space if the image is not compressed. When dealing with image databases that contain thousands, or even millions, of images, providing adequate storage in an image processing system can be a challenge. Digital storage for image processing applications falls into three principal categories: (1) short-term storage for use during processing; (2) on-line storage for relatively fast recall; and (3) archival storage, characterized by infrequent access. Storage is measured in bytes (eight bits), Kbytes (103 bytes), Mbytes (106 bytes), Gbytes (109 bytes), and Tbytes (1012 bytes).
Cloud
Image displays Computer Mass storage
Hardcopy
Specialized image processing hardware
Image sensors
Problem domain
Image processing software
Network
Cloud
FIGURE 1.24
Components of a general-purpose image processing system.
www.EBooksWorld.ir


46 Chapter 1 Introduction
One method of providing short-term storage is computer memory. Another is by specialized boards, called frame buffers, that store one or more images and can be accessed rapidly, usually at video rates (e.g., at 30 complete images per second). The latter method allows virtually instantaneous image zoom, as well as scroll (vertical shifts) and pan (horizontal shifts). Frame buffers usually are housed in the specialized image processing hardware unit in Fig. 1.24. On-line storage generally takes the form of magnetic disks or optical-media storage. The key factor characterizing on-line storage is frequent access to the stored data. Finally, archival storage is characterized by massive storage requirements but infrequent need for access. Magnetic tapes and optical disks housed in “jukeboxes” are the usual media for archival applications. Image displays in use today are mainly color, flat screen monitors. Monitors are driven by the outputs of image and graphics display cards that are an integral part of the computer system. Seldom are there requirements for image display applications that cannot be met by display cards and GPUs available commercially as part of the computer system. In some cases, it is necessary to have stereo displays, and these are implemented in the form of headgear containing two small displays embedded in goggles worn by the user. Hardcopy devices for recording images include laser printers, film cameras, heatsensitive devices, ink-jet units, and digital units, such as optical and CD-ROM disks. Film provides the highest possible resolution, but paper is the obvious medium of choice for written material. For presentations, images are displayed on film transparencies or in a digital medium if image projection equipment is used. The latter approach is gaining acceptance as the standard for image presentations. Networking and cloud communication are almost default functions in any computer system in use today. Because of the large amount of data inherent in image processing applications, the key consideration in image transmission is bandwidth. In dedicated networks, this typically is not a problem, but communications with remote sites via the internet are not always as efficient. Fortunately, transmission bandwidth is improving quickly as a result of optical fiber and other broadband technologies. Image data compression continues to play a major role in the transmission of large amounts of image data.
Summary, References, and Further Reading
The main purpose of the material presented in this chapter is to provide a sense of perspective about the origins of digital image processing and, more important, about current and future areas of application of this technology. Although the coverage of these topics in this chapter was necessarily incomplete due to space limitations, it should have left you with a clear impression of the breadth and practical scope of digital image processing. As we proceed in the following chapters with the development of image processing theory and applications, numerous examples are provided to keep a clear focus on the utility and promise of these techniques. Upon concluding the study of the final chapter, a reader of this book will have arrived at a level of understanding that is the foundation for most of the work currently underway in this field. In past editions, we have provided a long list of journals and books to give readers an idea of the breadth of the image processing literature, and where this literature is reported. The list has been updated, and it has become so extensive that it is more practical to include it in the book website: www.ImageProcessingPlace.com, in the section entitled Publications.
www.EBooksWorld.ir


47
2
Digital Image Fundamentals
Preview
This chapter is an introduction to a number of basic concepts in digital image processing that are used throughout the book. Section 2.1 summarizes some important aspects of the human visual system, including image formation in the eye and its capabilities for brightness adaptation and discrimination. Section 2.2 discusses light, other components of the electromagnetic spectrum, and their imaging characteristics. Section 2.3 discusses imaging sensors and how they are used to generate digital images. Section 2.4 introduces the concepts of uniform image sampling and intensity quantization. Additional topics discussed in that section include digital image representation, the effects of varying the number of samples and intensity levels in an image, the concepts of spatial and intensity resolution, and the principles of image interpolation. Section 2.5 deals with a variety of basic relationships between pixels. Finally, Section 2.6 is an introduction to the principal mathematical tools we use throughout the book. A second objective of that section is to help you begin developing a “feel” for how these tools are used in a variety of basic image processing tasks.
Upon completion of this chapter, readers should:
Have an understanding of some important functions and limitations of human vision.
Be familiar with the electromagnetic energy spectrum, including basic properties of light.
Know how digital images are generated and represented.
Understand the basics of image sampling and quantization.
Be familiar with spatial and intensity resolution and their effects on image appearance.
Have an understanding of basic geometric relationships between image pixels.
Be familiar with the principal mathematical tools used in digital image processing.
Be able to apply a variety of introductory digital image processing techniques.
Those who wish to succeed must ask the right preliminary questions. Aristotle
www.EBooksWorld.ir


48 Chapter 2 Digital Image Fundamentals
2.1 ELEMENTS OF VISUAL PERCEPTION
Although the field of digital image processing is built on a foundation of mathematics, human intuition and analysis often play a role in the choice of one technique versus another, and this choice often is made based on subjective, visual judgments. Thus, developing an understanding of basic characteristics of human visual perception as a first step in our journey through this book is appropriate. In particular, our interest is in the elementary mechanics of how images are formed and perceived by humans. We are interested in learning the physical limitations of human vision in terms of factors that also are used in our work with digital images. Factors such as how human and electronic imaging devices compare in terms of resolution and ability to adapt to changes in illumination are not only interesting, they are also important from a practical point of view.
STRUCTURE OF THE HUMAN EYE
Figure 2.1 shows a simplified cross section of the human eye. The eye is nearly a sphere (with a diameter of about 20 mm) enclosed by three membranes: the cornea and sclera outer cover; the choroid; and the retina. The cornea is a tough, transparent tissue that covers the anterior surface of the eye. Continuous with the cornea, the sclera is an opaque membrane that encloses the remainder of the optic globe. The choroid lies directly below the sclera. This membrane contains a network of blood vessels that serve as the major source of nutrition to the eye. Even superficial
2.1
Retina
Blind spot Sclera
Choroid
Nerve&sheath
Fovea
Vitreous humor
Visual axis
Ciliary fibers
Ciliary muscle
Iris
Cornea
Lens
Anterior chamber
Ciliarybody
FIGURE 2.1 Simplified diagram of a cross section of the human eye.
www.EBooksWorld.ir


2.1 Elements of Visual Perception 49
injury to the choroid can lead to severe eye damage as a result of inflammation that restricts blood flow. The choroid coat is heavily pigmented, which helps reduce the amount of extraneous light entering the eye and the backscatter within the optic globe. At its anterior extreme, the choroid is divided into the ciliary body and the iris. The latter contracts or expands to control the amount of light that enters the eye. The central opening of the iris (the pupil) varies in diameter from approximately 2 to 8 mm. The front of the iris contains the visible pigment of the eye, whereas the back contains a black pigment. The lens consists of concentric layers of fibrous cells and is suspended by fibers that attach to the ciliary body. It is composed of 60% to 70% water, about 6% fat, and more protein than any other tissue in the eye. The lens is colored by a slightly yellow pigmentation that increases with age. In extreme cases, excessive clouding of the lens, referred to as cataracts, can lead to poor color discrimination and loss of clear vision. The lens absorbs approximately 8% of the visible light spectrum, with higher absorption at shorter wavelengths. Both infrared and ultraviolet light are absorbed by proteins within the lens and, in excessive amounts, can damage the eye. The innermost membrane of the eye is the retina, which lines the inside of the wall’s entire posterior portion. When the eye is focused, light from an object is imaged on the retina. Pattern vision is afforded by discrete light receptors distributed over the surface of the retina. There are two types of receptors: cones and rods. There are between 6 and 7 million cones in each eye. They are located primarily in the central portion of the retina, called the fovea, and are highly sensitive to color. Humans can resolve fine details because each cone is connected to its own nerve end. Muscles rotate the eye until the image of a region of interest falls on the fovea. Cone vision is called photopic or bright-light vision. The number of rods is much larger: Some 75 to 150 million are distributed over the retina. The larger area of distribution, and the fact that several rods are connected to a single nerve ending, reduces the amount of detail discernible by these receptors. Rods capture an overall image of the field of view. They are not involved in color vision, and are sensitive to low levels of illumination. For example, objects that appear brightly colored in daylight appear as colorless forms in moonlight because only the rods are stimulated. This phenomenon is known as scotopic or dim-light vision. Figure 2.2 shows the density of rods and cones for a cross section of the right eye, passing through the region where the optic nerve emerges from the eye.The absence of receptors in this area causes the so-called blind spot (see Fig. 2.1). Except for this region, the distribution of receptors is radially symmetric about the fovea. Receptor density is measured in degrees from the visual axis. Note in Fig. 2.2 that cones are most dense in the center area of the fovea, and that rods increase in density from the center out to approximately 20° off axis. Then, their density decreases out to the periphery of the retina. The fovea itself is a circular indentation in the retina of about 1.5 mm in diameter, so it has an area of approximately 1.77 mm2. As Fig. 2.2 shows, the density of cones in that area of the retina is on the order of 150,000 elements per mm2. Based on these figures, the number of cones in the fovea, which is the region of highest acuity
www.EBooksWorld.ir


50 Chapter 2 Digital Image Fundamentals
in the eye, is about 265,000 elements. Modern electronic imaging chips exceed this number by a large factor. While the ability of humans to integrate intelligence and experience with vision makes purely quantitative comparisons somewhat superficial, keep in mind for future discussions that electronic imaging sensors can easily exceed the capability of the eye in resolving image detail.
IMAGE FORMATION IN THE EYE
In an ordinary photographic camera, the lens has a fixed focal length. Focusing at various distances is achieved by varying the distance between the lens and the imaging plane, where the film (or imaging chip in the case of a digital camera) is located. In the human eye, the converse is true; the distance between the center of the lens and the imaging sensor (the retina) is fixed, and the focal length needed to achieve proper focus is obtained by varying the shape of the lens. The fibers in the ciliary body accomplish this by flattening or thickening the lens for distant or near objects, respectively. The distance between the center of the lens and the retina along the visual axis is approximately 17 mm. The range of focal lengths is approximately 14 mm to 17 mm, the latter taking place when the eye is relaxed and focused at distances greater than about 3 m. The geometry in Fig. 2.3 illustrates how to obtain the dimensions of an image formed on the retina. For example, suppose that a person is looking at a tree 15 m high at a distance of 100 m. Letting h denote the height of that object in the retinal image, the geometry of Fig. 2.3 yields 15 100 = h 17 or h = 2.5 mm. As indicated earlier in this section, the retinal image is focused primarily on the region of the fovea. Perception then takes place by the relative excitation of light receptors, which transform radiant energy into electrical impulses that ultimately are decoded by the brain.
BRIGHTNESS ADAPTATION AND DISCRIMINATION
Because digital images are displayed as sets of discrete intensities, the eye’s ability to discriminate between different intensity levels is an important consideration
FIGURE 2.2
Distribution of rods and cones in the retina.
Blind spot Cones
Rods
No. of rods or cones per mm2
Degrees from visual axis (center of fovea)
180,000
135,000
90,000
45,000
80 60 40 20 0 20 40 60 80
www.EBooksWorld.ir


2.1 Elements of Visual Perception 51
in presenting image processing results. The range of light intensity levels to which the human visual system can adapt is enormous—on the order of 1010— from the scotopic threshold to the glare limit. Experimental evidence indicates that subjective brightness (intensity as perceived by the human visual system) is a logarithmic function of the light intensity incident on the eye. Figure 2.4, a plot of light intensity versus subjective brightness, illustrates this characteristic. The long solid curve represents the range of intensities to which the visual system can adapt. In photopic vision alone, the range is about 106. The transition from scotopic to photopic vision is gradual over the approximate range from 0.001 to 0.1 millilambert (−3 to −1 mL in the log scale), as the double branches of the adaptation curve in this range show. The key point in interpreting the impressive dynamic range depicted in Fig. 2.4 is that the visual system cannot operate over such a range simultaneously. Rather, it accomplishes this large variation by changing its overall sensitivity, a phenomenon known as brightness adaptation. The total range of distinct intensity levels the eye can discriminate simultaneously is rather small when compared with the total adaptation range. For a given set of conditions, the current sensitivity level of the visual system is called the brightness adaptation level, which may correspond, for example,
FIGURE 2.3 Graphical representation of the eye looking at a palm tree. Point C is the focal center of the lens.
15 m
C
100 m 17 mm
FIGURE 2.4
Range of subjective brightness sensations showing a particular adaptation level, Ba .
Glare limit
Subjective brightness
Adaptation range
Scotopic threshold
Log of intensity (mL)
Scotopic
Photopic
6 4 20 2 4
Ba
Bb
www.EBooksWorld.ir


52 Chapter 2 Digital Image Fundamentals
to brightness Ba in Fig. 2.4. The short intersecting curve represents the range of subjective brightness that the eye can perceive when adapted to this level. This range is rather restricted, having a level Bb at, and below which, all stimuli are perceived as indistinguishable blacks. The upper portion of the curve is not actually restricted but, if extended too far, loses its meaning because much higher intensities would simply raise the adaptation level higher than Ba. The ability of the eye to discriminate between changes in light intensity at any specific adaptation level is of considerable interest. A classic experiment used to determine the capability of the human visual system for brightness discrimination consists of having a subject look at a flat, uniformly illuminated area large enough to occupy the entire field of view. This area typically is a diffuser, such as opaque glass, illuminated from behind by a light source, I, with variable intensity. To this field is added an increment of illumination, I , in the form of a short-duration flash that appears as a circle in the center of the uniformly illuminated field, as Fig. 2.5 shows. If I is not bright enough, the subject says “no,” indicating no perceivable change. As I gets stronger, the subject may give a positive response of “yes,” indicating a perceived change. Finally, when I is strong enough, the subject will give a response of “yes” all the time. The quantity Ic I , where Ic is the increment of illumination discriminable 50% of the time with background illumination I, is called the Weber ratio. A small value of Ic I means that a small percentage change in intensity is discriminable. This represents “good” brightness discrimination. Conversely, a large value of Ic I means that a large percentage change in intensity is required for the eye to detect the change. This represents “poor” brightness discrimination. A plot of Ic I as a function of log I has the characteristic shape shown in Fig. 2.6. This curve shows that brightness discrimination is poor (the Weber ratio is large) at low levels of illumination, and it improves significantly (the Weber ratio decreases) as background illumination increases. The two branches in the curve reflect the fact that at low levels of illumination vision is carried out by the rods, whereas, at high levels, vision is a function of cones. If the background illumination is held constant and the intensity of the other source, instead of flashing, is now allowed to vary incrementally from never being perceived to always being perceived, the typical observer can discern a total of one to two dozen different intensity changes. Roughly, this result is related to the number of different intensities a person can see at any one point or small area in a monochrome image. This does not mean that an image can be represented by such a small number of intensity values because, as the eye roams about the image, the average
FIGURE 2.5 Basic experimental setup used to characterize brightness discrimination.
I
II
+
www.EBooksWorld.ir


2.1 Elements of Visual Perception 53
background changes, thus allowing a different set of incremental changes to be detected at each new adaptation level. The net result is that the eye is capable of a broader range of overall intensity discrimination. In fact, as we will show in Section 2.4, the eye is capable of detecting objectionable effects in monochrome images whose overall intensity is represented by fewer than approximately two dozen levels. Two phenomena demonstrate that perceived brightness is not a simple function of intensity. The first is based on the fact that the visual system tends to undershoot or overshoot around the boundary of regions of different intensities. Figure 2.7(a) shows a striking example of this phenomenon. Although the intensity of the stripes
FIGURE 2.6
A typical plot of the Weber ratio as a function of intensity.
1.5
2.04 3 2 1 0 log I
log Ic/I
1234
1.0
0.5
0.5
1.0
0
Actual intensity
Perceived intensity
FIGURE 2.7
Illustration of the Mach band effect. Perceived intensity is not a simple function of actual intensity.
b
a
c
www.EBooksWorld.ir


54 Chapter 2 Digital Image Fundamentals
is constant [see Fig. 2.7(b)], we actually perceive a brightness pattern that is strongly scalloped near the boundaries, as Fig. 2.7(c) shows. These perceived scalloped bands are called Mach bands after Ernst Mach, who first described the phenomenon in 1865. The second phenomenon, called simultaneous contrast, is that a region’s perceived brightness does not depend only on its intensity, as Fig. 2.8 demonstrates. All the center squares have exactly the same intensity, but each appears to the eye to become darker as the background gets lighter. A more familiar example is a piece of paper that looks white when lying on a desk, but can appear totally black when used to shield the eyes while looking directly at a bright sky. Other examples of human perception phenomena are optical illusions, in which the eye fills in nonexisting details or wrongly perceives geometrical properties of objects. Figure 2.9 shows some examples. In Fig. 2.9(a), the outline of a square is seen clearly, despite the fact that no lines defining such a figure are part of the image. The same effect, this time with a circle, can be seen in Fig. 2.9(b); note how just a few lines are sufficient to give the illusion of a complete circle. The two horizontal line segments in Fig. 2.9(c) are of the same length, but one appears shorter than the other. Finally, all long lines in Fig. 2.9(d) are equidistant and parallel. Yet, the crosshatching creates the illusion that those lines are far from being parallel.
2.2 LIGHT AND THE ELECTROMAGNETIC SPECTRUM
The electromagnetic spectrum was introduced in Section 1.3. We now consider this topic in more detail. In 1666, Sir Isaac Newton discovered that when a beam of sunlight passes through a glass prism, the emerging beam of light is not white but consists instead of a continuous spectrum of colors ranging from violet at one end to red at the other. As Fig. 2.10 shows, the range of colors we perceive in visible light is a small portion of the electromagnetic spectrum. On one end of the spectrum are radio waves with wavelengths billions of times longer than those of visible light. On the other end of the spectrum are gamma rays with wavelengths millions of times smaller than those of visible light. We showed examples in Section 1.3 of images in most of the bands in the EM spectrum.
2.2
abc
FIGURE 2.8 Examples of simultaneous contrast. All the inner squares have the same intensity, but they appear progressively darker as the background becomes lighter.
www.EBooksWorld.ir


2.2 Light and the Electromagnetic Spectrum 55
The electromagnetic spectrum can be expressed in terms of wavelength, frequency, or energy. Wavelength (l ) and frequency (n) are related by the expression
l n
= c (2-1)
where c is the speed of light (2 998 108
. * m/s). Figure 2.11 shows a schematic representation of one wavelength. The energy of the various components of the electromagnetic spectrum is given by the expression
E = hn (2-2)
where h is Planck’s constant. The units of wavelength are meters, with the terms microns (denoted mm and equal to 10−6 m) and nanometers (denoted nm and equal to 10−9 m) being used just as frequently. Frequency is measured in Hertz (Hz), with one Hz being equal to one cycle of a sinusoidal wave per second. A commonly used unit of energy is the electron-volt. Electromagnetic waves can be visualized as propagating sinusoidal waves with wavelength l (Fig. 2.11), or they can be thought of as a stream of massless particles,
ab cd
FIGURE 2.9 Some well-known optical illusions.
www.EBooksWorld.ir


56 Chapter 2 Digital Image Fundamentals
each traveling in a wavelike pattern and moving at the speed of light. Each massless particle contains a certain amount (or bundle) of energy, called a photon. We see from Eq. (2-2) that energy is proportional to frequency, so the higher-frequency (shorter wavelength) electromagnetic phenomena carry more energy per photon. Thus, radio waves have photons with low energies, microwaves have more energy than radio waves, infrared still more, then visible, ultraviolet, X-rays, and finally gamma rays, the most energetic of all. High-energy electromagnetic radiation, especially in the X-ray and gamma ray bands, is particularly harmful to living organisms. Light is a type of electromagnetic radiation that can be sensed by the eye. The visible (color) spectrum is shown expanded in Fig. 2.10 for the purpose of discussion (we will discuss color in detail in Chapter 6). The visible band of the electromagnetic spectrum spans the range from approximately 0.43 mm (violet) to about 0.79 mm (red). For convenience, the color spectrum is divided into six broad regions: violet, blue, green, yellow, orange, and red. No color (or other component of the
Infrared Microwaves Radio waves
Visible spectrum
Gamma rays X-rays Ultraviolet
0.4 10 6 0.5 10 6 0.6 10 6 0.7 10 6 Ultraviolet Violet Blue Green Yellow Orange Red Infrared
105
106
107
108
109
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
Frequency (Hz)
10 9
10 8
10 7
10 6
10 5
10 4
10 3
10 2
10 1
1
101
102
103
104
105
106
Energy of one photon (electron volts)
103
102
101
1
10 1
10 2
10 3
10 4
10 5
10 6
10 7
10 8
10 9
10 10
10 11
10 12
Wavelength (meters)
FIGURE 2.10 The electromagnetic spectrum. The visible spectrum is shown zoomed to facilitate explanations, but note that it encompasses a very narrow range of the total EM spectrum.
l
FIGURE 2.11 Graphical representation of one wavelength.
www.EBooksWorld.ir


2.3 Image Sensing and Acquisition 57
electromagnetic spectrum) ends abruptly; rather, each range blends smoothly into the next, as Fig. 2.10 shows. The colors perceived in an object are determined by the nature of the light reflected by the object. A body that reflects light relatively balanced in all visible wavelengths appears white to the observer. However, a body that favors reflectance in a limited range of the visible spectrum exhibits some shades of color. For example, green objects reflect light with wavelengths primarily in the 500 to 570 nm range, while absorbing most of the energy at other wavelengths. Light that is void of color is called monochromatic (or achromatic) light. The only attribute of monochromatic light is its intensity. Because the intensity of monochromatic light is perceived to vary from black to grays and finally to white, the term gray level is used commonly to denote monochromatic intensity (we use the terms intensity and gray level interchangeably in subsequent discussions). The range of values of monochromatic light from black to white is usually called the gray scale, and monochromatic images are frequently referred to as grayscale images. Chromatic (color) light spans the electromagnetic energy spectrum from approximately 0.43 to 0.79 mm, as noted previously. In addition to frequency, three other quantities are used to describe a chromatic light source: radiance, luminance, and brightness. Radiance is the total amount of energy that flows from the light source, and it is usually measured in watts (W). Luminance, measured in lumens (lm), gives a measure of the amount of energy an observer perceives from a light source. For example, light emitted from a source operating in the far infrared region of the spectrum could have significant energy (radiance), but an observer would hardly perceive it; its luminance would be almost zero. Finally, as discussed in Section 2.1, brightness is a subjective descriptor of light perception that is practically impossible to measure. It embodies the achromatic notion of intensity and is one of the key factors in describing color sensation. In principle, if a sensor can be developed that is capable of detecting energy radiated in a band of the electromagnetic spectrum, we can image events of interest in that band. Note, however, that the wavelength of an electromagnetic wave required to “see” an object must be of the same size as, or smaller than, the object. For example, a water molecule has a diameter on the order of 10−10 m. Thus, to study these molecules, we would need a source capable of emitting energy in the far (highenergy) ultraviolet band or soft (low-energy) X-ray bands. Although imaging is based predominantly on energy from electromagnetic wave radiation, this is not the only method for generating images. For example, we saw in Section 1.3 that sound reflected from objects can be used to form ultrasonic images. Other sources of digital images are electron beams for electron microscopy, and software for generating synthetic images used in graphics and visualization.
2.3 IMAGE SENSING AND ACQUISITION
Most of the images in which we are interested are generated by the combination of an “illumination” source and the reflection or absorption of energy from that source by the elements of the “scene” being imaged. We enclose illumination and scene in quotes to emphasize the fact that they are considerably more general than the
2.3
www.EBooksWorld.ir


58 Chapter 2 Digital Image Fundamentals
familiar situation in which a visible light source illuminates a familiar 3-D scene. For example, the illumination may originate from a source of electromagnetic energy, such as a radar, infrared, or X-ray system. But, as noted earlier, it could originate from less traditional sources, such as ultrasound or even a computer-generated illumination pattern. Similarly, the scene elements could be familiar objects, but they can just as easily be molecules, buried rock formations, or a human brain. Depending on the nature of the source, illumination energy is reflected from, or transmitted through, objects. An example in the first category is light reflected from a planar surface. An example in the second category is when X-rays pass through a patient’s body for the purpose of generating a diagnostic X-ray image. In some applications, the reflected or transmitted energy is focused onto a photo converter (e.g., a phosphor screen) that converts the energy into visible light. Electron microscopy and some applications of gamma imaging use this approach. Figure 2.12 shows the three principal sensor arrangements used to transform incident energy into digital images. The idea is simple: Incoming energy is transformed into a voltage by a combination of the input electrical power and sensor material that is responsive to the type of energy being detected. The output voltage waveform is the response of the sensor, and a digital quantity is obtained by digitizing that response. In this section, we look at the principal modalities for image sensing and generation. We will discuss image digitizing in Section 2.4.
IMAGE ACQUISITION USING A SINGLE SENSING ELEMENT
Figure 2.12(a) shows the components of a single sensing element. A familiar sensor of this type is the photodiode, which is constructed of silicon materials and whose output is a voltage proportional to light intensity. Using a filter in front of a sensor improves its selectivity. For example, an optical green-transmission filter favors light in the green band of the color spectrum. As a consequence, the sensor output would be stronger for green light than for other visible light components. In order to generate a 2-D image using a single sensing element, there has to be relative displacements in both the x- and y-directions between the sensor and the area to be imaged. Figure 2.13 shows an arrangement used in high-precision scanning, where a film negative is mounted onto a drum whose mechanical rotation provides displacement in one dimension. The sensor is mounted on a lead screw that provides motion in the perpendicular direction. A light source is contained inside the drum. As the light passes through the film, its intensity is modified by the film density before it is captured by the sensor. This "modulation" of the light intensity causes corresponding variations in the sensor voltage, which are ultimately converted to image intensity levels by digitization. This method is an inexpensive way to obtain high-resolution images because mechanical motion can be controlled with high precision. The main disadvantages of this method are that it is slow and not readily portable. Other similar mechanical arrangements use a flat imaging bed, with the sensor moving in two linear directions. These types of mechanical digitizers sometimes are referred to as transmission microdensitometers. Systems in which light is reflected from the medium, instead of passing through it, are called reflection microdensitometers. Another example of imaging with a single sensing element places a laser source coincident with the
www.EBooksWorld.ir


2.3 Image Sensing and Acquisition 59
Sensing material
Voltage waveform out
Filter
Energy
Power in
Housing
b
a
c
FIGURE 2.12
(a) Single sensing element. (b) Line sensor. (c) Array sensor.
Sensor
Linear motion
One image line out per increment of rotation and full linear displacement of sensor from left to right
Film
Rotation
FIGURE 2.13
Combining a single sensing element with mechanical motion to generate a 2-D image.
www.EBooksWorld.ir


60 Chapter 2 Digital Image Fundamentals
sensor. Moving mirrors are used to control the outgoing beam in a scanning pattern and to direct the reflected laser signal onto the sensor.
IMAGE ACQUISITION USING SENSOR STRIPS
A geometry used more frequently than single sensors is an in-line sensor strip, as in Fig. 2.12(b). The strip provides imaging elements in one direction. Motion perpendicular to the strip provides imaging in the other direction, as shown in Fig. 2.14(a). This arrangement is used in most flat bed scanners. Sensing devices with 4000 or more in-line sensors are possible. In-line sensors are used routinely in airborne imaging applications, in which the imaging system is mounted on an aircraft that flies at a constant altitude and speed over the geographical area to be imaged. Onedimensional imaging sensor strips that respond to various bands of the electromagnetic spectrum are mounted perpendicular to the direction of flight. An imaging strip gives one line of an image at a time, and the motion of the strip relative to the scene completes the other dimension of a 2-D image. Lenses or other focusing schemes are used to project the area to be scanned onto the sensors. Sensor strips in a ring configuration are used in medical and industrial imaging to obtain cross-sectional (“slice”) images of 3-D objects, as Fig. 2.14(b) shows. A rotating X-ray source provides illumination, and X-ray sensitive sensors opposite the source collect the energy that passes through the object. This is the basis for medical and industrial computerized axial tomography (CAT) imaging, as indicated in Sections 1.2 and 1.3. The output of the sensors is processed by reconstruction algorithms whose objective is to transform the sensed data into meaningful crosssectional images (see Section 5.11). In other words, images are not obtained directly
Sensor strip
Linear motion
Imaged area
One image line out per increment of linear motion
Image reconstruction
3-D object
Linearmotion
Sensor ring
X-ray source
Cross-sectional images of 3-D object
Source
rotation
ab
FIGURE 2.14 (a) Image acquisition using a linear sensor strip. (b) Image acquisition using a circular sensor strip.
www.EBooksWorld.ir


2.3 Image Sensing and Acquisition 61
from the sensors by motion alone; they also require extensive computer processing. A 3-D digital volume consisting of stacked images is generated as the object is moved in a direction perpendicular to the sensor ring. Other modalities of imaging based on the CAT principle include magnetic resonance imaging (MRI) and positron emission tomography (PET). The illumination sources, sensors, and types of images are different, but conceptually their applications are very similar to the basic imaging approach shown in Fig. 2.14(b).
IMAGE ACQUISITION USING SENSOR ARRAYS
Figure 2.12(c) shows individual sensing elements arranged in the form of a 2-D array. Electromagnetic and ultrasonic sensing devices frequently are arranged in this manner. This is also the predominant arrangement found in digital cameras. A typical sensor for these cameras is a CCD (charge-coupled device) array, which can be manufactured with a broad range of sensing properties and can be packaged in rugged arrays of 4000 * 4000 elements or more. CCD sensors are used widely in digital cameras and other light-sensing instruments. The response of each sensor is proportional to the integral of the light energy projected onto the surface of the sensor, a property that is used in astronomical and other applications requiring low noise images. Noise reduction is achieved by letting the sensor integrate the input light signal over minutes or even hours. Because the sensor array in Fig. 2.12(c) is twodimensional, its key advantage is that a complete image can be obtained by focusing the energy pattern onto the surface of the array. Motion obviously is not necessary, as is the case with the sensor arrangements discussed in the preceding two sections. Figure 2.15 shows the principal manner in which array sensors are used. This figure shows the energy from an illumination source being reflected from a scene (as mentioned at the beginning of this section, the energy also could be transmitted through the scene). The first function performed by the imaging system in Fig. 2.15(c) is to collect the incoming energy and focus it onto an image plane. If the illumination is light, the front end of the imaging system is an optical lens that projects the viewed scene onto the focal plane of the lens, as Fig. 2.15(d) shows. The sensor array, which is coincident with the focal plane, produces outputs proportional to the integral of the light received at each sensor. Digital and analog circuitry sweep these outputs and convert them to an analog signal, which is then digitized by another section of the imaging system. The output is a digital image, as shown diagrammatically in Fig. 2.15(e). Converting images into digital form is the topic of Section 2.4.
A SIMPLE IMAGE FORMATION MODEL
As introduced in Section 1.1, we denote images by two-dimensional functions of the form f (x, y). The value of f at spatial coordinates (x, y) is a scalar quantity whose physical meaning is determined by the source of the image, and whose values are proportional to energy radiated by a physical source (e.g., electromagnetic waves). As a consequence, f (x, y) must be nonnegative† and finite; that is,
† Image intensities can become negative during processing, or as a result of interpretation. For example, in radar images, objects moving toward the radar often are interpreted as having negative velocities while objects moving away are interpreted as having positive velocities. Thus, a velocity image might be coded as having both positive and negative values. When storing and displaying images, we normally scale the intensities so that the smallest negative value becomes 0 (see Section 2.6 regarding intensity scaling).
In some cases, the source is imaged directly, as in obtaining images of the sun.
www.EBooksWorld.ir


62 Chapter 2 Digital Image Fundamentals
0 ≤ f (x, y) < (2-3)
Function f (x, y) is characterized by two components: (1) the amount of source illumination incident on the scene being viewed, and (2) the amount of illumination reflected by the objects in the scene. Appropriately, these are called the illumination and reflectance components, and are denoted by i(x, y) and r(x, y), respectively. The two functions combine as a product to form f (x, y):
f (x, y) = i(x, y)r(x, y) (2-4)
where
0 ≤ i(x, y) < (2-5)
and
0 ≤ r(x, y) ≤ 1 (2-6)
Thus, reflectance is bounded by 0 (total absorption) and 1 (total reflectance). The nature of i(x, y) is determined by the illumination source, and r(x, y) is determined by the characteristics of the imaged objects. These expressions are applicable also to images formed via transmission of the illumination through a medium, such as a
Illumination (energy) source
Imaging system
(Internal) image plane
Output (digitized) image
Scene
b
a c de
FIGURE 2.15 An example of digital image acquisition. (a) Illumination (energy) source. (b) A scene. (c) Imaging system. (d) Projection of the scene onto the image plane. (e) Digitized image.
www.EBooksWorld.ir


2.4 Image Sampling and Quantization 63
chest X-ray. In this case, we would deal with a transmissivity instead of a reflectivity function, but the limits would be the same as in Eq. (2-6), and the image function formed would be modeled as the product in Eq. (2-4).
EXAMPLE 2.1: Some typical values of illumination and reflectance.
The following numerical quantities illustrate some typical values of illumination and reflectance for visible light. On a clear day, the sun may produce in excess of 90, 000 lm/m2 of illumination on the surface of the earth. This value decreases to less than 10, 000 lm/m2 on a cloudy day. On a clear evening, a full moon yields about 0.1 lm/m2 of illumination. The typical illumination level in a commercial office is about 1, 000 lm/m2. Similarly, the following are typical values of r(x, y): 0.01 for black velvet, 0.65 for stainless steel, 0.80 for flat-white wall paint, 0.90 for silver-plated metal, and 0.93 for snow.
Let the intensity (gray level) of a monochrome image at any coordinates (x, y) be denoted by
/ = f (x, y) (2-7)
From Eqs. (2-4) through (2-6) it is evident that / lies in the range
LL
min max
≤ / ≤ (2-8)
In theory, the requirement on Lmin is that it be nonnegative, and on Lmax that it be finite. In practice, L i r
min min min
= and L i r
max max max
= . From Example 2.1, using average office illumination and reflectance values as guidelines, we may expect Lmin ≈ 10 and Lmax ≈ 1000 to be typical indoor values in the absence of additional
illumination. The units of these quantities are lum/m2. However, actual units seldom are of interest, except in cases where photometric measurements are being performed. The interval [ , ]
min max
L L is called the intensity (or gray) scale. Common practice is to shift this interval numerically to the interval [0, 1], or [0,C], where / = 0 is considered black and / = 1 (or C) is considered white on the scale. All intermediate values are shades of gray varying from black to white.
2.4 IMAGE SAMPLING AND QUANTIZATION
As discussed in the previous section, there are numerous ways to acquire images, but our objective in all is the same: to generate digital images from sensed data. The output of most sensors is a continuous voltage waveform whose amplitude and spatial behavior are related to the physical phenomenon being sensed. To create a digital image, we need to convert the continuous sensed data into a digital format. This requires two processes: sampling and quantization.
BASIC CONCEPTS IN SAMPLING AND QUANTIZATION
Figure 2.16(a) shows a continuous image f that we want to convert to digital form. An image may be continuous with respect to the x- and y-coordinates, and also in
2.4
The discussion of sampling in this section is of an intuitive nature. We will discuss this topic in depth in Chapter 4.
www.EBooksWorld.ir


64 Chapter 2 Digital Image Fundamentals
amplitude. To digitize it, we have to sample the function in both coordinates and also in amplitude. Digitizing the coordinate values is called sampling. Digitizing the amplitude values is called quantization. The one-dimensional function in Fig. 2.16(b) is a plot of amplitude (intensity level) values of the continuous image along the line segment AB in Fig. 2.16(a). The random variations are due to image noise. To sample this function, we take equally spaced samples along line AB, as shown in Fig. 2.16(c). The samples are shown as small dark squares superimposed on the function, and their (discrete) spatial locations are indicated by corresponding tick marks in the bottom of the figure. The set of dark squares constitute the sampled function. However, the values of the samples still span (vertically) a continuous range of intensity values. In order to form a digital function, the intensity values also must be converted (quantized) into discrete quantities. The vertical gray bar in Fig. 2.16(c) depicts the intensity scale divided into eight discrete intervals, ranging from black to white. The vertical tick marks indicate the specific value assigned to each of the eight intensity intervals. The continuous intensity levels are quantized by assigning one of the eight values to each sample, depending on the vertical proximity of a sample to a vertical tick mark. The digital samples resulting from both sampling and quantization are shown as white squares in Fig. 2.16(d). Starting at the top of the continuous image and carrying out this procedure downward, line by line, produces a two-dimensional digital image. It is implied in Fig. 2.16 that, in addition to the number of discrete levels used, the accuracy achieved in quantization is highly dependent on the noise content of the sampled signal.
ab cd
FIGURE 2.16
(a) Continuous image. (b) A scan line showing intensity variations along line AB in the continuous image. (c) Sampling and quantization. (d) Digital scan line. (The black border in (a) is included for clarity. It is not part of the image).
AB
AB
Sampling
A BA B
Quantization
www.EBooksWorld.ir


2.4 Image Sampling and Quantization 65
In practice, the method of sampling is determined by the sensor arrangement used to generate the image. When an image is generated by a single sensing element combined with mechanical motion, as in Fig. 2.13, the output of the sensor is quantized in the manner described above. However, spatial sampling is accomplished by selecting the number of individual mechanical increments at which we activate the sensor to collect data. Mechanical motion can be very exact so, in principle, there is almost no limit on how fine we can sample an image using this approach. In practice, limits on sampling accuracy are determined by other factors, such as the quality of the optical components used in the system. When a sensing strip is used for image acquisition, the number of sensors in the strip establishes the samples in the resulting image in one direction, and mechanical motion establishes the number of samples in the other. Quantization of the sensor outputs completes the process of generating a digital image. When a sensing array is used for image acquisition, no motion is required. The number of sensors in the array establishes the limits of sampling in both directions. Quantization of the sensor outputs is as explained above. Figure 2.17 illustrates this concept. Figure 2.17(a) shows a continuous image projected onto the plane of a 2-D sensor. Figure 2.17(b) shows the image after sampling and quantization. The quality of a digital image is determined to a large degree by the number of samples and discrete intensity levels used in sampling and quantization. However, as we will show later in this section, image content also plays a role in the choice of these parameters.
REPRESENTING DIGITAL IMAGES
Let f (s, t) represent a continuous image function of two continuous variables, s and t. We convert this function into a digital image by sampling and quantization, as explained in the previous section. Suppose that we sample the continuous image into a digital image, f (x, y), containing M rows and N columns, where (x, y) are discrete coordinates. For notational clarity and convenience, we use integer values for these discrete coordinates: x = 0, 1, 2, ... , M − 1 and y = 0, 1, 2, ... , N − 1. Thus, for example, the value of the digital image at the origin is f (0, 0), and its value at the next coordinates along the first row is f (0,1). Here, the notation (0, 1) is used
ab
FIGURE 2.17
(a) Continuous image projected onto a sensor array. (b) Result of image sampling and quantization.
www.EBooksWorld.ir


66 Chapter 2 Digital Image Fundamentals
to denote the second sample along the first row. It does not mean that these are the values of the physical coordinates when the image was sampled. In general, the value of a digital image at any coordinates (x, y) is denoted f (x, y), where x and y are integers. When we need to refer to specific coordinates (i, j), we use the notation f (i, j), where the arguments are integers. The section of the real plane spanned by the coordinates of an image is called the spatial domain, with x and y being referred to as spatial variables or spatial coordinates.
Figure 2.18 shows three ways of representing f (x, y). Figure 2.18(a) is a plot of the function, with two axes determining spatial location and the third axis being the values of f as a function of x and y. This representation is useful when working with grayscale sets whose elements are expressed as triplets of the form (x, y, z), where x and y are spatial coordinates and z is the value of f at coordinates (x, y). We will work with this representation briefly in Section 2.6. The representation in Fig. 2.18(b) is more common, and it shows f (x, y) as it would appear on a computer display or photograph. Here, the intensity of each point in the display is proportional to the value of f at that point. In this figure, there are only three equally spaced intensity values. If the intensity is normalized to the interval [0,1], then each point in the image has the value 0, 0.5, or 1. A monitor or printer converts these three values to black, gray, or white, respectively, as in Fig. 2.18(b). This type of representation includes color images, and allows us to view results at a glance. As Fig. 2.18(c) shows, the third representation is an array (matrix) composed of the numerical values of f (x, y). This is the representation used for computer processing. In equation form, we write the representation of an M * N numerical array as
f xy
f f fN
f f fN
fM
(, )
(, ) (,) (, )
(, ) (, ) (, )
(,
=
− −
−
00 01 0 1
10 11 1 1
1 0) f (M − 1,1) f (M − 1, N − 1)
⎡
⎣
⎢⎢⎢⎢
⎤
⎦
⎥⎥⎥⎥
(2-9)
The right side of this equation is a digital image represented as an array of real numbers. Each element of this array is called an image element, picture element, pixel, or pel. We use the terms image and pixel throughout the book to denote a digital image and its elements. Figure 2.19 shows a graphical representation of an image array, where the x- and y-axis are used to denote the rows and columns of the array. Specific pixels are values of the array at a fixed pair of coordinates. As mentioned earlier, we generally use f (i, j) when referring to a pixel with coordinates (i, j). We can also represent a digital image in a traditional matrix form:
A=
⎡
⎣
−
−
− − −−
aa a
aa a
aa a
N
N
M M MN
00 01 0 1
10 11 1 1
10 11 1 1
,, ,
,, ,
,, ,
⎢⎢⎢⎢⎢
⎤
⎦
⎥⎥⎥⎥⎥
(2-10)
Clearly, a f i j
ij = ( , ), so Eqs. (2-9) and (2-10) denote identical arrays.
www.EBooksWorld.ir


2.4 Image Sampling and Quantization 67
As Fig. 2.19 shows, we define the origin of an image at the top left corner. This is a convention based on the fact that many image displays (e.g., TV monitors) sweep an image starting at the top left and moving to the right, one row at a time. More important is the fact that the first element of a matrix is by convention at the top left of the array. Choosing the origin of f (x, y) at that point makes sense mathematically because digital images in reality are matrices. In fact, as you will see, sometimes we use x and y interchangeably in equations with the rows (r) and columns (c) of a matrix. It is important to note that the representation in Fig. 2.19, in which the positive x-axis extends downward and the positive y-axis extends to the right, is precisely the right-handed Cartesian coordinate system with which you are familiar,† but shown rotated by 90° so that the origin appears on the top, left.
†Recall that a right-handed coordinate system is such that, when the index of the right hand points in the direction of the positive x-axis and the middle finger points in the (perpendicular) direction of the positive y-axis, the thumb points up. As Figs. 2.18 and 2.19 show, this indeed is the case in our image coordinate system. In practice, you will also find implementations based on a left-handed system, in which the x- and y-axis are interchanged from the way we show them in Figs. 2.18 and 2.19. For example, MATLAB uses a left-handed system for image processing. Both systems are perfectly valid, provided they are used consistently.
x
y
f (x, y)
.5
y
x
Origin
0 0 0 0 0 0
0 0 0 0 0
0 0
0 0
0 0
0 0
0
1
1
11 1
.5
.5 .5
.5 .5
.5
b
a c
FIGURE 2.18
(a) Image plotted as a surface. (b) Image displayed as a visual intensity array. (c) Image shown as a 2-D numerical array. (The numbers 0, .5, and 1 represent black, gray, and white, respectively.)
www.EBooksWorld.ir


68 Chapter 2 Digital Image Fundamentals
The center of an M × N digital image with origin at(0, 0) and range to (M − 1, N − 1) is obtained by dividing M and N by 2 and rounding down to the nearest integer. This operation sometimes is denoted using the floor operator, JiK, as shown in Fig. 2.19. This holds true for M and N even or odd. For example, the center of an image of size 1023 × 1024 is at (511, 512). Some programming languages (e.g., MATLAB) start indexing at 1 instead of at 0. The center of an image in that case is found at (x , y ) (M ) , (N ) .
c c= + +
()
floor 2 1 floor 2 1 To express sampling and quantization in more formal mathematical terms, let Z and R denote the set of integers and the set of real numbers, respectively. The sampling process may be viewed as partitioning the xy-plane into a grid, with the coordinates of the center of each cell in the grid being a pair of elements from the Cartesian product Z2 (also denoted Z × Z) which, as you may recall, is the set of all ordered pairs of elements (zi, zj ) with zi and zj being integers from set Z. Hence,
f (x, y) is a digital image if (x, y) are integers from Z2 and f is a function that assigns an intensity value (that is, a real number from the set of real numbers, R) to each distinct pair of coordinates (x, y). This functional assignment is the quantization process described earlier. If the intensity levels also are integers, then R = Z, and a digital image becomes a 2-D function whose coordinates and amplitude values are integers. This is the representation we use in the book. Image digitization requires that decisions be made regarding the values for M, N, and for the number, L, of discrete intensity levels. There are no restrictions placed on M and N, other than they have to be positive integers. However, digital storage and quantizing hardware considerations usually lead to the number of intensity levels, L, being an integer power of two; that is
Lk
= 2 (2-11)
where k is an integer. We assume that the discrete levels are equally spaced and that they are integers in the range [0, L − 1].
The floor of z, sometimes denoted JzK, is the largest integer that is less than or equal to z. The ceiling of z, denoted LzM, is the smallest integer that is greater than or equal to z.
See Eq. (2-41) in Section 2.6 for a formal definition of the Cartesian product.
FIGURE 2.19 Coordinate convention used to represent digital images. Because coordinate values are integers, there is a one-to-one correspondence between x and y and the rows (r) and columns (c) of a matrix.
Origin
0 N-1
M- 1
0y
x
i
j
pixel f(i, j)
Image f(x, y)
1
1
2
Center
The coordinates of the image center are
xc
yc
xc, yc = N
Q2R
floor
M
Q2R
floor ,
ab
AB
www.EBooksWorld.ir


2.4 Image Sampling and Quantization 69
Sometimes, the range of values spanned by the gray scale is referred to as the dynamic range, a term used in different ways in different fields. Here, we define the dynamic range of an imaging system to be the ratio of the maximum measurable intensity to the minimum detectable intensity level in the system. As a rule, the upper limit is determined by saturation and the lower limit by noise, although noise can be present also in lighter intensities. Figure 2.20 shows examples of saturation and slight visible noise. Because the darker regions are composed primarily of pixels with the minimum detectable intensity, the background in Fig. 2.20 is the noisiest part of the image; however, dark background noise typically is much harder to see. The dynamic range establishes the lowest and highest intensity levels that a system can represent and, consequently, that an image can have. Closely associated with this concept is image contrast, which we define as the difference in intensity between the highest and lowest intensity levels in an image. The contrast ratio is the ratio of these two quantities. When an appreciable number of pixels in an image have a high dynamic range, we can expect the image to have high contrast. Conversely, an image with low dynamic range typically has a dull, washed-out gray look. We will discuss these concepts in more detail in Chapter 3. The number, b, of bits required to store a digital image is
b = M * N * k (2-12)
When M = N, this equation becomes
b = N 2k (2-13)
Noise
Saturation
FIGURE 2.20
An image exhibiting saturation and noise. Saturation is the highest value beyond which all intensity values are clipped (note how the entire saturated area has a high, constant intensity level). Visible noise in this case appears as a grainy texture pattern. The dark background is noisier, but the noise is difficult to see.
www.EBooksWorld.ir


70 Chapter 2 Digital Image Fundamentals
Figure 2.21 shows the number of megabytes required to store square images for various values of N and k (as usual, one byte equals 8 bits and a megabyte equals 106 bytes). When an image can have 2k possible intensity levels, it is common practice to refer to it as a “k-bit image,” (e,g., a 256-level image is called an 8-bit image). Note that storage requirements for large 8-bit images (e.g., 10, 000 * 10, 000 pixels) are not insignificant.
LINEAR VS. COORDINATE INDEXING
The convention discussed in the previous section, in which the location of a pixel is given by its 2-D coordinates, is referred to as coordinate indexing, or subscript indexing. Another type of indexing used extensively in programming image processing algorithms is linear indexing, which consists of a 1-D string of nonnegative integers based on computing offsets from coordinates (0, 0). There are two principal types of linear indexing, one is based on a row scan of an image, and the other on a column scan. Figure 2.22 illustrates the principle of linear indexing based on a column scan. The idea is to scan an image column by column, starting at the origin and proceeding down and then to the right. The linear index is based on counting pixels as we scan the image in the manner shown in Fig. 2.22.Thus, a scan of the first (leftmost) column yields linear indices 0 through M − 1. A scan of the second column yields indices M through 2M − 1, and so on, until the last pixel in the last column is assigned the linear index value MN − 1. Thus, a linear index, denoted by a , has one of MN possible values: 0, 1, 2, ... , MN − 1, as Fig. 2.22 shows. The important thing to notice here is that each pixel is assigned a linear index value that identifies it uniquely. The formula for generating linear indices based on a column scan is straightforward and can be determined by inspection. For any pair of coordinates (x, y), the corresponding linear index value is
a = My + x (2-14)
N
* 103
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 6 7 8 9 10
k=8
7
6
5
4
3
2
1
00
Megabytes ( )
*
b
8 106
FIGURE 2.21 Number of megabytes required to store images for various values of N and k.
www.EBooksWorld.ir


2.4 Image Sampling and Quantization 71
Conversely, the coordinate indices for a given linear index value a are given by the equations†
x = a mod M (2-15)
and
y = ( x) M
a - (2-16)
Recall that a mod M means “the remainder of the division of a by M.” This is a formal way of stating that row numbers repeat themselves at the start of every column. Thus, when a = 0, the remainder of the division of 0 by M is 0, so x = 0. When a = 1, the remainder is 1, and so x = 1. You can see that x will continue to be equal to a until a = M − 1. When a = M (which is at the beginning of the second column), the remainder is 0, and thus x = 0 again, and it increases by 1 until the next column is reached, when the pattern repeats itself. Similar comments apply to Eq. (2-16). See Problem 2.11 for a derivation of the preceding two equations.
SPATIAL AND INTENSITY RESOLUTION
Intuitively, spatial resolution is a measure of the smallest discernible detail in an image. Quantitatively, spatial resolution can be stated in several ways, with line pairs per unit distance, and dots (pixels) per unit distance being common measures. Suppose that we construct a chart with alternating black and white vertical lines, each of width W units (W can be less than 1). The width of a line pair is thus 2W, and there are W 2 line pairs per unit distance. For example, if the width of a line is 0.1 mm, there are 5 line pairs per unit distance (i.e., per mm). A widely used definition of image resolution is the largest number of discernible line pairs per unit distance (e.g., 100 line pairs per mm). Dots per unit distance is a measure of image resolution used in the printing and publishing industry. In the U.S., this measure usually is expressed as dots per inch (dpi). To give you an idea of quality, newspapers are printed with a
†When working with modular number systems, it is more accurate to write x ≡ a mod M , where the symbol ≡ means congruence. However, our interest here is just on converting from linear to coordinate indexing, so we use the more familiar equal sign.
x
y
Image f(x, y)
(0, 0) α = 0
(M - 1, 0) α = M - 1 (M - 1, N - 1) α = MN - 1
(0, 1) α = M (0, 2) α = 2M
(M - 1, 1) α = 2M - 1
x, y)
FIGURE 2.22
Illustration of column scanning for generating linear indices. Shown are several 2-D coordinates (in parentheses) and their corresponding linear indices.
www.EBooksWorld.ir


72 Chapter 2 Digital Image Fundamentals
resolution of 75 dpi, magazines at 133 dpi, glossy brochures at 175 dpi, and the book page at which you are presently looking was printed at 2400 dpi. To be meaningful, measures of spatial resolution must be stated with respect to spatial units. Image size by itself does not tell the complete story. For example, to say that an image has a resolution of 1024 * 1024 pixels is not a meaningful statement without stating the spatial dimensions encompassed by the image. Size by itself is helpful only in making comparisons between imaging capabilities. For instance, a digital camera with a 20-megapixel CCD imaging chip can be expected to have a higher capability to resolve detail than an 8-megapixel camera, assuming that both cameras are equipped with comparable lenses and the comparison images are taken at the same distance. Intensity resolution similarly refers to the smallest discernible change in intensity level. We have considerable discretion regarding the number of spatial samples (pixels) used to generate a digital image, but this is not true regarding the number of intensity levels. Based on hardware considerations, the number of intensity levels usually is an integer power of two, as we mentioned when discussing Eq. (2-11). The most common number is 8 bits, with 16 bits being used in some applications in which enhancement of specific intensity ranges is necessary. Intensity quantization using 32 bits is rare. Sometimes one finds systems that can digitize the intensity levels of an image using 10 or 12 bits, but these are not as common. Unlike spatial resolution, which must be based on a per-unit-of-distance basis to be meaningful, it is common practice to refer to the number of bits used to quantize intensity as the “intensity resolution.” For example, it is common to say that an image whose intensity is quantized into 256 levels has 8 bits of intensity resolution. However, keep in mind that discernible changes in intensity are influenced also by noise and saturation values, and by the capabilities of human perception to analyze and interpret details in the context of an entire scene (see Section 2.1).The following two examples illustrate the effects of spatial and intensity resolution on discernible detail. Later in this section, we will discuss how these two parameters interact in determining perceived image quality.
EXAMPLE 2.2: Effects of reducing the spatial resolution of a digital image.
Figure 2.23 shows the effects of reducing the spatial resolution of an image. The images in Figs. 2.23(a) through (d) have resolutions of 930, 300, 150, and 72 dpi, respectively. Naturally, the lower resolution images are smaller than the original image in (a). For example, the original image is of size 2136 * 2140 pixels, but the 72 dpi image is an array of only 165 * 166 pixels. In order to facilitate comparisons, all the smaller images were zoomed back to the original size (the method used for zooming will be discussed later in this section).This is somewhat equivalent to “getting closer” to the smaller images so that we can make comparable statements about visible details. There are some small visual differences between Figs. 2.23(a) and (b), the most notable being a slight distortion in the seconds marker pointing to 60 on the right side of the chronometer. For the most part, however, Fig. 2.23(b) is quite acceptable. In fact, 300 dpi is the typical minimum image spatial resolution used for book publishing, so one would not expect to see much difference between these two images. Figure 2.23(c) begins to show visible degradation (see, for example, the outer edges of the chronometer
www.EBooksWorld.ir


2.4 Image Sampling and Quantization 73
case and compare the seconds marker with the previous two images). The numbers also show visible degradation. Figure 2.23(d) shows degradation that is visible in most features of the image. When printing at such low resolutions, the printing and publishing industry uses a number of techniques (such as locally varying the pixel size) to produce much better results than those in Fig. 2.23(d). Also, as we will show later in this section, it is possible to improve on the results of Fig. 2.23 by the choice of interpolation method used.
EXAMPLE 2.3: Effects of varying the number of intensity levels in a digital image.
Figure 2.24(a) is a 774 × 640 CT projection image, displayed using 256 intensity levels (see Chapter 1 regarding CT images). The objective of this example is to reduce the number of intensities of the image from 256 to 2 in integer powers of 2, while keeping the spatial resolution constant. Figures 2.24(b) through (d) were obtained by reducing the number of intensity levels to 128, 64, and 32, respectively (we will discuss in Chapter 3 how to reduce the number of levels).
ab cd
FIGURE 2.23 Effects of reducing spatial resolution. The images shown are at: (a) 930 dpi, (b) 300 dpi, (c) 150 dpi, and (d) 72 dpi.
www.EBooksWorld.ir


74 Chapter 2 Digital Image Fundamentals
The 128- and 64-level images are visually identical for all practical purposes. However, the 32-level image in Fig. 2.24(d) has a set of almost imperceptible, very fine ridge-like structures in areas of constant intensity. These structures are clearly visible in the 16-level image in Fig. 2.24(e). This effect, caused by using an insufficient number of intensity levels in smooth areas of a digital image, is called false contouring, so named because the ridges resemble topographic contours in a map. False contouring generally is quite objectionable in images displayed using 16 or fewer uniformly spaced intensity levels, as the images in Figs. 2.24(e)-(h) show. As a very rough guideline, and assuming integer powers of 2 for convenience, images of size 256 * 256 pixels with 64 intensity levels, and printed on a size format on the order of 5 * 5 cm, are about the lowest spatial and intensity resolution images that can be expected to be reasonably free of objectionable sampling distortions and false contouring.
ab cd
FIGURE 2.24
(a) 774 × 640, 256-level image. (b)-(d) Image displayed in 128, 64, and 32 intensity levels, while keeping the spatial resolution constant. (Original image courtesy of the Dr. David R. Pickens, Department of Radiology & Radiological Sciences, Vanderbilt University Medical Center.)
www.EBooksWorld.ir


2.4 Image Sampling and Quantization 75
The results in Examples 2.2 and 2.3 illustrate the effects produced on image quality by varying spatial and intensity resolution independently. However, these results did not consider any relationships that might exist between these two parameters. An early study by Huang [1965] attempted to quantify experimentally the effects on image quality produced by the interaction of these two variables. The experiment consisted of a set of subjective tests. Images similar to those shown in Fig. 2.25 were used. The woman’s face represents an image with relatively little detail; the picture of the cameraman contains an intermediate amount of detail; and the crowd picture contains, by comparison, a large amount of detail. Sets of these three types of images of various sizes and intensity resolution were generated by varying N and k [see Eq. (2-13)]. Observers were then asked to rank
ef
gh
FIGURE 2.24 (Continued) (e)-(h) Image displayed in 16, 8, 4, and 2 intensity levels.
www.EBooksWorld.ir


76 Chapter 2 Digital Image Fundamentals
them according to their subjective quality. Results were summarized in the form of so-called isopreference curves in the Nk-plane. (Figure 2.26 shows average isopreference curves representative of the types of images in Fig. 2.25.) Each point in the Nk-plane represents an image having values of N and k equal to the coordinates of that point. Points lying on an isopreference curve correspond to images of equal subjective quality. It was found in the course of the experiments that the isopreference curves tended to shift right and upward, but their shapes in each of the three image categories were similar to those in Fig. 2.26. These results were not unexpected, because a shift up and right in the curves simply means larger values for N and k, which implies better picture quality.
abc
FIGURE 2.25 (a) Image with a low level of detail. (b) Image with a medium level of detail. (c) Image with a relatively large amount of detail. (Image (b) courtesy of the Massachusetts Institute of Technology.)
Face
32 64 128 256
4
5
k
N
Crowd
Cameraman
FIGURE 2.26
Representative isopreference curves for the three types of images in Fig. 2.25.
www.EBooksWorld.ir


2.4 Image Sampling and Quantization 77
Observe that isopreference curves tend to become more vertical as the detail in the image increases.This result suggests that for images with a large amount of detail only a few intensity levels may be needed. For example, the isopreference curve in Fig. 2.26 corresponding to the crowd is nearly vertical. This indicates that, for a fixed value of N, the perceived quality for this type of image is nearly independent of the number of intensity levels used (for the range of intensity levels shown in Fig. 2.26). The perceived quality in the other two image categories remained the same in some intervals in which the number of samples was increased, but the number of intensity levels actually decreased. The most likely reason for this result is that a decrease in k tends to increase the apparent contrast, a visual effect often perceived as improved image quality.
IMAGE INTERPOLATION
Interpolation is used in tasks such as zooming, shrinking, rotating, and geometrically correcting digital images. Our principal objective in this section is to introduce interpolation and apply it to image resizing (shrinking and zooming), which are basically image resampling methods. Uses of interpolation in applications such as rotation and geometric corrections will be discussed in Section 2.6. Interpolation is the process of using known data to estimate values at unknown locations. We begin the discussion of this topic with a short example. Suppose that an image of size 500 * 500 pixels has to be enlarged 1.5 times to 750 * 750 pixels. A simple way to visualize zooming is to create an imaginary 750 * 750 grid with the same pixel spacing as the original image, then shrink it so that it exactly overlays the original image. Obviously, the pixel spacing in the shrunken 750 * 750 grid will be less than the pixel spacing in the original image. To assign an intensity value to any point in the overlay, we look for its closest pixel in the underlying original image and assign the intensity of that pixel to the new pixel in the 750 * 750 grid. When intensities have been assigned to all the points in the overlay grid, we expand it back to the specified size to obtain the resized image. The method just discussed is called nearest neighbor interpolation because it assigns to each new location the intensity of its nearest neighbor in the original image (see Section 2.5 regarding neighborhoods). This approach is simple but, it has the tendency to produce undesirable artifacts, such as severe distortion of straight edges. A more suitable approach is bilinear interpolation, in which we use the four nearest neighbors to estimate the intensity at a given location. Let (x, y) denote the coordinates of the location to which we want to assign an intensity value (think of it as a point of the grid described previously), and let v(x, y) denote that intensity value. For bilinear interpolation, the assigned value is obtained using the equation
v(x, y) = ax + by + cxy + d (2-17)
where the four coefficients are determined from the four equations in four unknowns that can be written using the four nearest neighbors of point (x, y). Bilinear interpolation gives much better results than nearest neighbor interpolation, with a modest increase in computational burden.
Contrary to what the name suggests, bilinear interpolation is not a linear operation because it involves multiplication of coordinates (which is not a linear operation). See Eq. (2-17).
www.EBooksWorld.ir


78 Chapter 2 Digital Image Fundamentals
The next level of complexity is bicubic interpolation, which involves the sixteen nearest neighbors of a point. The intensity value assigned to point (x, y) is obtained using the equation
v(x, y) a x y
ij
ij
ij
=
= ∑ = ∑0
3
0
3
(2-18)
The sixteen coefficients are determined from the sixteen equations with sixteen unknowns that can be written using the sixteen nearest neighbors of point (x, y). Observe that Eq. (2-18) reduces in form to Eq. (2-17) if the limits of both summations in the former equation are 0 to 1. Generally, bicubic interpolation does a better job of preserving fine detail than its bilinear counterpart. Bicubic interpolation is the standard used in commercial image editing applications, such as Adobe Photoshop and Corel Photopaint. Although images are displayed with integer coordinates, it is possible during processing to work with subpixel accuracy by increasing the size of the image using interpolation to “fill the gaps” between pixels in the original image.
EXAMPLE 2.4: Comparison of interpolation approaches for image shrinking and zooming.
Figure 2.27(a) is the same as Fig. 2.23(d), which was obtained by reducing the resolution of the 930 dpi image in Fig. 2.23(a) to 72 dpi (the size shrank from 2136 * 2140 to 165 * 166 pixels) and then zooming the reduced image back to its original size. To generate Fig. 2.23(d) we used nearest neighbor interpolation both to shrink and zoom the image. As noted earlier, the result in Fig. 2.27(a) is rather poor. Figures 2.27(b) and (c) are the results of repeating the same procedure but using, respectively, bilinear and bicubic interpolation for both shrinking and zooming.The result obtained by using bilinear interpolation is a significant improvement over nearest neighbor interpolation, but the resulting image is blurred slightly. Much sharper results can be obtained using bicubic interpolation, as Fig. 2.27(c) shows.
FIGURE 2.27 (a) Image reduced to 72 dpi and zoomed back to its original 930 dpi using nearest neighbor interpolation. This figure is the same as Fig. 2.23(d). (b) Image reduced to 72 dpi and zoomed using bilinear interpolation. (c) Same as (b) but using bicubic interpolation.
abc
www.EBooksWorld.ir


2.5 Some Basic Relationships Between Pixels 79
It is possible to use more neighbors in interpolation, and there are more complex techniques, such as using splines or wavelets, that in some instances can yield better results than the methods just discussed. While preserving fine detail is an exceptionally important consideration in image generation for 3-D graphics (for example, see Hughes and Andries [2013]), the extra computational burden seldom is justifiable for general-purpose digital image processing, where bilinear or bicubic interpolation typically are the methods of choice.
2.5 SOME BASIC RELATIONSHIPS BETWEEN PIXELS
In this section, we discuss several important relationships between pixels in a digital image. When referring in the following discussion to particular pixels, we use lowercase letters, such as p and q.
NEIGHBORS OF A PIXEL
A pixel p at coordinates (x, y) has two horizontal and two vertical neighbors with coordinates
(x + 1, y), (x − 1, y), (x, y + 1), (x, y − 1)
This set of pixels, called the 4-neighbors of p, is denoted N4( p). The four diagonal neighbors of p have coordinates
(x + 1, y + 1), (x + 1, y − 1), (x − 1, y + 1), (x − 1, y − 1)
and are denoted N p
D( ). These neighbors, together with the 4-neighbors, are called the 8-neighbors of p, denoted by N8( p). The set of image locations of the neighbors of a point p is called the neighborhood of p. The neighborhood is said to be closed if it contains p. Otherwise, the neighborhood is said to be open.
ADJACENCY, CONNECTIVITY, REGIONS, AND BOUNDARIES
Let V be the set of intensity values used to define adjacency. In a binary image, V = {1} if we are referring to adjacency of pixels with value 1. In a grayscale image, the idea is the same, but set V typically contains more elements. For example, if we are dealing with the adjacency of pixels whose values are in the range 0 to 255, set V could be any subset of these 256 values. We consider three types of adjacency:
1. 4-adjacency. Two pixels p and q with values from V are 4-adjacent if q is in the set N4( p).
2. 8-adjacency. Two pixels p and q with values from V are 8-adjacent if q is in the set N8( p).
3. m-adjacency (also called mixed adjacency). Two pixels p and q with values from V are m-adjacent if
2.5
www.EBooksWorld.ir


80 Chapter 2 Digital Image Fundamentals
(a) q is in N4( p), or (b) q is in N p
D( ) and the set N p N q
44
( ) ̈ ( ) has no pixels whose values are from V.
Mixed adjacency is a modification of 8-adjacency, and is introduced to eliminate the ambiguities that may result from using 8-adjacency. For example, consider the pixel arrangement in Fig. 2.28(a) and let V = {1}. The three pixels at the top of Fig. 2.28(b) show multiple (ambiguous) 8-adjacency, as indicated by the dashed lines. This ambiguity is removed by using m-adjacency, as in Fig. 2.28(c). In other words, the center and upper-right diagonal pixels are not m-adjacent because they do not satisfy condition (b). A digital path (or curve) from pixel p with coordinates (x0, y0 ) to pixel q with coordinates (xn, yn ) is a sequence of distinct pixels with coordinates
(x0, y0 ), (x1, y1),..., (xn, yn )
where points (xi , yi ) and (x , y )
i−1 i−1 are adjacent for 1 ≤ i ≤ n. In this case, n is the length of the path. If (x0, y0 ) = (xn, yn ) the path is a closed path. We can define 4-, 8-, or m-paths, depending on the type of adjacency specified. For example, the paths in Fig. 2.28(b) between the top right and bottom right points are 8-paths, and the path in Fig. 2.28(c) is an m-path. Let S represent a subset of pixels in an image. Two pixels p and q are said to be connected in S if there exists a path between them consisting entirely of pixels in S. For any pixel p in S, the set of pixels that are connected to it in S is called a connected component of S. If it only has one component, and that component is connected, then S is called a connected set. Let R represent a subset of pixels in an image. We call R a region of the image if R is a connected set. Two regions, Ri and Rj are said to be adjacent if their union forms a connected set. Regions that are not adjacent are said to be disjoint. We consider 4and 8-adjacency when referring to regions. For our definition to make sense, the type of adjacency used must be specified. For example, the two regions of 1’s in Fig. 2.28(d) are adjacent only if 8-adjacency is used (according to the definition in the previous
We use the symbols  ̈ and  ́ to denote set intersection and union, respectively. Given sets A and B, recall that their intersection is the set of elements that are members of both A and B. The union of these two sets is the set of elements that are members of A, of B, or of both. We will discuss sets in more detail in Section 2.6.
011 010 001
0 0 0 0 0 0
0 1 1 1 1 0
0 0 0 0 0 0
111 101 010
Ri
Rj
001 111 111
0 0 0 0 0 0
0 1 1 1 1 0
0 1 1 1 1 0
0 0 0 1 1 0
0 0 0 0 0 0
011 010 001
0 0 0
11 10 01
abcde f
FIGURE 2.28 (a) An arrangement of pixels. (b) Pixels that are 8-adjacent (adjacency is shown by dashed lines). (c) m-adjacency. (d) Two regions (of 1’s) that are 8-adjacent. (e) The circled point is on the boundary of the 1-valued pixels only if 8-adjacency between the region and background is used. (f) The inner boundary of the 1-valued region does not form a closed path, but its outer boundary does.
www.EBooksWorld.ir


2.5 Some Basic Relationships Between Pixels 81
paragraph, a 4-path between the two regions does not exist, so their union is not a connected set). Suppose an image contains K disjoint regions, R k K
k , = 1, 2, ... , , none of which
touches the image border.† Let Ru denote the union of all the K regions, and let
Ru
c
( ) denote its complement (recall that the complement of a set A is the set of points that are not in A). We call all the points in Ru the foreground, and all the points in Ru
c
( ) the background of the image. The boundary (also called the border or contour) of a region R is the set of pixels in R that are adjacent to pixels in the complement of R. Stated another way, the border of a region is the set of pixels in the region that have at least one background neighbor. Here again, we must specify the connectivity being used to define adjacency. For example, the point circled in Fig. 2.28(e) is not a member of the border of the 1-valued region if 4-connectivity is used between the region and its background, because the only possible connection between that point and the background is diagonal. As a rule, adjacency between points in a region and its background is defined using 8-connectivity to handle situations such as this. The preceding definition sometimes is referred to as the inner border of the region to distinguish it from its outer border, which is the corresponding border in the background. This distinction is important in the development of border-following algorithms. Such algorithms usually are formulated to follow the outer boundary in order to guarantee that the result will form a closed path. For instance, the inner border of the 1-valued region in Fig. 2.28(f) is the region itself. This border does not satisfy the definition of a closed path. On the other hand, the outer border of the region does form a closed path around the region. If R happens to be an entire image, then its boundary (or border) is defined as the set of pixels in the first and last rows and columns of the image. This extra definition is required because an image has no neighbors beyond its border. Normally, when we refer to a region, we are referring to a subset of an image, and any pixels in the boundary of the region that happen to coincide with the border of the image are included implicitly as part of the region boundary. The concept of an edge is found frequently in discussions dealing with regions and boundaries. However, there is a key difference between these two concepts. The boundary of a finite region forms a closed path and is thus a “global” concept. As we will discuss in detail in Chapter 10, edges are formed from pixels with derivative values that exceed a preset threshold. Thus, an edge is a “local” concept that is based on a measure of intensity-level discontinuity at a point. It is possible to link edge points into edge segments, and sometimes these segments are linked in such a way that they correspond to boundaries, but this is not always the case. The one exception in which edges and boundaries correspond is in binary images. Depending on the type of connectivity and edge operators used (we will discuss these in Chapter 10), the edge extracted from a binary region will be the same as the region boundary. This is
† We make this assumption to avoid having to deal with special cases. This can be done without loss of generality because if one or more regions touch the border of an image, we can simply pad the image with a 1-pixel-wide border of background values.
www.EBooksWorld.ir


82 Chapter 2 Digital Image Fundamentals
intuitive. Conceptually, until we arrive at Chapter 10, it is helpful to think of edges as intensity discontinuities, and of boundaries as closed paths.
DISTANCE MEASURES
For pixels p, q, and s, with coordinates (x, y), (u, v), and (w, z), respectively, D is a distance function or metric if
(a) D( p,q) ≥ 0 (D( p,q) = 0 iff p = q), (b) D( p,q) = D(q, p), and
(c) D( p, s) ≤ D( p, q) + D(q, s).
The Euclidean distance between p and q is defined as
D pq x y
e( , ) = ( − ) + ( − )
⎡⎣ ⎤⎦
uv
2 21
2 (2-19)
For this distance measure, the pixels having a distance less than or equal to some value r from (x, y) are the points contained in a disk of radius r centered at (x, y). The D4 distance, (called the city-block distance) between p and q is defined as
D pq x y
4( , ) = − u + − v (2-20)
In this case, pixels having a D4 distance from (x, y) that is less than or equal to some value d form a diamond centered at (x, y). For example, the pixels with D4 distance ≤ 2 from (x, y) (the center point) form the following contours of constant distance:
2
212
21012
212
2
The pixels with D4 = 1 are the 4-neighbors of (x, y). The D8 distance (called the chessboard distance) between p and q is defined as
D pq x y
8( , ) = max( − u , − v ) (2-21)
In this case, the pixels with D8 distance from (x, y) less than or equal to some value d form a square centered at (x, y). For example, the pixels with D8 distance ≤ 2 form the following contours of constant distance:
22222
21112
21012
21112
22222
The pixels with D8 = 1 are the 8-neighbors of the pixel at (x, y).
www.EBooksWorld.ir


2.6 Introduction to the Basic Mathematical Tools Used in Digital Image Processing 83
Note that the D4 and D8 distances between p and q are independent of any paths that might exist between these points because these distances involve only the coordinates of the points. In the case of m-adjacency, however, the Dm distance between two points is defined as the shortest m-path between the points. In this case, the distance between two pixels will depend on the values of the pixels along the path, as well as the values of their neighbors. For instance, consider the following arrangement of pixels and assume that p, p2 , and p4 have a value of 1, and that p1 and p3 can be 0 or 1:
pp
pp
p
34
12
Suppose that we consider adjacency of pixels valued 1 (i.e.,V = {1}). If p1 and p3 are 0, the length of the shortest m-path (the Dm distance) between p and p4 is 2. If p1 is 1, then p2 and p will no longer be m-adjacent (see the definition of m-adjacency given earlier) and the length of the shortest m-path becomes 3 (the path goes through the points p p p p
1 2 4 ). Similar comments apply if p3 is 1 (and p1 is 0); in this case, the length of the shortest m-path also is 3. Finally, if both p1 and p3 are 1, the length of the shortest m-path between p and p4 is 4. In this case, the path goes through the sequence of points p p p p p
1 2 3 4.
2.6 INTRODUCTION TO THE BASIC MATHEMATICAL TOOLS USED IN DIGITAL IMAGE PROCESSING
This section has two principal objectives: (1) to introduce various mathematical tools we use throughout the book; and (2) to help you begin developing a “feel” for how these tools are used by applying them to a variety of basic image-processing tasks, some of which will be used numerous times in subsequent discussions.
ELEMENTWISE VERSUS MATRIX OPERATIONS
An elementwise operation involving one or more images is carried out on a pixel-bypixel basis. We mentioned earlier in this chapter that images can be viewed equivalently as matrices. In fact, as you will see later in this section, there are many situations in which operations between images are carried out using matrix theory. It is for this reason that a clear distinction must be made between elementwise and matrix operations. For example, consider the following 2 * 2 images (matrices):
aa
aa
bb
bb
11 12
21 22
11 12
21 22
⎡
⎣⎢ ⎤
⎦⎥ ⎡
⎣⎢ ⎤
⎦⎥
and
The elementwise product (often denoted using the symbol } or z ) of these two images is
aa
aa
bb
bb
ab ab
ab a
11 12
21 22
11 12
21 22
11 11 12 12
21 21 2
⎡
⎣⎢ ⎤
⎦⎥ ⎡
⎣⎢ ⎤
} ⎦⎥ =
2b22
⎡
⎣⎢ ⎤
⎦⎥
2.6
You may find it helpful to download and study the review material dealing with probability, vectors, linear algebra, and linear systems. The review is available in the Tutorials section of the book website.
The elementwise product of two matrices is also called the Hadamard product of the matrices.
The symbol | is often used to denote elementwise division.
www.EBooksWorld.ir


84 Chapter 2 Digital Image Fundamentals
That is, the elementwise product is obtained by multiplying pairs of corresponding pixels. On the other hand, the matrix product of the images is formed using the rules of matrix multiplication:
aa
aa
bb
bb
ab ab ab a
11 12
21 22
11 12
21 22
11 11 12 21 11 12
⎡
⎣⎢ ⎤
⎦⎥ ⎡
⎣⎢ ⎤
⎦⎥ = + + 12 22
21 11 22 21 21 12 22 22
b
a b +a b a b +a b
⎡
⎣⎢ ⎤
⎦⎥
We assume elementwise operations throughout the book, unless stated otherwise. For example, when we refer to raising an image to a power, we mean that each individual pixel is raised to that power; when we refer to dividing an image by another, we mean that the division is between corresponding pixel pairs, and so on. The terms elementwise addition and subtraction of two images are redundant because these are elementwise operations by definition. However, you may see them used sometimes to clarify notational ambiguities.
LINEAR VERSUS NONLINEAR OPERATIONS
One of the most important classifications of an image processing method is whether it is linear or nonlinear. Consider a general operator, , that produces an output image, g(x, y), from a given input image, f (x, y):
f (x, y) g(x, y)
[ ] = (2-22)
Given two arbitrary constants, a and b, and two arbitrary images f x y
1( , ) and f x y
2( , ), is said to be a linear operator if
af x y bf x y a f x y b f x y
ag x y bg
12 1 2
12
(, ) (, ) (, ) (, )
(, ) (
+
[ ]= [ ]+ [ ]
= + x, y) (2-23)
This equation indicates that the output of a linear operation applied to the sum of two inputs is the same as performing the operation individually on the inputs and then summing the results. In addition, the output of a linear operation on a constant multiplied by an input is the same as the output of the operation due to the original input multiplied by that constant. The first property is called the property of additivity, and the second is called the property of homogeneity. By definition, an operator that fails to satisfy Eq. (2-23) is said to be nonlinear. As an example, suppose that is the sum operator, Σ. The function performed by this operator is simply to sum its inputs. To test for linearity, we start with the left side of Eq. (2-23) and attempt to prove that it is equal to the right side:
af x y bf x y af x y bf x y
a f xy b f xy
12 1 2
12
(, ) (, ) (, ) (, )
(, ) (, )
+
[ ]= +
=+
∑ ∑∑ ∑∑
= ag x y + bg x y
12
(, ) (, )
where the first step follows from the fact that summation is distributive. So, an expansion of the left side is equal to the right side of Eq. (2-23), and we conclude that the sum operator is linear.
These are image summations, not the sums of all the elements of an image.
www.EBooksWorld.ir


2.6 Introduction to the Basic Mathematical Tools Used in Digital Image Processing 85
On the other hand, suppose that we are working with the max operation, whose function is to find the maximum value of the pixels in an image. For our purposes here, the simplest way to prove that this operator is nonlinear is to find an example that fails the test in Eq. (2-23). Consider the following two images
ff
12
02
23
65
47
=⎡
⎣⎢ ⎤
⎦⎥ = ⎡
⎣⎢ ⎤
⎦⎥
and
and suppose that we let a = 1 and b = −1. To test for linearity, we again start with the left side of Eq. (2-23):
max (1) 0 2 ( ) max
2 3 165
47
63
24
⎡
⎣⎢ ⎤
⎦⎥ + − ⎡
⎣⎢ ⎤
⎦⎥
⎧⎨⎩
⎫⎬⎭
= −−
−−
⎡
⎣⎢ ⎤
⎦⎥
⎧⎨⎩
⎫⎬⎭
= −2
Working next with the right side, we obtain
(1)max 0 2 ( )max ( )
23 1 65
4 7 3 17 4
⎡
⎣⎢ ⎤
⎦⎥
⎧⎨⎩
⎫⎬⎭
+− ⎡
⎣⎢ ⎤
⎦⎥
⎧⎨⎩
⎫⎬⎭
= + − =−
The left and right sides of Eq. (2-23) are not equal in this case, so we have proved that the max operator is nonlinear. As you will see in the next three chapters, linear operations are exceptionally important because they encompass a large body of theoretical and practical results that are applicable to image processing. The scope of nonlinear operations is considerably more limited. However, you will encounter in the following chapters several nonlinear image processing operations whose performance far exceeds what is achievable by their linear counterparts.
ARITHMETIC OPERATIONS
Arithmetic operations between two images f (x, y) and g(x, y) are denoted as
sxy f xy gxy
dxy f xy gxy
pxy f xy gx
(, ) (, ) (, )
(, ) (, ) (, )
(, ) (, ) (,
=+ =− = ×y
xy f xy gxy
)
v( , ) = ( , ) ÷ ( , )
(2-24)
These are elementwise operations which, as noted earlier in this section, means that they are performed between corresponding pixel pairs in f and g for x = 0, 1, 2, ... , M − 1 and y = 0, 1, 2, ... , N − 1. As usual, M and N are the row and column sizes of the images. Clearly, s, d, p, and v are images of size M × N also. Note that image arithmetic in the manner just defined involves images of the same size. The following examples illustrate the important role of arithmetic operations in digital image processing.
www.EBooksWorld.ir


86 Chapter 2 Digital Image Fundamentals
EXAMPLE 2.5: Using image addition (averaging) for noise reduction.
Suppose that g(x, y) is a corrupted image formed by the addition of noise, h(x, y), to a noiseless image f (x, y); that is,
g(x, y) = f (x, y) + h(x, y) (2-25)
where the assumption is that at every pair of coordinates (x, y) the noise is uncorrelated† and has zero average value. We assume also that the noise and image values are uncorrelated (this is a typical assumption for additive noise). The objective of the following procedure is to reduce the noise content of the output image by adding a set of noisy input images, g x y
i( , ) .
{ } This is a technique used frequently for image enhancement. If the noise satisfies the constraints just stated, it can be shown (Problem 2.26) that if an image g(x, y) is formed by averaging K different noisy images,
gxy K g xy
i i
K
( , )= ( , )
1 = ∑
1
(2-26)
then it follows that
E g(x, y) f (x, y)
{ } = (2-27)
and
sg x y sh x y
K
(,) (,)
22
1
= (2-28)
where E g(x, y)
{ } is the expected value of g(x, y), and sg(x,y)
2 and sh(x,y)
2 are the variances of g(x, y) and h(x, y), respectively, all at coordinates (x, y). These variances are arrays of the same size as the input image, and there is a scalar variance value for each pixel location. The standard deviation (square root of the variance) at any point (x, y) in the average image is
sg x y sh x y
K
(,) (,)
= 1 (2-29)
As K increases, Eqs. (2-28) and (2-29) indicate that the variability (as measured by the variance or the standard deviation) of the pixel values at each location (x, y) decreases. Because E g(x, y) f (x, y),
{ }=
this means that g(x, y) approaches the noiseless image f (x, y) as the number of noisy images used in the averaging process increases. In order to avoid blurring and other artifacts in the output (average) image, it is necessary that the images g x y
i( , ) be registered (i.e., spatially aligned). An important application of image averaging is in the field of astronomy, where imaging under very low light levels often cause sensor noise to render individual images virtually useless for analysis (lowering the temperature of the sensor helps reduce noise). Figure 2.29(a) shows an 8-bit image of the Galaxy Pair NGC 3314, in which noise corruption was simulated by adding to it Gaussian noise with zero mean and a standard deviation of 64 intensity levels. This image, which is representative of noisy astronomical images taken under low light conditions, is useless for all practical purposes. Figures 2.29(b) through (f) show the results of averaging 5, 10, 20, 50, and 100 images, respectively. We see from Fig. 2.29(b) that an average of only 10 images resulted in some visible improvement. According to Eq.
†The variance of a random variable z with mean z is defined as E{(z − z)2}, where E{ } is the expected value of the argument. The covariance of two random variables zi and zj is defined as E z z z z
i ij j
{( − )( − )}. If the variables are uncorrelated, their covariance is 0, and vice versa. (Do not confuse correlation and statistical independence. If two random variables are statistically independent, their correlation is zero. However, the converse is not true in general.)
www.EBooksWorld.ir


2.6 Introduction to the Basic Mathematical Tools Used in Digital Image Processing 87
(2-29), the standard deviation of the noise in Fig. 2.29(b) is less than half (1 5 = 0.45) the standard deviation of the noise in Fig. 2.29(a), or (0.45)(64) ≈ 29 intensity levels. Similarly, the standard deviations of the noise in Figs. 2.29(c) through (f) are 0.32, 0.22, 0.14, and 0.10 of the original, which translates approximately into 20, 14, 9, and 6 intensity levels, respectively. We see in these images a progression of more visible detail as the standard deviation of the noise decreases. The last two images are visually identical for all practical purposes. This is not unexpected, as the difference between the standard deviations of their noise level is only about 3 intensity levels According to the discussion in connection with Fig. 2.5, this difference is below what a human generally is able to detect.
EXAMPLE 2.6: Comparing images using subtraction.
Image subtraction is used routinely for enhancing differences between images. For example, the image in Fig. 2.30(b) was obtained by setting to zero the least-significant bit of every pixel in Fig. 2.30(a). Visually, these images are indistinguishable. However, as Fig. 2.30(c) shows, subtracting one image from
abc de f
FIGURE 2.29 (a) Image of Galaxy Pair NGC 3314 corrupted by additive Gaussian noise. (b)-(f) Result of averaging 5, 10, 20, 50, and 1,00 noisy images, respectively. All images are of size 566 × 598 pixels, and all were scaled so that their intensities would span the full [0, 255] intensity scale. (Original image courtesy of NASA.)
www.EBooksWorld.ir


88 Chapter 2 Digital Image Fundamentals
the other clearly shows their differences. Black (0) values in the difference image indicate locations where there is no difference between the images in Figs. 2.30(a) and (b). We saw in Fig. 2.23 that detail was lost as the resolution was reduced in the chronometer image shown in Fig. 2.23(a). A vivid indication of image change as a function of resolution can be obtained by displaying the differences between the original image and its various lower-resolution counterparts. Figure 2.31(a) shows the difference between the 930 dpi and 72 dpi images. As you can see, the differences are quite noticeable. The intensity at any point in the difference image is proportional to the magnitude of the numerical difference between the two images at that point. Therefore, we can analyze which areas of the original image are affected the most when resolution is reduced. The next two images in Fig. 2.31 show proportionally less overall intensities, indicating smaller differences between the 930 dpi image and 150 dpi and 300 dpi images, as expected.
abc
FIGURE 2.30 (a) Infrared image of the Washington, D.C. area. (b) Image resulting from setting to zero the least significant bit of every pixel in (a). (c) Difference of the two images, scaled to the range [0, 255] for clarity. (Original image courtesy of NASA.)
abc
FIGURE 2.31 (a) Difference between the 930 dpi and 72 dpi images in Fig. 2.23. (b) Difference between the 930 dpi and 150 dpi images. (c) Difference between the 930 dpi and 300 dpi images.
www.EBooksWorld.ir


2.6 Introduction to the Basic Mathematical Tools Used in Digital Image Processing 89
As a final illustration, we discuss briefly an area of medical imaging called mask mode radiography, a commercially successful and highly beneficial use of image subtraction. Consider image differences of the form
g(x, y) = f (x, y) − h(x, y) (2-30)
In this case h(x, y), the mask, is an X-ray image of a region of a patient’s body captured by an intensified TV camera (instead of traditional X-ray film) located opposite an X-ray source. The procedure consists of injecting an X-ray contrast medium into the patient’s bloodstream, taking a series of images called live images [samples of which are denoted as f (x, y)] of the same anatomical region as h(x, y), and subtracting the mask from the series of incoming live images after injection of the contrast medium. The net effect of subtracting the mask from each sample live image is that the areas that are different between f (x, y) and h(x, y) appear in the output image, g(x, y), as enhanced detail. Because images can be captured at TV rates, this procedure outputs a video showing how the contrast medium propagates through the various arteries in the area being observed. Figure 2.32(a) shows a mask X-ray image of the top of a patient’s head prior to injection of an iodine medium into the bloodstream, and Fig. 2.32(b) is a sample of a live image taken after the medium was
ab cd
FIGURE 2.32 Digital subtraction angiography. (a) Mask image. (b) A live image. (c) Difference between (a) and (b). (d) Enhanced difference image. (Figures (a) and (b) courtesy of the Image Sciences Institute, University Medical Center, Utrecht, The Netherlands.)
www.EBooksWorld.ir


90 Chapter 2 Digital Image Fundamentals
injected. Figure 2.32(c) is the difference between (a) and (b). Some fine blood vessel structures are visible in this image.The difference is clear in Fig. 2.32(d), which was obtained by sharpening the image and enhancing its contrast (we will discuss these techniques in the next chapter). Figure 2.32(d) is a “snapshot” of how the medium is propagating through the blood vessels in the subject’s brain.
EXAMPLE 2.7: Using image multiplication and division for shading correction and for masking.
An important application of image multiplication (and division) is shading correction. Suppose that an imaging sensor produces images that can be modeled as the product of a “perfect image,” denoted by f (x, y), times a shading function, h(x, y); that is, g(x, y) = f (x, y)h(x, y). If h(x, y) is known or can be estimated, we can obtain f (x, y) (or an estimate of it) by multiplying the sensed image by the inverse of h(x, y) (i.e., dividing g by h using elementwise division). If access to the imaging system is possible, we can obtain a good approximation to the shading function by imaging a target of constant intensity.When the sensor is not available, we often can estimate the shading pattern directly from a shaded image using the approaches discussed in Sections 3.5 and 9.8. Figure 2.33 shows an example of shading correction using an estimate of the shading pattern. The corrected image is not perfect because of errors in the shading pattern (this is typical), but the result definitely is an improvement over the shaded image in Fig. 2.33 (a). See Section 3.5 for a discussion of how we estimated Fig. 2.33 (b). Another use of image multiplication is in masking, also called region of interest (ROI), operations. As Fig. 2.34 shows, the process consists of multiplying a given image by a mask image that has 1’s in the ROI and 0’s elsewhere. There can be more than one ROI in the mask image, and the shape of the ROI can be arbitrary.
A few comments about implementing image arithmetic operations are in order before we leave this section. In practice, most images are displayed using 8 bits (even 24-bit color images consist of three separate 8-bit channels). Thus, we expect image values to be in the range from 0 to 255. When images are saved in a standard image format, such as TIFF or JPEG, conversion to this range is automatic. When image values exceed the allowed range, clipping or scaling becomes necessary. For example, the values in the difference of two 8-bit images can range from a minimum of −255
abc
FIGURE 2.33 Shading correction. (a) Shaded test pattern. (b) Estimated shading pattern. (c) Product of (a) by the reciprocal of (b). (See Section 3.5 for a discussion of how (b) was estimated.)
www.EBooksWorld.ir


2.6 Introduction to the Basic Mathematical Tools Used in Digital Image Processing 91
to a maximum of 255, and the values of the sum of two such images can range from 0 to 510. When converting images to eight bits, many software applications simply set all negative values to 0 and set to 255 all values that exceed this limit. Given a digital image g resulting from one or more arithmetic (or other) operations, an approach guaranteeing that the full range of a values is “captured” into a fixed number of bits is as follows. First, we perform the operation
gg g
m = − min( ) (2-31)
which creates an image whose minimum value is 0. Then, we perform the operation
g Kg g
sm m
=[ ]
max( ) (2-32)
which creates a scaled image, gs, whose values are in the range [0, K]. When working with 8-bit images, setting K = 255 gives us a scaled image whose intensities span the full 8-bit scale from 0 to 255. Similar comments apply to 16-bit images or higher. This approach can be used for all arithmetic operations. When performing division, we have the extra requirement that a small number should be added to the pixels of the divisor image to avoid division by 0.
SET AND LOGICAL OPERATIONS
In this section, we discuss the basics of set theory. We also introduce and illustrate some important set and logical operations.
Basic Set Operations
A set is a collection of distinct objects. If a is an element of set A, then we write
a ∈ A (2-33)
Similarly, if a is not an element of A we write
a x A (2-34)
The set with no elements is called the null or empty set, and is denoted by ∅.
These are elementwise subtraction and division.
abc
FIGURE 2.34 (a) Digital dental X-ray image. (b) ROI mask for isolating teeth with fillings (white corresponds to 1 and black corresponds to 0). (c) Product of (a) and (b).
www.EBooksWorld.ir


92 Chapter 2 Digital Image Fundamentals
A set is denoted by the contents of two braces: { i }. For example, the expression
C= c c= d d D
{}
-, H
means that C is the set of elements, c, such that c is formed by multiplying each of the elements of set D by −1. If every element of a set A is also an element of a set B, then A is said to be a subset of B, denoted as
A 8 B (2-35)
The union of two sets A and B, denoted as
C = A ́ B (2-36)
is a set C consisting of elements belonging either to A, to B, or to both. Similarly, the intersection of two sets A and B, denoted by
D = A ̈ B (2-37)
is a set D consisting of elements belonging to both A and B. Sets A and B are said to be disjoint or mutually exclusive if they have no elements in common, in which case,
A ̈ B = ∅ (2-38)
The sample space, Æ, (also called the set universe) is the set of all possible set elements in a given application. By definition, these set elements are members of the sample space for that application. For example, if you are working with the set of real numbers, then the sample space is the real line, which contains all the real numbers. In image processing, we typically define Æ to be the rectangle containing all the pixels in an image. The complement of a set A is the set of elements that are not in A:
AA
c ={ }
w w x (2-39)
The difference of two sets A and B, denoted A − B, is defined as
A B A B A Bc
− = {w w H ,w x } =  ̈ (2-40)
This is the set of elements that belong to A, but not to B. We can define Ac in terms of Æ and the set difference operation; that is, A A
c = Æ − . Table 2.1 shows several important set properties and relationships. Figure 2.35 shows diagrammatically (in so-called Venn diagrams) some of the set relationships in Table 2.1. The shaded areas in the various figures correspond to the set operation indicated above or below the figure. Figure 2.35(a) shows the sample set, Æ. As no earlier, this is the set of all possible elements in a given application. Figure 2.35(b) shows that the complement of a set A is the set of all elements in Æ that are not in A, which agrees with our earlier definition. Observe that Figs. 2.35(e) and (g) are identical, which proves the validity of Eq. (2-40) using Venn diagrams. This
www.EBooksWorld.ir


2.6 Introduction to the Basic Mathematical Tools Used in Digital Image Processing 93
is an example of the usefulness of Venn diagrams for proving equivalences between set relationships. When applying the concepts just discussed to image processing, we let sets represent objects (regions) in a binary image, and the elements of the sets are the (x, y) coordinates of those objects. For example, if we want to know whether two objects, A and B, of a binary image overlap, all we have to do is compute A ̈ B. If the result is not the empty set, we know that some of the elements of the two objects overlap. Keep in mind that the only way that the operations illustrated in Fig. 2.35 can make sense in the context of image processing is if the images containing the sets are binary, in which case we can talk about set membership based on coordinates, the assumption being that all members of the sets have the same intensity value (typically denoted by 1). We will discuss set operations involving binary images in more detail in the following section and in Chapter 9. The preceding concepts are not applicable when dealing with grayscale images, because we have not defined yet a mechanism for assigning intensity values to the pixels resulting from a set operation. In Sections 3.8 and 9.6 we will define the union and intersection operations for grayscale values as the maximum and minimum of corresponding pixel pairs, respectively. We define the complement of a grayscale image as the pairwise differences between a constant and the intensity of every pixel in the image. The fact that we deal with corresponding pixel pairs tells us that grayscale set operations are elementwise operations, as defined earlier. The following example is a brief illustration of set operations involving grayscale images. We will discuss these concepts further in the two sections just mentioned.
Description Expressions
Operations between the sample space and null sets
Æ Æ Æ ́ Æ Æ ̈
cc
= ∅; ∅ = ; ∅ = ; ∅ = ∅
Union and intersection with the null and sample space sets
A  ́ ∅ = A; A  ̈ ∅ = ∅; A  ́ Æ = Æ; A  ̈ Æ = A
Union and intersection of a set with itself
A  ́ A = A; A  ̈ A = A
Union and intersection of a set with its complement
AA AA
cc
 ́ = Æ;  ̈ = ∅
Commutative laws A B B A ABBA
 ́ ́  ̈ ̈
= =
Associative laws ( ) ( ) () ()
AB C A BC AB C A BC
 ́ ́  ́ ́  ̈ ̈  ̈ ̈
= =
Distributive laws ( ) ( ) ( ) ( ) ( )( )
AB C AC BC AB C AC BC
 ́ ̈  ̈ ́ ̈  ̈ ́  ́ ̈ ́
= =
DeMorgan’s laws ( ) ()
AB A B AB A B
c cc
c cc
 ́ ̈  ̈ ́
= =
TABLE 2.1
Some important set operations and relationships.
www.EBooksWorld.ir


94 Chapter 2 Digital Image Fundamentals
EXAMPLE 2.8: Illustration of set operations involving grayscale images.
Let the elements of a grayscale image be represented by a set A whose elements are triplets of the form (x, y, z), where x and y are spatial coordinates, and z denotes intensity values. We define the complement of A as the set
A xyK z xyz A
c= −
{}
( , , ) ( , , )H
which is the set of pixels of A whose intensities have been subtracted from a constant K. This constant is equal to the maximum intensity value in the image, 2 1
k − , where k is the number of bits used to represent z. Let A denote the 8-bit grayscale image in Fig. 2.36(a), and suppose that we want to form the negative of A using grayscale set operations. The negative is the set complement, and this is an 8-bit image, so all we have to do is let K = 255 in the set defined above:
A xy z xyz A
c= −
{}
( , , 255 ) ( , , ) H
Figure 2.36(b) shows the result. We show this only for illustrative purposes. Image negatives generally are computed using an intensity transformation function, as discussed later in this section.
Ac A  ̈ B
AA
B
A − B Bc
B
C
A
A Bc
 ̈ A  ̈ (B  ́ C)
A ́B
Ω
B
a bc d fh
eg
FIGURE 2.35 Venn diagrams corresponding to some of the set operations in Table 2.1. The results of the operations,
such as Ac , are shown shaded. Figures (e) and (g) are the same, proving via Venn diagrams that A B A Bc
−= ̈ [see Eq. (2-40)].
www.EBooksWorld.ir


2.6 Introduction to the Basic Mathematical Tools Used in Digital Image Processing 95
The union of two grayscale sets A and B with the same number of elements is defined as the set
A B ab a Ab B
z
 ́ ={ H H }
max( , ) ,
where it is understood that the max operation is applied to pairs of corresponding elements. If A and B are grayscale images of the same size, we see that their the union is an array formed from the maximum intensity between pairs of spatially corresponding elements. As an illustration, suppose that A again represents the image in Fig. 2.36(a), and let B denote a rectangular array of the same size as A, but in which all values of z are equal to 3 times the mean intensity, z, of the elements of A. Figure 2.36(c) shows the result of performing the set union, in which all values exceeding 3z appear as values from A and all other pixels have value 3z, which is a mid-gray value.
Before leaving the discussion of sets, we introduce some additional concepts that are used later in the book. The Cartesian product of two sets X and Y, denoted X × Y, is the set of all possible ordered pairs whose first component is a member of X and whose second component is a member of Y. In other words,
X * Y = (x, y) x H X and y H Y
{ } (2-41)
For example, if X is a set of M equally spaced values on the x-axis and Y is a set of N equally spaced values on the y-axis, we see that the Cartesian product of these two sets define the coordinates of an M-by-N rectangular array (i.e., the coordinates of an image). As another example, if X and Y denote the specific x- and y-coordinates of a group of 8-connected, 1-valued pixels in a binary image, then set X × Y represents the region (object) comprised of those pixels.
We follow convention in using the symbol × to denote the Cartesian product. This is not to be confused with our use of the same symbol throughout the book to denote the size of an M-by-N image (i.e., M × N).
abc
FIGURE 2.36
Set operations involving grayscale images. (a) Original image. (b) Image negative obtained using grayscale set complementation. (c) The union of image (a) and a constant image. (Original image courtesy of G.E. Medical Systems.)
www.EBooksWorld.ir


96 Chapter 2 Digital Image Fundamentals
A relation (or, more precisely, a binary relation) on a set A is a collection of ordered pairs of elements from A. That is, a binary relation is a subset of the Cartesian product A × A.A binary relation between two sets, A and B, is a subset of A × B. A partial order on a set S is a relation * on S such that * is:
(a) reflexive: for any a H S, a*a;
(b) transitive: for any a,b,c H S, a*b and b*c implies that a*c;
(c) antisymmetric: for any a,b H S, a*b and b*a implies that a = b.
where, for example, a*b reads “a is related to b.” This means that a and b are in set * , which itself is a subset of S × S according to the preceding definition of a relation. A set with a partial order is called a partially ordered set. Let the symbol U denote an ordering relation. An expression of the form
a1 a2 a3 an
U U UU
reads: a1 precedes a2 or is the same as a2, a2 precedes a3 or is the same as a3 , and so on. When working with numbers, the symbol U typically is replaced by more traditional symbols. For example, the set of real numbers ordered by the relation “less than or equal to” (denoted by ≤ ) is a partially ordered set (see Problem 2.33). Similarly, the set of natural numbers, paired with the relation “divisible by” (denoted by ÷), is a partially ordered set. Of more interest to us later in the book are strict orderings. A strict ordering on a set S is a relation * on S, such that * is:
(a) antireflexive: for any a H S, ¬a*a;
(b) transitive: for any a,b,c H S, a*b and b*c implies that a*c.
where ¬a*a means that a is not related to a. Let the symbol E denote a strict ordering relation. An expression of the form
a1 a2 a3 an
EEE E
reads a1 precedes a2, a2 precedes a3, and so on. A set with a strict ordering is called a strict-ordered set.
As an example, consider the set composed of the English alphabet of lowercase
letters, S = {a b c z}
, , , , . Based on the preceding definition, the ordering
aEbEcE Ez
is strict because no member of the set can precede itself (antireflexivity) and, for any three letters in S, if the first precedes the second, and the second precedes the third, then the first precedes the third (transitivity). Similarly, the set of integers paired with the relation “less than (<)” is a strict-ordered set.
Logical Operations
Logical operations deal with TRUE (typically denoted by 1) and FALSE (typically denoted by 0) variables and expressions. For our purposes, this means binary images
www.EBooksWorld.ir


2.6 Introduction to the Basic Mathematical Tools Used in Digital Image Processing 97
composed of foreground (1-valued) pixels, and a background composed of 0-valued pixels. We work with set and logical operators on binary images using one of two basic approaches: (1) we can use the coordinates of individual regions of foreground pixels in a single image as sets, or (2) we can work with one or more images of the same size and perform logical operations between corresponding pixels in those arrays. In the first category, a binary image can be viewed as a Venn diagram in which the coordinates of individual regions of 1-valued pixels are treated as sets. The union of these sets with the set composed of 0-valued pixels comprises the set universe, Æ. In this representation, we work with single images using all the set operations defined in the previous section. For example, given a binary image with two 1-valued regions, R1 and R2, we can determine if the regions overlap (i.e., if they have at least one pair of coordinates in common) by performing the set intersection operation R R
12
 ̈ (see Fig. 2.35). In the second approach, we perform logical operations on the pixels of one binary image, or on the corresponding pixels of two or more binary images of the same size. Logical operators can be defined in terms of truth tables, as Table 2.2 shows for two logical variables a and b. The logical AND operation (also denoted ¿) yields a 1 (TRUE) only when both a and b are 1. Otherwise, it yields 0 (FALSE). Similarly, the logical OR (¡) yields 1 when both a or b or both are 1, and 0 otherwise. The NOT ( ) operator is self explanatory. When applied to two binary images, AND and OR operate on pairs of corresponding pixels between the images. That is, they are elementwise operators (see the definition of elementwise operators given earlier in this chapter) in this context. The operators AND, OR, and NOT are functionally complete, in the sense that they can be used as the basis for constructing any other logical operator. Figure 2.37 illustrates the logical operations defined in Table 2.2 using the second approach discussed above. The NOT of binary image B1 is an array obtained by changing all 1-valued pixels to 0, and vice versa. The AND of B1 and B2 contains a 1 at all spatial locations where the corresponding elements of B1 and B2 are 1; the operation yields 0’s elsewhere. Similarly, the OR of these two images is an array that contains a 1 in locations where the corresponding elements of B1, or B2, or both, are 1. The array contains 0’s elsewhere. The result in the fourth row of Fig. 2.37 corresponds to the set of 1-valued pixels in B1 but not in B2. The last row in the figure is the XOR (exclusive OR) operation, which yields 1 in the locations where the corresponding elements of B1 or B2, (but not both) are 1. Note that the logical
a b a ANDb aOR b NOT(a)
00 0 0 1
01 0 1 1
10 0 1 0
11 1 1 0
TABLE 2.2 Truth table defining the logical operators AND(¿), OR(¡), and NOT( ).
www.EBooksWorld.ir


98 Chapter 2 Digital Image Fundamentals
expressions in the last two rows of Fig. 2.37 were constructed using operators from Table 2.2; these are examples of the functionally complete nature of these operators. We can arrive at the same results in Fig. 2.37 using the first approach discussed above. To do this, we begin by labeling the individual 1-valued regions in each of the two images (in this case there is only one such region in each image). Let A and B denote the set of coordinates of all the 1-valued pixels in images B1 and B2, respectively. Then we form a single array by ORing the two images, while keeping the labels A and B. The result would look like the array B B
12
OR in Fig. 2.37, but with the two white regions labeled A and B. In other words, the resulting array would look like a Venn diagram. With reference to the Venn diagrams and set operations defined in the previous section, we obtain the results in the rightmost column of Fig. 2.37 using set operations as follows: A B
c = NOT( 1), A  ̈ B = B1 B2
AND , A ́ B = B1 B2
OR , and similarly for the other results in Fig. 2.37. We will make extensive use in Chapter 9 of the concepts developed in this section.
SPATIAL OPERATIONS
Spatial operations are performed directly on the pixels of an image. We classify spatial operations into three broad categories: (1) single-pixel operations, (2) neighborhood operations, and (3) geometric spatial transformations.
FIGURE 2.37
Illustration of logical operations involving foreground (white) pixels. Black represents binary 0’s and white binary 1’s. The dashed lines are shown for reference only. They are not part of the result.
NOT
NOT(B1)
B1 AND B2
B1 OR B2
B1 AND [NOT (B2)]
B1 XOR B2
AND
B1
B1 B2
OR
XOR
AND
NOT
www.EBooksWorld.ir