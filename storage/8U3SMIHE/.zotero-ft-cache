CTooth+: A Large-scale Dental Cone Beam
Computed Tomography Dataset and Benchmark
for Tooth Volume Segmentation
Weiwei Cui2, Yaqi Wang1( ), Yilong Li2, Dan Song4, Xingyong Zuo4, Jiaojiao Wang1, Yifan Zhang3, Huiyu Zhou5, Bung san Chong2, Liaoyuan Zeng4, Qianni Zhang2( )
1 Communication University of Zhejiang 2 Queen Mary University of London 3 West China Hospital of Stomatology, Sichuan University 4 University of Electronic Science and Technology of China 5 University of Leicester
Abstract. Accurate tooth volume segmentation is a prerequisite for computer-aided dental analysis. Deep learning-based tooth segmentation methods have achieved satisfying performances but require a large quantity of tooth data with ground truth. The dental data publicly available is limited meaning the existing methods can not be reproduced, evaluated and applied in clinical practice. In this paper, we establish a 3D dental CBCT dataset CTooth+, with 22 fully annotated volumes and 146 unlabeled volumes. We further evaluate several state-of-the-art tooth volume segmentation strategies based on fully-supervised learning, semi-supervised learning and active learning, and define the performance principles. This work provides a new benchmark for the tooth volume segmentation task, and the experiment can serve as the baseline for future AI-based dental imaging research and clinical application development. The codebase and dataset are released here.
Keywords: 3D segmentation · dental dataset · fully supervised learning · semi-supervised learning · active learning.
1 Introduction
Accurately segmented tooth volumes provide valuable 3D information for the clinical diagnosis such as root shape, curvature, tooth size, the spatial relationship of multiple teeth. However, manually delineating all tooth regions is labour-consuming, error-prone and expensive. Some learning-based methods have been proposed to automatically segment tooth regions and achieve approving results. Several shallow learning-based methods try to quickly segment teeth from X-ray or CBCT images such as regionbased [16], threshold-based [2], and cluster-based [3] approaches. Recently, deep learning-based methods attempt to solve 3D tooth segmentation. Mask R-CNN
arXiv:2208.01643v1 [eess.IV] 2 Aug 2022


2 Weiwei et al.
is applied on tooth segmentation and detection [8]. Modified 3D Unet structures are well-designed with initial dental masks or complex backbones [25]. However, these methods are mostly evaluated on small or in-house datasets. It is still hard to reproduce these segmentation performances as the dental dataset and code are not published.
Fig. 1. A few samples from different publicly available dental image datasets are illustrated. (a) Dental X-ray Image, (b) LNDb, (c) AGMB, (d) Teeth dataset, and (e) CTooth+.
We review several dental image datasets and summarise their contributions. Seven types of tooth structures are marked on Dental X-ray Image dataset [23]. LNDb dataset contains polygon boundary annotations of teeth on X-ray images [21]. AGMB evaluates root canal therapy on RGB images [14]. Teeth dataset is proposed for caries classification [1]. Some samples of these existing datasets are shown in Figure 1. To our knowledge, no 3D dental CBCT dataset has ever been published for open-access in the medical image processing domain. Our work is the first comprehensive study on 3D dental data collection, annotation and evaluation. We publish a 3D dataset CTooth+ and release tooth segmentation performances based on fully-supervised learning, semi-supervised learning and active learning methods. CTooth+ dataset provides a research fundamental for following-up automatic dental segmentation studies.
2 CTooth+ dataset
2.1 Dataset Summary
The main properties of the existing 2D and 3D dental datasets are summarized in Table 1. Compared with the published dental datasets, most of the existing


CTooth+ 3
datasets contain only 2D images from various tooth imaging modalities and the amount of data is relative small. Our CTooth+ fully maintains the threedimensional characteristics of teeth, and the number of data samples exceeds 30k slices, far exceeding the existing 2D dental datasets. The data set consists of 5504 annotated CBCT images of 22 patients and 25876 unlabeled images of 146 patients. All patient information is coded for the purpose of protecting privacy. For each volume, we roughly spent 6 hours to annotate tooth regions and 1 more hour to check and refine the annotations. In total, the CTooth+ dataset took us around 10 months to collect, annotate and review.
Table 1. Summary of publicly available dental datasets.
Dataset Year Modality Type Scan Dental X-ray Image [23] 2015 2D Bitewing 120 LNDb [21] 2016 2D Panomatic X-ray 1,500 Teeth dataset [1] 2020 2D Intraoral RGB image 77 AGMB [14] 2021 2D Root canal image 245 Our CTooth [7] 2022 3D CBCT 7,368 Our CTooth+ 2022 3D CBCT 31,380
The images in CTooth+ were acquired with a OP300, manufactured by Instrumentarium Orthopantomographr. CBCT slices were acquired in the DICOM format at the University of Electronic Science and Technology of China hospital. All CBCT slices were scanned before dental operations, with a resolution of 266 × 266 pixels in the axial view. The in-plane resolution is about 0.25 × 0.25mm2 and the slice thickness range from 0.25 mm to 0.3 mm.
Fig. 2. Dataset annotation and quality control procedure.


4 Weiwei et al.
2.2 Expert Annotation and quality assessment
Figure 2 illustrates the whole procedure for CTooth+ dataset annotation and quality control procedure. Scans were annotated by 15 dentists. Twelve junior dentists with at least two years of experience manually marked all teeth regions. They first used ITKSNAP [27] to delineate tooth regions slice-by-slice in the axial view. Then the annotations were modified according to the coronal view and sagittal view.
Fig. 3. Annotation adjustment.
Three senior experts with at least ten years of experience were invited to evaluate the tooth annotations. The senior experts assessed the annotation quality, and marked a quality level (excellent, good, fail and poor) on each tooth annotation. “Excellent” annotations were stored in the CTooth+ dataset directly. “Good” annotations were fed into Phototshop software [17] for fine-tuning according to the experts’ feedback. “Fair” and “Poor” annotations and their feedback were put back into the unlabelled data pool and were marked again. In Figure 3, we illustrate a set of “Good” annotations before and after adjust
Fig. 4. Annotation statistics of CTooth+.


CTooth+ 5
ment. It is clear that the tooth boundaries are more precise and smoother after necessary adjustment. Statistics of annotated teeth are illustrated in Figure 4. All image volumes have about 12 teeth, 200 or 300 slices, and 150 slices with teeth except for the 9th volume. The unlabelled images have similar statistics as the annotated images. The similar data statistics attributes ensure the stability of model training. In addition, variance in tooth shape, restorations, implants inside each volume forces the segmentation model to learn with robustness and generalizability.
2.3 Potential research topics
Fully-supervised learning (FSL) based segmentation efficiently exploit labelled data and solve complex challenges, e.g. imbalanced distributions. FSL based tooth segmentation has been studied recently but no open-access dataset is published for evaluating these methods. Here, we propose the 3D dental dataset CTooth+ and reproduce eight FSL segmentation methods based on it. Semi-supervised learning (SSL) requires less expert annotations for model training, relieving the time and labour burden associated to data annotation. To our knowledge, there is no SSL-based tooth volume segmentation method published. This work attempts to apply four state-of-the-art SSL medical segmentation methods on CTooth+ and evaluate their performances. Compared to FSL (accurate but expensive) and SSL (economical but affected by noise), various active learning (AL) strategies are designed to enlarge the training set by iteratively selecting informative samples. In this paper, we extend six active learning methods to their 3D version and evaluate their tooth segmentation performances on CTooth+.
3 Experiments and results
3.1 Evaluation metrics and implementations on the CTooth+
Evaluation Metrics: The segmentation results are evaluated using dice similarity coefficient (DSC), intersection-over-union (IOU), sensitivity (SEN), positive predictive value (ppv), Hausdorff distance (HD), average symmetric surface distance (ASSD), surface overlap (SO), and surface dice (SD) [7].
Implementation Details: Kaiming initialization [10] is used for initializing all the weights of models. The Adam optimizer is used with a learning rate of 0.0004 and a step learning scheduler (with step size=50 and γ = 0.9). All networks are trained for 300 epochs using a sever with 2 Nvidia A100s and 48 GB CPU memory. All images are divided into 3D patches (size (64,128,128)) with batch size 4 to 8 according to the model complexity. We choose 20% image volumes for evaluation and the other volumes for training the fully-supervised (with labelled images) and semi-supervised methods (with labelled and unlabelled images). Cross entropy loss [28] is exploited to train all models.


6 Weiwei et al.
3.2 Benchmark for fully-supervised tooth volume segmentation
We present the 3D FSL tooth segmentation performances on 8 fully-supervised segmentation methods. In Table 2, Attention Unet [19] outperforms other methods in most metrics including DSC 86.60 %, IOU 76.45 % and PPV 87.79 %, ASSD 0.27 mm, respectively. Hausdorff distance on nnUnet [12] is minimal at 1.29 mm, and the sensitivity metric on Voxresnet [26] achieves the best. DenseUnet [9] has a satisfying results on the accuracy of tooth surface (SO and SD). However, we observe that 3D SkipDenseNet [4] and DenseVoxelNet [26] are both inefficient for segmenting 3D tooth volumes since their network structures are deeper than others causing network overfitting on CToooth+.
Fig. 5. Evaluations on segmentation when changing the amount of training volumes.
Table 2. Evaluation comparison among differnet tooth volume segmentation methods trained on 17 volumes.
Method DSC IOU SEN PPV HD ASSD SO SD 3D SkipDenseNet [4] 64.99 49.16 73.54 69.49 7.61 1.08 80.17 76.40 DenseVoxelNet [26] 76.45 62.22 83.16 75.36 5.10 0.62 89.54 88.76 3D Unet [6] 79.51 66.40 78.21 82.78 8.02 1.01 89.22 88.76 VNet [18] 81.21 68.58 80.88 83.27 1.61 0.29 93.11 92.90 Voxresnet [26] 85.07 74.25 86.58 84.29 5.14 0.45 94.11 94.04 nnUnet [12] 85.48 74.83 84.56 87.22 1.29 0.27 95.09 95.03 Dense Unet [9] 86.27 76.11 90.80 83.23 2.08 0.39 95.98 95.91 Attention Unet [19] 86.60 76.45 86.11 87.79 1.72 0.27 95.25 95.20


CTooth+ 7
We further perform an ablation study on the FSL tooth segmentation task. Figure 5 shows the quantitative segmentation performances among the FSL segmentation methods when changing the number of training sample volumes. It is clear that all performance metrics increase when the number of data samples increases evenly. However, noise and uneven data sampling make the increase in data volume unproportional to the performance gain. Hence, more designs are considered to increase network robustness and reduce the noise effect.
3.3 Benchmark for semi-supervised tooth volume segmentation
SSL based tooth segmentation exploits less ground truth and a large number of unlabeled images for training. In Table 3, we compare the segmentation performances of four state-of-the-art SSL strategies trained by 9 labelled volumes and 8 unlabelled volumes. Experimental results show that all these SSL models achieve better performance than the FSL based Dense Unet trained on only 9 labelled volumes. CTCT [15] outperforms others.
Fig. 6. Qualitative SSL segmentation results. A closer look reveals clear tooth boundaries at the right bottom corner of each slices.
Table 3. 3D tooth segmentation performance comparison among 4 SSL methods. All models are trained by 9 labelled volumes.
# Unlabeled volume Method DSC IOU SEN PPV HD ASSD / Dense Unet [9] 78.99 65.55 78.81 81.71 4.29 0.57
8
MT [22] 82.66 70.55 83.05 83.11 2.76 0.52 CPS [5] 83.17 71.48 83.10 83.02 4.13 0.55 DCT [20] 83.10 71.33 83.62 83.10 4.28 0.56 CTCT [15] 85.32 74.60 87.55 84.22 2.81 0.43


8 Weiwei et al.
In Figure 6, we compare the segmentation details among SSL methods. CPS [5] and MT [22] are not as accurate as CTCT [15] method especially in the tooth root regions. We also compare the 3D tool model based on the segmentation boundaries between ground truth and predicted results of CTCT as shown in sub-figure (f) and (g). It can be seen that the boundary details of CTCT are close to the ground truth.
3.4 Benchmark for active learning based tooth volume segmentation
To reduce the noise effect, we reproduce six AL based medical segmentation methods based on the Attention Unet backbone and present the performances. In Table 4, CEAL [11] achieves the comparable performances as FSL but uses 12 % less training data. However, ENT [11] and MAR [13] both have similar performances as the FSL when they are trained on 72 patches. These experiments present that active learning-based tooth volume segmentation is effective but still needs more designs to explore tooth information representation.
Table 4. Evaluation comparison among differnet tooth volume segmentation methods.
# 3D Patches AL strategy DSC IOU SEN PPV HD ASSD SO SD 56 \ 81.44 68.86 80.88 83.73 2.71 0.37 92.12 91.85 72 \ 85.28 74.41 84.69 86.90 1.88 0.28 94.28 94.20 82 \ 86.60 76.45 86.11 87.79 1.72 0.27 95.25 95.20
72
ENT [11] 83.92 72.49 82.44 86.36 1.42 0.27 94.21 94.14 MAR [13] 84.88 73.86 83.30 87.30 1.63 0.29 94.08 94.03 CEAL [24] 86.58 76.43 87.85 86.01 1.05 0.21 95.92 95.89
Acknowledgement
The work was supported by the the Natural Science Foundation of China under Grant No. 62002316.
4 Conclusion
This work is the first to collect and publish a 3D dental dataset CTooth+ with annotated 3D structures of teeth according to quality assessment from experts, and evaluate the tooth volume segmentation on FSL, SSL and AL methods systematically as benchmarks. In future, we will release more data from multiple dental organisations and release more annotations on the tooth structures.


CTooth+ 9
References
1. A, P.: teeth dataset. https://www.kaggle.com/pushkar34/teeth-dataset (2020) 2. Ajaz, A., Kathirvelu, D.: Dental biometrics: Computer aided human identification system using the dental panoramic radiographs. In: 2013 international conference on communication and signal processing. pp. 717–721. IEEE (2013) 3. Alsmadi, M.K.: A hybrid fuzzy c-means and neutrosophic for jaw lesions segmentation. Ain Shams Engineering Journal 9(4), 697–706 (2018) 4. Bui, T.D., Shin, J., Moon, T.: 3d densely convolutional networks for volumetric segmentation. arXiv preprint arXiv:1709.03199 (2017) 5. Chen, X., Yuan, Y., Zeng, G., Wang, J.: Semi-supervised semantic segmentation with cross pseudo supervision. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2613–2622 (2021) 6.  ̧Cic ̧ek, O ̈ ., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3d u-net: learning dense volumetric segmentation from sparse annotation. In: International conference on medical image computing and computer-assisted intervention. pp. 424–432. Springer (2016) 7. Cui, W., Wang, Y., Zhang, Q., Zhou, H., Song, D., Zuo, X., Jia, G., Zeng, L.: CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume Segmentation on Cone Beam Computed Tomography Images. arXiv e-prints arXiv:2206.08778 (Jun 2022) 8. Cui, Z., Li, C., Wang, W.: Toothnet: Automatic tooth instance segmentation and identification from cone beam ct images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6368–6377 (2019) 9. Guan, S., Khan, A.A., Sikdar, S., Chitnis, P.V.: Fully dense unet for 2-d sparse photoacoustic tomography artifact removal. IEEE journal of biomedical and health informatics 24(2), 568–576 (2019) 10. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification. In: Proceedings of the IEEE international conference on computer vision. pp. 1026–1034 (2015) 11. Hwa, R.: Sample selection for statistical parsing. Computational linguistics 30(3), 253–276 (2004) 12. Isensee, F., Petersen, J., Klein, A., Zimmerer, D., Jaeger, P.F., Kohl, S., Wasserthal, J., Koehler, G., Norajitra, T., Wirkert, S., et al.: nnu-net: Selfadapting framework for u-net-based medical image segmentation. arXiv preprint arXiv:1809.10486 (2018) 13. Joshi, A.J., Porikli, F., Papanikolopoulos, N.: Multi-class active learning for image classification. In: 2009 ieee conference on computer vision and pattern recognition. pp. 2372–2379. IEEE (2009) 14. Li, Y., Zeng, G., Zhang, Y., Wang, J., Jin, Q., Sun, L., Zhang, Q., Lian, Q., Qian, G., Xia, N., et al.: Agmb-transformer: Anatomy-guided multi-branch transformer network for automated evaluation of root canal therapy. IEEE Journal of Biomedical and Health Informatics (2021) 15. Luo, X., Hu, M., Song, T., Wang, G., Zhang, S.: Semi-supervised medical image segmentation via cross teaching between cnn and transformer. arXiv preprint arXiv:2112.04894 (2021) 16. Lurie, A., Tosoni, G.M., Tsimikas, J., Walker Jr, F.: Recursive hierarchic segmentation analysis of bone mineral density changes on digital panoramic images. Oral surgery, oral medicine, oral pathology and oral radiology 113(4), 549–558 (2012) 17. Manovich, L.: Inside photoshop. Computational Culture (1) (2011)


10 Weiwei et al.
18. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 2016 fourth international conference on 3D vision (3DV). pp. 565–571. IEEE (2016) 19. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999 (2018) 20. Qiao, S., Shen, W., Zhang, Z., Wang, B., Yuille, A.: Deep co-training for semisupervised image recognition. In: Proceedings of the european conference on computer vision (eccv). pp. 135–152 (2018) 21. Silva, G., Oliveira, L., Pithon, M.: Automatic segmenting teeth in x-ray images: Trends, a novel data set, benchmarking and future perspectives. Expert Systems with Applications 107, 15–31 (2018) 22. Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems 30 (2017) 23. Wang, C.W., Huang, C.T., Lee, J.H., Li, C.H., Chang, S.W., Siao, M.J., Lai, T.M., Ibragimov, B., Vrtovec, T., Ronneberger, O., et al.: A benchmark for comparison of dental radiography analysis algorithms. Medical image analysis 31, 63–76 (2016) 24. Wang, K., Zhang, D., Li, Y., Zhang, R., Lin, L.: Cost-effective active learning for deep image classification. IEEE Transactions on Circuits and Systems for Video Technology 27(12), 2591–2600 (2016) 25. Yang, S., Lee, S.J., Yong, T.H., Yoo, J.Y., Chun, S., Kim, J., Seol, Y.J., Kim, G., Yi, W.J.: A deep learning-based method for tooth segmentation on cbct images affected by metal artifacts. In: 43rd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (2021) 26. Yu, L., Cheng, J.Z., Dou, Q., Yang, X., Chen, H., Qin, J., Heng, P.A.: Automatic 3d cardiovascular mr segmentation with densely-connected volumetric convnets. In: International conference on medical image computing and computer-assisted intervention. pp. 287–295. Springer (2017) 27. Yushkevich, P.A., Piven, J., Hazlett, H.C., Smith, R.G., Ho, S., Gee, J.C., Gerig, G.: User-guided 3d active contour segmentation of anatomical structures: significantly improved efficiency and reliability. Neuroimage 31(3), 1116–1128 (2006) 28. Zhang, Z., Sabuncu, M.: Generalized cross entropy loss for training deep neural networks with noisy labels. Advances in neural information processing systems 31 (2018)