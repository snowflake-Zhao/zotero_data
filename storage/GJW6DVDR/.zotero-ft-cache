1
Dr. Mathias Jesussek
Dr. Hannah Volk-Jesussek
Statistics made easy
A clear and simple introduction
5th edition


2
Â©DATAtab e.U. | Graz | 2024
The work including all its parts is protected by copyright. Any use not expressly permitted by copyright law requires the prior consent of DATAtab e.U.
Cover design: Mathias Jesussek


3
Introduction........................................................................................................................................... 17
1. Descriptive statistics and inferential statistics .............................................................................. 18
1.1 Subareas of statistics................................................................................................................. 18
1.2 Descriptive statistics.................................................................................................................. 19
1.3 Inferential Statistics................................................................................................................... 23
2. Level of measurment..................................................................................................................... 27
2.1 Nominal variables...................................................................................................................... 27
2.2 Ordinal variables........................................................................................................................ 28
2.3 Categorical variables ................................................................................................................. 29
2.4 Metric variables......................................................................................................................... 29
2.5 Ratio scale and interval scale .................................................................................................... 30
3. Sampling ........................................................................................................................................ 31
3.1 Full or total survey vs. random sample ..................................................................................... 31
3.2 Population and sample.............................................................................................................. 32
3.3 Types of sampling? .................................................................................................................... 32
3.4 Probability selection.................................................................................................................. 33
3.5 Deliberate selection .................................................................................................................. 33
3.6 Arbitrary selection..................................................................................................................... 34
3.7 Sample selection in online surveys............................................................................................ 34
3.8 Sample description in bachelor or master thesis ...................................................................... 34
4. Location parameter ....................................................................................................................... 36
4.1 Mean value (arithmetic mean).................................................................................................. 37
4.2 Geometric mean and quadratic mean ...................................................................................... 38
4.3 Median....................................................................................................................................... 38
4.4 Mean and median in comparison.............................................................................................. 40
4.5 Mode (Modal value) .................................................................................................................. 40
4.6 Advantage and disadvantage of the Mean, Median and Mode................................................ 42
5. Dispersion parameter.................................................................................................................... 45
5.1 Standard deviation .................................................................................................................... 46
5.2 Variance..................................................................................................................................... 47
5.3 Difference between variance and standard deviation.............................................................. 48
5.4 Range......................................................................................................................................... 49
5.5 Quartile...................................................................................................................................... 50
5.6 Interquartile range .................................................................................................................... 51
5.7 Example dispersion parameter ................................................................................................. 51
6. Frequency table............................................................................................................................. 54


4
6.1 Absolute and relative frequencies ............................................................................................ 54
6.2 Valid percent ............................................................................................................................. 55
6.3 Frequency table in statistics ...................................................................................................... 56
6.4 Example frequency table........................................................................................................... 57
6.5 Frequency table in APA style..................................................................................................... 59
7. Contingency table (Crosstab) ........................................................................................................ 60
7.1 Crosstabs in statistics ................................................................................................................ 60
7.2 Interpretatation of crosstabs .................................................................................................... 61
7.3 Example crosstab....................................................................................................................... 62
7.4 Testing a crosstab for significance............................................................................................. 63
8. Charts............................................................................................................................................. 64
8.1 Bar chart .................................................................................................................................... 65
8.2 Bar chart for frequencies........................................................................................................... 66
8.3 Grouped bar charts ................................................................................................................... 67
8.4 Bar chart for mean values ......................................................................................................... 68
8.5 Error bar .................................................................................................................................... 68
8.6 Example bar chart...................................................................................................................... 69
8.7 Histogram .................................................................................................................................. 71
8.8 Histogram example ................................................................................................................... 72
8.9 Bar chart vs. Histogram ............................................................................................................. 73
8.10 Scatter plot ................................................................................................................................ 74
8.11 Line charts ................................................................................................................................. 75
8.12 Boxplot ...................................................................................................................................... 76
8.13 Bland-Altman plot ..................................................................................................................... 81
8.14 Create charts online with DATAtab ........................................................................................... 85
9. Inferential Statistics....................................................................................................................... 87
9.1 Hypotheses................................................................................................................................ 87
9.2 Null and alternative hypothesis................................................................................................. 90
9.3 Difference and correlation hypotheses..................................................................................... 91
9.4 Directional and undirectional hypotheses ................................................................................ 93
9.5 Hypothesis Testing .................................................................................................................... 96
9.5.1 Hypothesis testing and the null hypothesis .......................................................................... 97
9.5.2 The uncertainty in hypothesis testing ................................................................................... 98
9.5.3 Level of significance or probability of error .......................................................................... 98
9.5.4 Example significance level and p-value ............................................................................... 100
9.5.5 Types of errors..................................................................................................................... 100


5
9.5.6 Significance vs effect size .................................................................................................... 101
9.5.7 Choosing the appropriate hypothesis test .......................................................................... 102
9.5.8 Examples for hypothesis tests............................................................................................. 105
9.6 The p-value .............................................................................................................................. 106
9.6.1 Defining the p-value ............................................................................................................ 106
9.6.2 Using the p-value................................................................................................................. 107
9.6.3 Significance level ................................................................................................................. 108
9.6.4 One-tailed p-values ............................................................................................................. 109
9.6.5 Calculate p-value ................................................................................................................. 110
9.6.6 Statistical tests and the p-value .......................................................................................... 111
9.6.7 Specify the p-value .............................................................................................................. 112
10. Checking assumptions of statistical tests................................................................................ 114
10.1 Levene test of variance homogeneity ..................................................................................... 114
10.2 Levene test example................................................................................................................ 116
10.3 Interpreting the Levene Test ................................................................................................... 118
10.4 Normality test.......................................................................................................................... 119
10.4.1 Statistical test for normal distribution ................................................................................ 119
10.4.2 Disadvantage of analytical tests for normal distribution .................................................... 121
10.4.3 Graphical test for normal distribution................................................................................. 122
10.5 Multicollinearity test ............................................................................................................... 124
10.5.1 How to avoid multicollinearity? .......................................................................................... 125
10.5.2 Multicollinearity test ........................................................................................................... 126
10.5.3 Tolerance value ................................................................................................................... 126
10.5.4 VIF Multicollinearity ............................................................................................................ 127
11. Statistical tests for differences ................................................................................................ 128
11.1 One sample t-test .................................................................................................................... 128
11.1.1 Basics of the one sample t-test ........................................................................................... 128
11.1.2 Examples of a t-test for one sample.................................................................................... 129
11.1.3 Assumptions of the one-sample t-test ................................................................................ 130
11.1.4 Hypotheses for the one-sample t-test ................................................................................ 131
11.1.5 Calculation of the one-sample t-test................................................................................... 132
11.1.6 One sample t-test with example ......................................................................................... 134
11.1.7 APA format | One-sample t-test ......................................................................................... 137
11.2 T-test for independent samples (unpaired t-test)................................................................... 138
11.2.1 Using an independent t-test................................................................................................ 138
11.2.2 Purpose of the independent/unpaired t-test...................................................................... 139


6
11.2.3 Examples for the unpaired t-test......................................................................................... 140
11.2.4 Research question and hypotheses for the unpaired t-test ............................................... 140
11.2.5 Assumptions unpaired/independent t-test......................................................................... 142
11.2.6 Calculate t-test for independent samples ........................................................................... 143
11.2.7 Confidence interval for the true mean difference .............................................................. 145
11.2.8 One-sided and two-sided unpaired t-test ........................................................................... 146
11.2.9 Effectsize unpaied t-test...................................................................................................... 146
11.2.10 Example t-test for independent samples ........................................................................ 147
11.2.11 Interpretation t-test for independent samples............................................................... 150
11.2.12 Report a t-test for independent samples ........................................................................ 151
11.3 Paired-samples t-test .............................................................................................................. 154
11.3.1 Why do you need the dependent t-test?............................................................................ 154
11.3.2 What is the advantage of a dependent t-test over an independent t-test?....................... 156
11.3.3 Examples of the t-test for paired samples .......................................................................... 156
11.3.4 Research question and hypotheses of the paired t-test ..................................................... 157
11.3.5 Assumptions paired t-test ................................................................................................... 158
11.3.6 Calculating a paired t-test ................................................................................................... 159
11.3.7 Example t-test for dependent samples with DATAtab........................................................ 160
11.3.8 Interpretation of a t-test for dependent samples............................................................... 163
11.3.9 Effect size dependent t-test ................................................................................................ 163
11.4 Mann-Whitney U test.............................................................................................................. 164
11.4.1 Assumptions Mann-Whitney U test .................................................................................... 165
11.4.2 Hypotheses Mann-Whitney U test ...................................................................................... 165
11.4.3 Calculate Mann-Whitney U test .......................................................................................... 166
11.4.4 Calculate Mann-Whitney U test with tied ranks ................................................................. 168
11.4.5 Mann-Whitney U test Example with DATAtab.................................................................... 170
11.4.6 Interpret Mann-Whitney U test .......................................................................................... 174
11.4.7 Mann-Whitney U test and effect size.................................................................................. 174
11.5 Wilcoxon test........................................................................................................................... 175
11.5.1 Assumptions of the Wilcoxon test ...................................................................................... 175
11.5.2 Hypotheses in the Wilcoxon test......................................................................................... 176
11.5.3 Wilcoxon test and test power ............................................................................................. 176
11.5.4 Calculate Wilcoxon test ....................................................................................................... 176
11.5.5 Calculate Wilcoxon signed-rank test with tied ranks .......................................................... 179
11.5.6 Effect size in the Wilcoxon signed-rank test ....................................................................... 181
11.5.7 Example Wilcoxon test with DATAtab................................................................................. 182


7
12. Frequency analysis .................................................................................................................. 184
12.1 Binomial test............................................................................................................................ 184
12.1.1 Hypotheses in binomial test................................................................................................ 184
12.1.2 Binomial test calculation ..................................................................................................... 185
12.1.3 Binomial test example......................................................................................................... 185
12.1.4 Interpretation of a Binomial Test ........................................................................................ 187
12.2 Chi-square test ........................................................................................................................ 188
12.2.1 Applications of the Chi-Square Test .................................................................................... 189
12.2.2 Calcualtion of Chi-Square- test............................................................................................ 190
12.2.3 Chi-Square Test of Independence ....................................................................................... 191
12.2.4 Chi-square distribution test................................................................................................. 193
12.2.5 Chi-square homogeneity test .............................................................................................. 194
12.2.6 Effect size for Chi-square test.............................................................................................. 194
12.2.7 Effect size vs. p-value .......................................................................................................... 195
12.2.8 Example: Chi-square test with DATAtab ............................................................................. 196
13. Statistical tests to test for differences in more than two groups. .......................................... 201
13.1 Analysis of Variance (ANOVA) ................................................................................................. 201
â¢ One-factor (or one-way) ANOVA................................................................................................. 201
â¢ Two-factors (or two-way) ANOVA............................................................................................... 201
â¢ One-factor ANOVA with repeated measurements ..................................................................... 201
â¢ Two-factors ANOVA with repeated measurements.................................................................... 201
13.1.1 Why not calculate multiple t-tests? .................................................................................... 202
13.1.2 Difference between one-way and two-way ANOVA........................................................... 202
13.1.3 Analysis of variance with and without repeated measures ................................................ 203
13.2 One-factor ANOVA .................................................................................................................. 204
13.3 One-factor ANOVA example.................................................................................................... 204
13.4 Analysis of variance hypotheses.............................................................................................. 206
13.5 Assumptions for one-way analysis of variance ....................................................................... 207
13.6 Welch's ANOVA ....................................................................................................................... 208
13.7 Effect size Eta squared (Î·2)...................................................................................................... 208
13.8 Two factor analysis of variance ............................................................................................... 208
13.9 Calculate example with DATAtab ............................................................................................ 209
13.10 Repeated Measures ANOVA................................................................................................ 211
13.10.1 What are dependent samples? ....................................................................................... 211
13.10.2 Difference of analysis of variance with and without repeated measurements .............. 212
13.10.3 Example of repeated measures ANOVA.......................................................................... 212


8
13.10.4 Research question and hypotheses................................................................................. 213
13.10.5 Assumptions ANOVA with repeated measures............................................................... 214
13.10.6 Results of the one-factor analysis of variance with repeated measures. ....................... 216
13.10.7 Effect size for repeated measures ANOVA...................................................................... 216
13.10.8 Bonferroni Post-hoc-Test ................................................................................................ 217
13.10.9 Calculate ANOVA with measurement repetitions with DATAtab ................................... 218
13.10.10 Calculate a repeated measures ANOVA by hand ............................................................ 219
13.11 Two-way ANOVA (without repeated measures) ................................................................. 221
13.11.1 What is a factor?.............................................................................................................. 222
13.11.2 Two factors...................................................................................................................... 223
13.11.3 Example Two-Way ANOVA.............................................................................................. 224
13.11.4 Hypotheses ...................................................................................................................... 224
13.11.5 Assumptions .................................................................................................................... 225
13.11.6 Calculation of a two-way ANOVA .................................................................................... 226
13.11.7 Calculating two-way ANOVA with DATAtab.................................................................... 230
13.11.8 Interpreting two-way ANOVA.......................................................................................... 233
13.11.9 Interaction effect............................................................................................................. 234
13.12 Two-way ANOVA with measurement repetition................................................................. 236
13.12.1 Sample with measurement repetition ............................................................................ 237
13.12.2 Example two-way ANOVA with repeated measures....................................................... 238
13.12.3 Hypotheses ...................................................................................................................... 239
13.12.4 Assumptions of the two-way analysis of variance with repeated measures .................. 239
13.12.5 Calculate two-way ANOVA with repeated measures...................................................... 240
13.12.6 Interpret two-way analysis of variance with repeated measures................................... 242
13.13 Kruskal-Wallis test ............................................................................................................... 244
13.13.1 Examples for the Kruskal-Wallis test ............................................................................... 244
13.13.2 Research question and hypotheses in the Kruskal-Wallis test........................................ 245
13.13.3 Assumptions of the Kruskal-Wallis test........................................................................... 245
13.13.4 Calculate Kruskal-Wallis test ........................................................................................... 246
13.13.5 Kruskal-Wallis test example ............................................................................................ 248
14. Statistical methods for testing correlations ............................................................................ 251
14.1 Correlation............................................................................................................................... 251
14.1.1 Correlation and causality..................................................................................................... 251
14.1.2 Correlation and causality example...................................................................................... 252
14.1.3 Correlation interpretation................................................................................................... 252
14.1.4 Direction of correlation ....................................................................................................... 252


9
14.1.5 Strength of correlation ........................................................................................................ 253
14.1.6 Scatter plot and correlation ................................................................................................ 254
14.1.7 Test correlation for significance .......................................................................................... 255
14.1.8 Directional and non-directional hypotheses ....................................................................... 256
14.2 Pearson correlation analysis.................................................................................................... 257
14.2.1 Pearson Correlation assumptions ....................................................................................... 259
14.3 Spearman rank correlation...................................................................................................... 260
14.4 Point biserial correlation ......................................................................................................... 261
14.5 Partial correlation.................................................................................................................... 261
14.5.1 Calculation of the partial correlation .................................................................................. 262
14.5.2 Partial correlation example ................................................................................................. 263
14.5.3 Partial correlation 2nd order............................................................................................... 263
14.5.4 Example: Pearson correlation ............................................................................................. 264
14.5.5 Directional (one-sided) correlation hypothesis................................................................... 268
15. Regression analysis.................................................................................................................. 269
15.1 Basics of regression ................................................................................................................. 269
15.1.1 Using a regression analysis.................................................................................................. 270
15.1.2 Types of regression analysis ................................................................................................ 271
15.1.3 Dummy variables and Reference category ......................................................................... 272
15.1.4 Examples of regression:....................................................................................................... 273
15.2 Linear regression ..................................................................................................................... 275
15.2.1 Simple Linear Regression..................................................................................................... 276
15.2.2 Multiple linear regression ................................................................................................... 279
15.2.2.1 Multiple Regression vs. Multivariate Regression ............................................................ 280
15.2.2.2 Coefficient of determination ........................................................................................... 280
15.2.2.3 Adjusted R2 ..................................................................................................................... 281
15.2.2.4 Standard estimation error............................................................................................... 281
15.2.2.5 Standardized and unstandardized regression coefficient ............................................... 281
15.2.2.6 Assumptions of Linear Regression................................................................................... 282
15.2.2.7 Linearity ........................................................................................................................... 282
15.2.2.8 Homoscedasticity ............................................................................................................ 283
15.2.2.9 Normal distribution of the error ..................................................................................... 284
15.2.2.10 Multicollinearity .......................................................................................................... 285
15.2.2.11 Significance test and Regression ................................................................................. 286
15.2.2.12 Example linear regression ........................................................................................... 288
15.2.2.13 Interpretation of the results........................................................................................ 289


10
15.2.2.14 Presenting the results of the regression ..................................................................... 290
15.2.3 Logistic regression ............................................................................................................... 291
15.2.3.1 What is logistic regression?............................................................................................. 292
15.2.3.2 Logistic regression and probabilities ............................................................................... 292
15.2.3.3 Calculate logistic regression ............................................................................................ 293
15.2.3.4 Logistic function .............................................................................................................. 294
15.2.4 Maximum Likelihood Method ............................................................................................. 296
15.2.4.1 The Likelihood Function .................................................................................................. 296
15.2.4.2 Maximum Likelihood Estimator ...................................................................................... 297
15.2.5 Multinomial logistic regression ........................................................................................... 297
15.2.6 Interpretation of the results................................................................................................ 298
15.2.7 Pseudo-R squared................................................................................................................ 299
15.2.8 Null Model ........................................................................................................................... 299
15.2.9 Cox and Snell R-square ........................................................................................................ 299
15.2.10 Nagelkerkes R-square...................................................................................................... 300
15.2.11 McFadden's R-square ...................................................................................................... 300
15.2.12 Chi2 Test and Logistic Regression.................................................................................... 300
15.2.13 Example logistic regression ............................................................................................. 301
15.2.14 Calculating logistic regression with DATAtab.................................................................. 302
16. Factor analysis ......................................................................................................................... 304
16.1 What is a factor? ..................................................................................................................... 304
16.2 Example factor analysis ........................................................................................................... 305
16.3 Research questions factor analysis ......................................................................................... 306
16.4 Factor load, eigenvalue, communalities.................................................................................. 307
16.5 Correlation Matrix ................................................................................................................... 308
16.6 Factor Analysis and dimensionality ......................................................................................... 308
16.6.1 Eigenvalue criterion (Kaiser criterion)................................................................................. 308
16.6.2 Scree-Test ............................................................................................................................ 309
16.6.3 Communalities..................................................................................................................... 310
16.6.4 Component matrix .............................................................................................................. 310
16.6.5 Rotation Matrix ................................................................................................................... 311
16.6.6 Varimax Rotation................................................................................................................. 311
17. Cluster analysis........................................................................................................................ 313
17.1 Example Hierarchical Cluster Analysis..................................................................................... 313
17.1.1 Calculating a Hierarchical Cluster Analysis.......................................................................... 314
17.1.2 Distance between two points.............................................................................................. 315


11
17.1.3 Euclidean Distance .............................................................................................................. 316
17.1.4 Manhattan Distance ............................................................................................................ 317
17.1.5 Maximum Distance.............................................................................................................. 317
17.1.6 Linking methods .................................................................................................................. 318
17.1.6.1 Single-linkage................................................................................................................... 319
17.1.6.2 Complete-linkage............................................................................................................. 319
17.1.6.3 Average-linkage ............................................................................................................... 320
17.1.7 Example Hierarchical Cluster Analysis................................................................................. 320
17.1.7.1 Calculate hierarchical cluster analysis with DATAtab...................................................... 326
17.2 K-means cluster analysis ......................................................................................................... 328
17.2.1 Optimal cluster number ...................................................................................................... 329
17.2.2 Elbow curve ......................................................................................................................... 329
17.2.3 Scaling data for k-means clustering..................................................................................... 330
17.2.4 K-means clustering calculator ............................................................................................. 330
17.2.5 Key Features ........................................................................................................................ 331
18. Market Basket Analysis [Association Analysis]........................................................................ 332
18.1 What does association analysis do?........................................................................................ 332
18.2 Market Basket Analysis Example............................................................................................. 333
18.3 Interpreting the results of a Market basket analysis .............................................................. 335
18.3.1 Frequency ............................................................................................................................ 335
18.3.2 Support ................................................................................................................................ 336
18.3.3 Confidence........................................................................................................................... 336
18.3.4 Lift........................................................................................................................................ 336
18.3.5 Market basket analysis and data mining............................................................................. 336
18.3.6 Critical note on the market basket analysis ........................................................................ 337
19. Cronbachâs Alpha..................................................................................................................... 338
19.1 Latent variables ....................................................................................................................... 338
19.2 Assumptions for Cronbachâs Alpha ......................................................................................... 340
19.3 Calculate Cronbachâs Alpha..................................................................................................... 341
19.4 Example Cronbachâs Alpha ...................................................................................................... 341
19.5 Interpret Cronbachâs Alpha ..................................................................................................... 344
20. Cohenâs Kappa ......................................................................................................................... 345
20.1 Cohenâs Kappa Example .......................................................................................................... 345
20.2 Inter-rater reliability................................................................................................................ 346
20.3 Use cases for Cohenâs Kappa................................................................................................... 346
20.4 Cohenâs Kappa reliability and validity ..................................................................................... 347


12
20.5 Calculate Cohenâs Kappa ......................................................................................................... 348
20.6 Cohen's Kappa interpretation ................................................................................................. 351
20.7 Cohen's Kappa Standard Error (SE) ......................................................................................... 352
20.8 Calculating Standard Error of Cohen's Kappa.......................................................................... 352
20.9 Interpreting Standard Error..................................................................................................... 352
20.10 Calculate Cohen's Kappa with DATAtab .............................................................................. 353
21. Weighted Cohenâs Kappa ........................................................................................................ 355
21.1 Reliability and validity ............................................................................................................. 356
21.2 Calculating weighted Cohen's Kappa ...................................................................................... 356
21.3 Calculate expected frequency ................................................................................................. 358
21.4 Calculate weighting matrix...................................................................................................... 359
21.5 Linear and quadratic weighting............................................................................................... 360
21.6 Calculate weighted Kappa ....................................................................................................... 361
21.7 Calculating Cohen's weighted kappa with DATAtab ............................................................... 362
22. Fleiss Kappa ............................................................................................................................. 365
22.1 Fleiss Kappa Example............................................................................................................... 365
22.2 Fleiss Kappa with repeated measurement.............................................................................. 366
22.3 Fleiss Kappa reliability and validity.......................................................................................... 367
22.4 Calculate Fleiss Kappa ............................................................................................................. 368
22.5 Fleiss Kappa interpretation ..................................................................................................... 371
22.6 Calculate Fleiss Kappa with DATAtab ...................................................................................... 372
23. Survival time analysis .............................................................................................................. 374
23.1 Basics of survival time analysis................................................................................................ 374
23.2 Use cases for survival time analysis ........................................................................................ 375
23.3 Example of survival time analysis............................................................................................ 377
23.4 Censored data ......................................................................................................................... 378
23.5 Methods of survival time analysis ........................................................................................... 379
23.6 Kaplan-Meier Curve................................................................................................................. 380
23.6.1 Survival rate......................................................................................................................... 380
23.6.2 Interpreting the Kaplan-Meier curve .................................................................................. 382
23.6.3 Calculating the Kaplan-Meier curve .................................................................................... 382
23.6.4 Draw Kaplan Meier curve .................................................................................................... 384
23.6.5 Censored data ..................................................................................................................... 385
23.6.6 Comparing different groups ................................................................................................ 387
23.6.7 Kaplan-Meier curve assumptions........................................................................................ 387
23.6.8 Create Kaplan Meier curve with DATAtab........................................................................... 388


13
23.7 Log Rank Test........................................................................................................................... 389
23.7.1 Hypotheses in the Log Rank Test......................................................................................... 392
23.7.2 Assumptions for the Log Rank Test ..................................................................................... 393
23.7.3 Calculate Log Rank Test....................................................................................................... 393
23.7.4 Calculate Log Rank Test with DATAtab................................................................................ 398
23.8 Cox regression ......................................................................................................................... 401
23.8.1 Survival time analysis .......................................................................................................... 401
23.8.2 Censoring............................................................................................................................. 402
23.8.3 Cox Regression Example...................................................................................................... 403
23.8.4 Calculate Cox Regression with DATAtab ............................................................................. 404
23.8.5 Interpretation of the Cox Regression .................................................................................. 405
23.8.6 Assumptions of a Cox Regression........................................................................................ 406
23.8.7 Calculate survival time analysis with DATAtab.................................................................... 408
References........................................................................................................................................... 412


14
List of Figures
Figure 1: Population and sample........................................................................................................... 18 Figure 2: Subareas of descriptive statistics ........................................................................................... 19 Figure 3: Task of inferential statistics.................................................................................................... 23 Figure 4: Methods of inferential statistics ............................................................................................ 24 Figure 5: Example inferential statistics ................................................................................................. 25 Figure 6: Scale or measurement levels.................................................................................................. 27 Figure 7: Sampling ................................................................................................................................. 31 Figure 8: Position dimensions ............................................................................................................... 36 Figure 9: Representation of the median ............................................................................................... 39 Figure 10: Median for even and odd number of feature carriers ......................................................... 39 Figure 11: Mean and median in comparison......................................................................................... 40 Figure 12: Measures of dispersion at a glance...................................................................................... 45 Figure 13: Calculation of the standard deviation .................................................................................. 47 Figure 14: Representation of the span.................................................................................................. 49 Figure 15: Illustration of the interquartile range .................................................................................. 51 Figure 16: Example of a frequency table............................................................................................... 54 Figure 17: Percentages and valid percentages...................................................................................... 55 Figure 18: Frequency of car brands....................................................................................................... 59 Figure 19: Example of a crosstab........................................................................................................... 60 Figure 20: Creation of a crosstab .......................................................................................................... 61 Figure 21: The most popular diagrams at a glance ............................................................................... 64 Figure 22: Horizontal and vertical bar charts ........................................................................................ 65 Figure 23: Bar chart for frequencies...................................................................................................... 66 Figure 24: Bar chart falls in the hospital................................................................................................ 67 Figure 25: Grouped bar charts .............................................................................................................. 67 Figure 26: Bar chart for mean values .................................................................................................... 68 Figure 27: Error bar ............................................................................................................................... 69 Figure 28: Example of a bar chart ......................................................................................................... 70 Figure 29: Nested bar chart................................................................................................................... 70 Figure 30: Example of a histogram........................................................................................................ 71 Figure 31: Example of a histogram........................................................................................................ 73 Figure 32: Example of a scatter plot...................................................................................................... 74 Figure 33: Interrelationships in the scatter diagram............................................................................. 75 Figure 34: Boxplot example................................................................................................................... 76 Figure 35: Interpret boxplot .................................................................................................................. 78 Figure 36: Interpret boxplot part 2 ....................................................................................................... 79 Figure 37: Interpret boxplot part 3 ....................................................................................................... 80 Figure 38: Interpret boxplot part 4 ....................................................................................................... 81 Figure 39: Bland-Altman Plot ................................................................................................................ 82 Figure 40: Create charts with DATAtab ................................................................................................. 85 Figure 41: Aim of inferential statistics................................................................................................... 87 Figure 42: Properties of hypotheses ..................................................................................................... 88 Figure 43: Hypotheses in the research process..................................................................................... 89 Figure 44: Difference hypotheses ......................................................................................................... 91 Figure 45: Correlation hypotheses ........................................................................................................ 92


15
Figure 46: Directional and undirectional hypotheses ........................................................................... 93 Figure 47: One-sided and two-sided testing ......................................................................................... 93 Figure 48: Logic of statistical inference................................................................................................. 96 Figure 49: Hypothesis testing and research process............................................................................. 97 Figure 50: Uncertainty in hypothesis testing ........................................................................................ 98 Figure 51: Types of errors in hypothesis tests..................................................................................... 101 Figure 52: Selection of the scale level with DATAtab.......................................................................... 103 Figure 53: Overview Hypothesis tests ................................................................................................. 104 Figure 54: Example p-value ................................................................................................................. 106 Figure 55: The interpretation of the p-value....................................................................................... 107 Figure 56: The t-distribution and chi-square distribution ................................................................... 111 Figure 57: The Levene test of variance homogeneity ......................................................................... 115 Figure 58: Graphical test of variance homogeneity ............................................................................ 117 Figure 59: Explanation of the Levene test........................................................................................... 118 Figure 60: Tests for normal distribution.............................................................................................. 120 Figure 61: Disadvantage of analytical tests for normal distribution ................................................... 121 Figure 62: Histogram with normal distribution curve ......................................................................... 122 Figure 63: Q-Q plot for testing normal distribution with DATAtab..................................................... 123 Figure 64: Results of the test for normal distribution with DATAtab ................................................. 123 Figure 65: Multicollinearity ................................................................................................................. 125 Figure 66: The 3 variants of the t-test................................................................................................. 129 Figure 67: One-sided and two-sided t-test.......................................................................................... 130 Figure 68: t-test statistic...................................................................................................................... 132 Figure 69: t-test types ......................................................................................................................... 138 Figure 70: Mean difference ................................................................................................................. 139 Figure 71: Calculation of the t-test for independent samples. ........................................................... 144 Figure 71: Calculation of the t-value ................................................................................................... 145 Figure 73: Boxplot showing the t-test results. .................................................................................... 151 Figure 74: Forms of the t-test.............................................................................................................. 154 Figure 75: t-test for dependent and paired samples, respectively. .................................................... 155 Figure 76: t-test and Mann-Whitney U-test........................................................................................ 164 Figure 77: Assumptions of the U-test.................................................................................................. 165 Figure 78: Calculate rank sums............................................................................................................ 166 Figure 79: Calculation of the Wilcoxon test ........................................................................................ 177 Figure 80: Example of categorical variables........................................................................................ 188 Figure 81: Use of the chi-square test .................................................................................................. 188 Figure 82: From questionnaire to crosstab ......................................................................................... 189 Figure 83: Types of analysis of variance.............................................................................................. 202 Figure 84 Variance elucidation of the ANOVA .................................................................................... 205 Figure 85: Measurement repetitions .................................................................................................. 211 Figure 86: Independent and dependent sample................................................................................. 212 Figure 87: Example independet and dependet sample ...................................................................... 213 Figure 88: Example null and alternative hypothesis ........................................................................... 214 Figure 89: Two-way ANOVA ................................................................................................................ 221 Figure 90: One-factorial vs. two-factorial ANOVA............................................................................... 224 Figure 91: Interaction effect part 1 ..................................................................................................... 234 Figure 92: Interaction effect part 2 ..................................................................................................... 235 Figure 93: Two-way ANOVA with measurement repetition ............................................................... 236


16
Figure 94: Example two-way ANOVA with measurement repetition ................................................. 238 Figure 86: Analysis of variance and Kruskal-Wallis test ...................................................................... 244 Figure 96: Assumptions of the Kurskal-Wallis test.............................................................................. 246 Figure 97: Scatter plot and correlation ............................................................................................... 254 Figure 97: Strength and direction of the correlation coefficients....................................................... 258 Figure 99: Partial correlation............................................................................................................... 262 Figure 100: Bogus correlation storks and birth rate ........................................................................... 263 Figure 101: Question of the regression............................................................................................... 269 Figure 102: Types of regression........................................................................................................... 273 Figure 103: Simple and multiple linear regression.............................................................................. 275 Figure 104: Scatter plot to show the correlation. ............................................................................... 276 Figure 105: Representation of the regression line.............................................................................. 277 Figure 106: Factors influencing a disease in the regression model .................................................... 292 Figure 107: Limits of linear regression ................................................................................................ 293 Figure 108: The logistic function ......................................................................................................... 294 Figure 109: Approximation of the logistic function............................................................................. 295 Figure 110: Likelihood function........................................................................................................... 296 Figure 111: Basics of factor analysis.................................................................................................... 306 Figure 112: Market basket analysis 1 .................................................................................................. 332 Figure 113: Market basket analysis 2 .................................................................................................. 333 Figure 113: Market basket analysis 3 .................................................................................................. 333 Figure 115: Scale with 4 items............................................................................................................. 338 Figure 116: Latent variable.................................................................................................................. 339 Figure 117: One and multiple latent variable(s).................................................................................. 340 Figure 118: Cohenâs Kappa example ................................................................................................... 346 Figure 119: Calculation of Cohenâs Kappa example ............................................................................ 348 Figure 120: Cohenâs Kappa calculation with DATAtab ........................................................................ 353 Figure 121: Calculating Fleiss Kappa with DATAtab ............................................................................ 373 Figure 122: Survival time analysis ....................................................................................................... 374 Figure 123: Survival time analysis use case......................................................................................... 375 Figure 123: Survival time analysis example......................................................................................... 377 Figure 125: Kaplan Meier Curve .......................................................................................................... 384 Figure 125: Log Rank Test.................................................................................................................... 389
List of tables
Table 1: Table of t-values .................................................................................................................... 133 Table 2: Strength of the correlation.................................................................................................... 253 Table 5: Results of the linear regression ............................................................................................. 289


17
Introduction
This book provides an overview of key topics in statistics. The main methods
and features of descriptive and inferential statistics are described and
illustrated with graphics. In addition, the book offers step-by-step
explanations for data analysis with the statistics software DATAtab
(datatab.net)
The aim is to present the background of the statistical methods and their
implementation in DATAtab in an easy-to-understand and clear way.
We start with the basics of descriptive and inferential statistics, their
differences, and applications. This is followed by an introduction to the central
basic concepts of statistics. The focus is on the concepts of variable or
characteristic, scale or level of measurement, sample, population, and
complete survey.
We then move on to statistical procedures for testing for differences in more
than two groups and look at various forms of analysis of variance. Building on
this, we look at statistical techniques for testing correlations and explore the
field of correlation analysis and partial correlations.
Finally, we look at regression, with examples of linear and logistic regression.
We conclude with structure discovery methods such as factor analysis and k
means cluster analysis.
Throughout the book, we try to explain why each term or method is
important, at what point in the research process it is relevant, and what
questions it can be used to answer.
We hope you enjoy reading and learning!


18
1. Descriptive statistics and inferential
statistics
Descriptive statistics and inferential statistics, along with exploratory
statistics, are the main areas of statistics. Descriptive statistics provides tools
to describe a sample. Starting from the sample, inferential statistics can now
be used to make a statement about the population.
Figure 1: Population and sample
1.1 Subareas of statistics
One main area of statistics is to make a statement about a population. In most
cases it is not possible to get all data of the population, so a sample is taken.
This sample can now be described using descriptive statistics, e.g. what the
mean value is and how strongly the sample scatters.
But this is not yet a statement about the population, that is the task of the
inferential statistics. The inferential statistics takes a sample from the
population, in order to make inferences about the population with this
sample. So, the goal of inferential statistics is to infer the unknown
parameters of the population from the known parameters of a sample.


19
Therefore, inferential statistics try to infer conclusions that go beyond the
immediate data, unlike descriptive statistics. To achieve this, hypothesis tests
such as the t-test or analysis of variance are used in inferential statistics.
1.2 Descriptive statistics
After collecting data, one of the first things to do is to graph the data, calculate
the mean and get an overview of the distributions of the data. This is the task
of descriptive statistics.
Thus, the goal of descriptive statistics is to gain an overview of the distribution
of data sets. Descriptive statistics helps to describe and illustrate data sets.
Definition: The term descriptive statistics covers statistical methods for
describing data using statistical characteristics, charts, graphics or tables.
It is important here that only the properties of the respective sample are
described and evaluated. However, no conclusions are drawn about other
points in time or the population. This is the task of inferential statistics or
concluding statistics.
The various sub-areas of descriptive statistics can be summarized as follows:
Figure 2: Subareas of descriptive statistics


20
Depending on which question and which measurement scale is available,
different key figures, tables and graphics are used for evaluation. The best
known of these are:
â¢ Location parameter: Mean value, median, mode, sum
â¢ Dispersion parameter: Standard deviation, variance, range
â¢ Tables: Absolute, relative and cumulative Frequencies
â¢ Charts: Histograms, bar charts, box plots, scatter charts, matrix plots
The first group of descriptive statistics are location parameter like the mean
and mode. They are used to express a central tendency of the data set. They
therefore describe where the center of a sample is located or where most of
the sample is.
The second group are measures of dispersion. They provide information
about how much the values of a variable in a sample differ from each other.
Measures of dispersion can therefore describe how strongly the values of a
variable deviate from the mean value: Are the values rather close together,
i.e. are they similar, or are they far apart and thus differ greatly? A classic
example is the standard deviation.
Which measures of location or dispersion are suitable for describing the data
depends on the respective scales of measurement of the variable. Here, a
distinction can be made between metric, ordinal and nominal scales of
measurement.
Finally, a large area of descriptive statistics is diagrams such as the bar chart,
the pie chart, or the histogram.


21
Thatâs how it works with DATAtab:
With DATAtab you can create diagrams directly in your web browser. The
following example shows the steps involved.
Example: A sample of 10 male basketball players is drawn and their height is
measured in meters. To get started, go to datatab.net and copy the data
below into the Statistics Calculator table. Then you click on âDescriptive
Statisticsâ in the calculator and select the variable "Height".
DATAtab will now give you the following table with descriptive statistics on
the height of the players. The table shows the relevant dispersion and location
parameter.
Statistics
Height
Mean value 1.67
Median 1.655
Fashion 1.64
Total 16.7
Standard deviation 0.066
Variance 0.004
Minimum 1.55
Maximum 1.78
Range 0.23


22
Players Height
1 1.62
2 1.72
3 1.55
4 1.7
5 1.78
6 1.65
7 1.64
8 1.64
9 1.66
10 1.74


23
1.3 Inferential Statistics
What's inferential statistics? In contrast to descriptive statistics, inferential
statistics want to make a statement about the population. However, since it
is almost impossible in most cases to survey the entire population, a sample
is used, i.e. a small data set originating from the population.
With this sample a statement about the population can be made. An
example would be if a sample of 1,000 citizens is taken from the population
of all Canadian citizens.
Figure 3: Task of inferential statistics
Depending on which statement is to be made about the population or which
question is to be answered about the population, different statistical methods
or hypothesis tests are used.
The best known are the hypothesis tests with which a group difference can be
tested, such as the t-test, the chi-square test or the analysis of variance.
Then there are the hypothesis tests with which a correlation of variables can
be tested, such as correlation analysis and regression.


24
Figure 4: Methods of inferential statistics
Inferential statistics definition
Inferential statistics is a branch of statistics that uses various analytical tools
to draw conclusions about the population from sample data. For a given
hypothesis about the population, inferential statistics uses a sample and gives
an indication of the validity of the hypothesis based on the sample collected.
Example inferential statistics
In the example above, a sample of 10 basketball players was drawn and then
exactly this sample was described, this is the task of descriptive statistics. If
you want to make a statement about the population you need the inferential
statistics. For example, it could be of interest if basketball players are larger
than the average male population. To test this hypothesis a t-Test is
calculated, the t-test compares the sample mean with the mean of the
population.


25
Figure 5: Example inferential statistics
Furthermore, the question could arise whether basketball players are larger
than football players. For this purpose, a sample of football players is drawn,
and then the mean value of the basketball players can be compared with the
mean value of the football players using an independent t-test. Now a
statement can be made, for example, whether basketball players are larger
than football players in the population or not.
Since this statement is only made based on the samples and it can also be
pure coincidence that the basketball players are larger in exactly this sample,
the statement can only be confirmed or re-submitted with a certain
probability. In the table below you will find an overview of the most common
inferential statistical methods used to make a statement about the
population:


26
Thatâs how it works with DATAtab:
â¢ DATAtab's Hypothesis Test Calculator allows you to calculate the
various tests in the field of inferential statistics easily and directly online
in your browser.
â¢ You will find instructions on how to do this directly after the explanation
of each statistical test in this book.


27
2. Level of measurment
One of the most important properties of variables is the level of
measurement, also called scales of measurement. The measurement scale is
important because it determines the permissible arithmetic operations and
thus specifies the possible statistical tests. The higher the level of
measurement, the more comparative statements and arithmetic operations
are possible.
Figure 6: Scale or measurement levels
The level of measurement of a variable can be either nominal, ordinal or
metric. In a nutshell: For nominal variables the values can be differentiated,
for ordinal variables the values can be sorted and for metric scale level the
distances between the values can be calculated. Nominal and ordinal
variables are also called categorical variables.
2.1 Nominal variables
The nominal scale is the lowest scale level in statistics and thus has the lowest
information content. Possible expressions of the variables can be
distinguished, but a meaningful order is not possible. If there are only two
expressions, such as in the case of gender (male and female), we also speak
of dichotomous or binary variables.
âª Only relations "equal", "unequal" possible
âª No logical ranking of the categories
âª The order of the answer categories is interchangeable


28
âª Nominal characteristics with only two expressions are also called
"binary" or "dichotomous".
2.2 Ordinal variables
The ordinal level of measurement is the next higher level, it contains
nominal information, only with the difference that a ranking can be formed,
therefore the term ranking scale is often used. In these cases, however, the
distances between the values are not interpretable, so it is not possible to
make a statement about the absolute distance between two values. A classic
representative of the ordinal scale are school grades, here a ranking can be
formed, but it cannot be said that the distance between A and B is the same
as the distance between B and C.
â¢ Next higher scale of measurement
â¢ "equal" and "unequal" or "greater" and "smaller" can be determined
â¢ There is a logical hierarchy of categories
â¢ The distances between the numerical values are not equal, i.e. cannot
be interpreted


29
2.3 Categorical variables
Variables that have a nominal scale or an ordinal scale are also called
categorical variables. In other words, categorical is an umbrella term for
variables scaled nominally and ordinally.
Categorical variables can have a limited and usually fixed number of
expressions, e.g. country with Germany, Austria, ... or gender with female and
male. It is important, however, that it must be a finite number of categories
or groups. The different categories can have a ranking, but do not have to.
2.4 Metric variables
Metric variables have the highest possible level of measurement. With a
metric level of measurement, the characteristic values can be compared and
sorted and distances between the values can be calculated. Examples would
be the weight and age of subjects.
â¢ Highest level of measurement
â¢ Creation of rankings possible
â¢ "equal" and "unequal", "greater" and "smaller" can also be
determined
â¢ Differences and sums can be formed meaningfully


30
2.5 Ratio scale and interval scale
The metric level of measurement can be further subdivided into interval scale
and ratio scale. As the name suggests, the values of the ratio scale can be put
into a ratio. Thus, a statement like the following can be made: "One value is
twice as large as another". For this, an absolute zero must be available as a
reference.
Example ratio scale:
The time of marathon runners is measured. Here the statement can be made
that the fastest runner is twice as fast as the last runner. This is possible
because there is an absolute zero point at the beginning of the marathon
where all runners start from zero.
Example interval scale:
If, however, the stopwatch is forgotten to start at the start of the marathon
and only the differences are measured starting from the fastest runner, the
runners cannot be put in proportion. In this case it can be said how big the
interval between the runners is (e.g. runner A is 22 minutes faster than runner
B), but it cannot be said that runner A ran 20 percent faster than runner B.
The classic example is the temperature indication in degrees Celsius and
Kelvin. The zero point of the Kelvin temperature scale is absolute zero,
therefore it is a ratio scale. At degrees Celsius the absolute zero point is 
273.15 Â°C, therefore the value zero on the degree Celsius scale cannot be
assumed as natural zero and therefore it is an interval scale.


31
3. Sampling
In the following chapter we will discuss central concepts of sampling theory.
You will learn what a population is and how to select elements from it. The
way you draw your sample influences which statistical methods are useful for
your topic and what statements you can make.
When planning an empirical study (e.g. a survey), sampling is very important.
Before you start collecting your data, you need to decide how you will select
the people who will take part in your study.
3.1 Full or total survey vs. random sample
The first question to ask is whether you need to draw a sample, or whether
you will conduct a full or total survey. In a complete or total survey you collect
data on all persons of the population or you already have data on all these
persons. This is often the case, for example, when you are working with
administrative data (e.g., grade lists of all students at a university) or have
user data available (e.g., sales figures in an online store). In practice, full or
total surveys are usually difficult to implement because they are expensive
and time-consuming. Therefore, if you are writing your bachelor's or master's
thesis and want to do a survey, you will most likely have to define a sample.
(HÃ¤der 2010: p. 139)
Figure 7: Sampling


32
3.2 Population and sample
As explained above, in a complete or total survey you are working with
everyone in the population. This means that you have data on the whole
population. What is the population? The population is all the elements that
are of interest for the research. For example, it could be all the people you
want to find out about through a survey. A sample is a selection from the
entire group of elements, i.e. a selection from the population (HÃ¤der 2010, p.
141).
3.3 Types of sampling?
There are several ways in which you can draw a sample. Thus, a sampling
procedure defines the way and the steps you use to select the elements from
your population. Three main groups of sampling procedures can be
distinguished (cf. Diekmann 2008, p. 378):
âª Probability selection
âª Deliberate selection
âª Arbitrary selection
In the next section we will look at these three forms in more detail and discuss
examples. We will then look at how to deal with sampling in an online survey
and how to describe sampling in a bachelor or master thesis.


33
3.4 Probability selection
When you perform probability selection, you get a random sample. For
example, you need a list of all elements of the population from which you
randomly select individuals. It is important that each element of the
population has the same probability of being included in the sample.
An example of this would be a random selection of households from the
central population register of a city. Using a computer, you can then randomly
select from this register, for example, a sample of 1000 addresses in the city.
You then contact these households and ask them (or a selected member of
the household) to participate in your survey.
Probability selection is often difficult to implement in practice, however,
because in many cases there is no list of the population, or the procedure of
this selection is too elaborate to be implemented in smaller empirical studies.
You can do probability selection in a single-stage or multistage way. Single
stage would be that you select your items in one step. Multi-step means that
you make your selection in several steps. For example, in the first stage you
select 50 municipalities of a state and in the second stage you select 50
addresses per municipality from these municipalities. (Diekmann 2008: pp.
380-386)
3.5 Deliberate selection
Deliberate selection is based on certain criteria, and it is based on the
distribution of characteristics in the population. Quota sampling is a well
known example of deliberate selection. To draw a quota sample, you look at
how certain characteristics (e.g., age and gender) are distributed in your
population. Quota characteristics can be gender, age, educational attainment,
place of residence, different hierarchies in a company, length of tenure with
a company, etc.


34
For example, if you are doing a survey in retail and you see that in your
population there are 40% younger women (up to 39 years), 30% older women
(40 years and older), 20% younger men and 10% older men, you try to achieve
this distribution in your sample as well.
Quota sampling is especially widespread in the field of market and opinion
research and is also often implemented in the context of bachelor or master
theses. This form of sampling is less time-consuming and cost-intensive and
can also be implemented well in smaller empirical studies. However, an
important prerequisite is that you have information about your population
and know how certain characteristics (e.g. age, gender, etc.) are distributed
there.
3.6 Arbitrary selection
The third group of sampling methods is arbitrary selection. Here you do not
control the process of sample selection. Arbitrary selection is often used in
experiments in psychology. Here you do not select your test persons
purposefully, but it takes part, who can and would like to take part. (Diekmann
2008: p. 379).
3.7 Sample selection in online surveys
With online surveys, it is mostly more difficult to determine the sample
selection in advance. In most cases, there is no list of the population from
which a selection can be made. One possibility here is to repeatedly look at
the already completed questionnaires during the course of the survey and
check the distributions of quota characteristics. If, for example, you notice
that older women are underrepresented, you can still actively approach or
write to this target group. The goal is to get as close as possible to the quota
plan.
3.8 Sample description in bachelor or master thesis
No matter which sampling method you choose, it is very important that you
explain your approach well in your bachelor or master thesis and make it


35
comprehensible. It should be clear to the reader of your thesis what the
population and sample of your study are and how you have selected them.
In your paper, you should answer the following questions:
âª Who was selected and interviewed (sample, population)?
âª Why were these people selected?
âª How did you contact respondents?


36
4. Location parameter
In descriptive statistics, the mean, median and mode are location parameter
(measures of central tendency). Based on data collected in a sample, the
location parameter provide information about where the "center" of the
distribution lies.
Measures of location can be used to summarize or describe a list of data with
only one parameter. An example would be that the average duration of
studies of sports students at the university XY is 11.1 semesters.
Figure 8: Position dimensions
Together with the dispersion parameters, the position parameters thus
describe a distribution in statistics. The most commonly used position
parameters are the mode, the median and the mean. Which location
parameter is used depends on the scale level of the variable and the
robustness against outliers.


37
4.1 Mean value (arithmetic mean)
The mean value can only be calculated for metric variables, i.e. when metric
scale level is given. It indicates where the center of gravity of a distribution is
to be found. In everyday life, it is also referred to as the "average".
Definition: The arithmetic mean is the sum of all observations divided by
their number n.
How can the mean value be calculated?
The mean value can be calculated by adding all expressions of a variable
and finally dividing the sum by the number of characteristic expressions.
Example: A group of 5 statistics students was asked how many cups of
coffee they drink per week. The result is 21, 25, 10, 8 and 11 cups. The
average is thus 15.
Tip: With DATAtab's statistics calculator you can easily calculate the mean
value or the desired position parameter for your data.


38
4.2 Geometric mean and quadratic mean
When talking about mean or average, mostly the arithmetic mean is meant,
but there are also other types of mean values.
Other mean values are, for example, the geometric mean and the quadratic
mean also called Root Mean Square (RMS).
Geometric mean: If there are n positive numbers, the geometric mean is the
nth root of the product of the n values.
Root Square Mean: The root square mean is obtained by dividing the sum of
the squares by the number of values and taking the square root.
4.3 Median
If the measured values of a variable are ordered by size, the value in the
middle is the median. The median is therefore the "middle value" of a
distribution. It leads to a division of the series into two parts: One half is
smaller and one larger than the median.
Since the data are ordered for the calculation of the median, the variables
must have an ordinal or metric scale level.
Definition: For an ordered series, the median is the value that divides the
series into an equal lower and upper range.


39
Figure 9: Representation of the median
For the median to be calculated, the variable must be ordinally scaled.
Ordinally scaled means that there is a ranking between the values of a
variable. This applies, for example, to school grades, height, or salary. In the
case of a birthplace variable, however, it is not possible to establish a ranking,
and therefore the median cannot be calculated here.
If there is an odd number of characteristic carriers (persons) and thus an odd
number of expressions, then the median is a value that actually occurs.
If there is an even number of feature carriers (persons), the two middle
expressions are added, and their sum is divided by two.
Figure 10: Median for even and odd number of feature carriers


40
4.4 Mean and median in comparison
Compared to the mean, the median is much more robust against scattering.
An outlier usually has no influence on the median, but it has a more or less
large influence on the mean.
Figure 11: Mean and median in comparison
4.5 Mode (Modal value)
The mode is the most common value. The mode is therefore the most
frequent value in a distribution and corresponds to the highest value in the
distribution. It is therefore the value that is "typical" for a distribution.
The mode can be used for both metric and categorical (nominal or ordinal)
variables.
Definition: The mode is the value of a distribution that occurs most often.


41
Calculate mode
Example: In a sample of 70 managers from Berlin, 20 drive a Daimler, 25 a
BMW, 10 a VW and 15 an Audi. The car brand BMW is the most common.
Thus the mode is "BMW".
Therefore the mode can easily be read in a frequency table, it is the most
frequent observed value.
Attention: There can also be several mode values. If two or more points occur
with the greatest frequency, then there are several mode values. In this case
one speaks then of a bimodal or multimodal distribution.


42
4.6 Advantage and disadvantage of the Mean, Median and Mode
If the distribution is symmetric, the mean and median are equal, and if the distribution is symmetric and unimodal, all three measures are equal. As a rule, however, the three measures have different values. Now, of course, the question is which of the measures of central tendency to use. Unfortunately, there is no clear rule for this, only a few decision aids.
Mean: The mean value is by far the most used. The disadvantages of the mean are that it is sensitive to outliers, the value does not have to exist in the data and for the interpretation to be meaningful, the data should have metric scale level.
Median: The great advantage of the median is that it is very robust against outliers and that the data only have to be scaled ordinally.
Mode: The mode is the value that occurs most frequently, which has the advantage that the value actually occurs. Furthermore, the mode can also be calculated for data that cannot be ordered and thus have a nominal scale level. The disadvantage is that the mode does not take into account the other existing data.


43
Thatâs how it works with DATAtab:
â¢ To calculate location measures with DATAtab, simply open the statistics
calculator and insert your data into the table.
â¢ After selecting your variables, you can then calculate, for example, the
mean, median and mode of your data.
â¢ As an example, the score can be used in a statistics exam. To do this,
copy the data into the statistics calculator, click on "Descriptive
statistics" and select the variable "Score".
Student Score
14
25
35
48
59
6 12
7 14
8 16
9 17
10 20
The result then looks like this:
Score
Mean value 11
Median 10,5
Fashion 5


44
Here you will find a brief summary of the three position measures discussed
so that you have the definitions at a glance:
Calculate mean:
The mean value is obtained by dividing the sum of all values by their number
of values.
Calculate median:
Due to the even number of values, the median is obtained by adding the two
middle values. The sum is then divided by two.
Calculate modus:
To obtain the mode, the frequency of occurrence of each value is counted.
The value that occurs most often is the modal value. In this case, the value 5
is the only one that occurs twice, so the modal value in this example is 5.


45
5. Dispersion parameter
Standard deviation, variance and range are among the measures of dispersion
(Measurement of Variability) in descriptive statistics. They are calculated to
describe the scatter of values of a sample around a location parameter. Put
simply, dispersion parameters are a measure of how much a sample
fluctuates around a mean value.
So, position measures give you information about the center of your data,
scatter measures give you information about how much your data scatters
around that center.
Figure 12: Measures of dispersion at a glance
Measurement of Central Tendency give you the information about the centre
of your data, dispersion measures give you the information how much your
data is spread around this centre.
The most common measures of dispersion for metric variables are standard
deviation and variance. These two measures relate each characteristic of a
variable to the mean value and thus indicate how far the individual
characteristics are scattered around the mean value.


46
5.1 Standard deviation
The standard deviation indicates the spread of a variable around its mean
value. Thus, the standard deviation is the mean deviation (root mean square)
of all measured values from the mean.
The standard deviation thus indicates how much the distribution of values
scatters around the mean value. If the individual values scatter strongly
around the mean value, a large standard deviation of the variable results.
There are two slightly different equations for the calculation. On the one
hand, the entire population can be used to calculate the standard deviation.
On the other hand, it can also be calculated if only one sample is available. If
all values of the population are available, the following results are obtained:
Often, however, the data of the entire population are not available.
Therefore, a sample is usually used to estimate the standard deviation of the
population. In this case, the calculation results in
The difference between the two formulas is that one is divided by n and the
other by n-1. It is customary to use s for the standard deviation of a sample
and Ï for the standard deviation of the population.


47
Figure 13: Calculation of the standard deviation
5.2 Variance
Just like the standard deviation, the variance measures the deviation from the
mean. For the calculation of the variance, the sum of the squared variances is
divided by the number of values.
The variance thus describes the squared average distance from the mean.
Because the values are squared, the result has a different unit (the unit
squared) than the original values. Therefore, it is difficult to relate the results.


48
5.3 Difference between variance and standard deviation
So, the difference between the dispersion parameter variance and standard
deviation is that the standard deviation measures the average distance from
the mean and the variance measures the squared average distance from the
mean.
In other words, the variance is the squared standard deviation and the
standard deviation is the root of the variance.
However, this squaring results in a key figure that is difficult to interpret, since
the unit does not correspond to the original data.
For this reason, it is advisable to always use the standard deviation to describe
a sample, as this makes interpretation easier.


49
5.4 Range
The range, also called span, is the distance between the minimum and
maximum of a distribution, i.e. the distance between the smallest and the
largest value.
For example, if the height of 7 people is queried and the largest value is 1.90m
and the smallest is 1.50m, the span is calculated as 1.90m - 1.50m to 0.4m.
Definition Range: The range indicates the distance between the highest and
the lowest value in a sample.
The range or span, often abbreviated with R, is therefore calculated by
Figure 14: Representation of the span


50
5.5 Quartile
Quartiles divide your data into four parts, as equal as possible. For the
calculation quartiles, the data must be sorted from the smallest to the largest
value.
â¢ Quartile (Q1): The middle value between the smallest value (minimum)
and the median.
â¢ Quartile (Q2): The median of the data, i.e. 50% of the values are smaller
and 50% of the values are larger.
â¢ Quartile (Q3): The middle value between the median and the largest
value (maximum).
Thus, 25% of all values are below the lower quartile (Q1) and 75% are below
the upper quartile (Q3).


51
5.6 Interquartile range
In contrast to the range in which 100% of all values lie, one often wants to
know the range in which the middle 50% of all values lie. This scattering
parameter is called interquartile range (IQR). The upper and lower 25% of the
values are therefore not taken into account for the interquartile range.
Figure 15: Illustration of the interquartile range
5.7 Example dispersion parameter
The calculation of range, variance and standard deviation shall now be
illustrated by an example. For this purpose, the results of students in a
statistics exam (scores) will be used.
Student Score
14
25
35


52
Student Score
48
59
6 12
7 14
8 16
9 17
10 20


53
Thatâs how it works with DATAtab:
The calculator for descriptive statistics on DATAtab will give you the range,
variance and standard deviation.
Copy the above data into the Online Statistics Calculator, click on Descriptive
Statistics and select the Score variable. The result will look like this:


54
6. Frequency table
Frequency tables are created when you want to display the absolute and
relative frequencies of the values of your variables or characteristics. Thus, a
frequency table for your data shows you how often each characteristic occurs.
A frequency table for the variable gender, for example, shows how often the
characteristics male and female occur in the sample.
6.1 Absolute and relative frequencies
Absolute frequencies are those values that indicate how often the respective
category of a variable occurs.
Relative frequencies, on the other hand, indicate how often the respective
expressions occur in relation to all cases, and are therefore usually given as
percentages.
Depending on the subject area and the question, the categories or
characteristics can be, for example, persons, companies, locations, or
households.
Frequency tables are often created to get an initial overview of data.
Afterwards, the result can be displayed graphically in a bar chart.
Figure 16: Example of a frequency table


55
6.2 Valid percent
It is particularly important to pay attention to missing or invalid values when
creating and interpreting frequency tables. In the field of survey research,
missing values are usually found where people have answered with "no
answer," "Can't say," or "Don't know."
So that the statistics are not distorted by the missing values, you should
indicate both percentages and valid percentages in the frequency table.
To calculate valid percent, you only need to divide the absolute frequencies
of a characteristic by the valid cases.
If you asked 30 people in a survey what their favorite car brand was, and 7
ticked "Can't say", then there are 23 valid values.
If 5 people indicated Ford, then the valid percentages are 5/23 and 21.7%,
respectively.
Figure 17: Percentages and valid percentages


56
6.3 Frequency table in statistics
Frequency tables mostly consist of the following columns:
âª Absolute frequency
âª Percentages (=relative frequency)
âª Valid percent
The column of valid percentages is now the one that shows the relative
frequencies of a characteristic in percent, but only considers those cases that
have valid bets. Since missing values can always occur, it is advisable to also
use this form of percentages.
Example for valid percent:
Letâs assume that a Sunday poll is taken, and the question is asked "Which
party would you vote for if there were an election next Sunday?" it could turn
out that there are still some undecided people.
In this case, both the percentages and the valid percentages would be
interesting.
The percentage based on all values would therefore show how much support
there is for a party in relation to all respondents, including the undecided.
The percentage of valid values, on the other hand, indicates the level of
agreement among those who have already decided.


57
6.4 Example frequency table
The procedure of creating a frequency table will now be illustrated with an
example:
Letâs assume that in a statistics course the participants were asked which
brand of car they drive. The answers are displayed in the following table:
Student Car brand
1 VW
2
3 BMW
4 Skoda
5 Skoda
6 VW
7 BMW
8 Opel
9 Opel
10 Skoda
11 VW
12 Daimler


58
Thatâs how it works with DATAtab:
â¢ Simply copy the table into the online statistics software and select the
variable car brand.
â¢ Now you can choose which values you want to calculate.
â¢ The result of the frequency table now looks like this:
The table shows the absolute frequencies, the relative percentages, and the
valid percentages.


59
Finally, DATAtab automatically gives you a graphical visualization of the
result, here in the form of a bar chart:
Figure 18: Frequency of car brands
If the variable is metrically scaled, a histogram is used to display the
frequencies.
6.5 Frequency table in APA style
If you want to create a frequency table in APA format, you have to consider
the following:
Font Times New Roman, size 12
Labeling All tables must be numbered in APA format
Margins As few margins as possible should be used


60
7. Contingency table (Crosstab)
Crosstabs, also called contingency tables, are used in descriptive statistics to
obtain an overview of two, usually categorical variables. In the crosstab, you
can then read how often the combination of the values of two characteristics
occurs.
The frequency is given either in absolute or relative frequency. Therefore,
with a cross-tulationab in statistics, one gets an insight into how two variables
are related.
Figure 19: Example of a crosstab
7.1 Crosstabs in statistics
If two categorical variables are present, a crosstab is obtained by entering the
values of the variables in a table. For the first variable, the values are plotted
from left to right, for the second variable from top to bottom. The individual
cells are then filled with either the absolute or the relative frequency.
Crosstabs and market research
Crosstabs are very often used in market research because they can be used
to compare customers or products very well. For example, the following
questions can be answered:
âª Which insurance is preferred by which age group?
âª Are the car brands different in the city and in the country?
âª Which apple variety sells best in which season?


61
7.2 Interpretatation of crosstabs
How do you interpret a crosstab? A crosstab shows the frequencies of two
variables.
In each cell of a crosstab, the frequencies of the characteristic combinations
are entered; in the example above, "female and without a degree" occurs
exactly 6 times.
Figure 20: Creation of a crosstab
Rows and columns in the crosstab
This means that the values of one variable are plotted in the rows and the
values of the other variable are plotted in the columns. Usually, the
independent variable is plotted in the columns and the dependent variable in
the rows.
Absolute and relative frequencies for crosstabs
When creating a crosstab, either the absolute or the relative frequencies can
be output:
â¢ Absolute frequencies are those values that indicate how often the
respective combination of two characteristic values occurs.
â¢ Relative frequencies, on the other hand, indicate how often the
respective combination of expressions occurs in relation to all cases; it
is therefore usually expressed as a percentage.


62
7.3 Example crosstab
The creation of crosstabs will now be examined in more detail using an
example.
In the example it is assumed that on a rainy day a student counts how many
people "with" and how many "without" umbrellas come to the statistics
lecture.
In addition, the student makes a note of the gender of the students.
Thatâs how it works with DATAtab:
â¢ Just open the Statistics Calculator and copy the table above.
â¢ Then you select the variables "gender" and "using an umbrella" in the
"Descriptive statistics" section.
â¢ DATAtab will then automatically create a crosstab for you.


63
The result is shown in the following crosstab. The cross-tabulation now
contains the absolute frequencies of the respective feature combinations.
This ist he result:
7.4 Testing a crosstab for significance
A crosstab can be used to examine whether there is a relationship between the two variables. However, since a crosstab is a descriptive statistic, a statement can only be made about the sample. If a statement is to be made about the population, the chi-square test is required.


64
8. Charts
In charts, data is presented graphically, so they are mainly used in statistics to
get an initial overview of the collected data and to present information in an
easily understandable way.
The most commonly used charts in statistics are bar charts, histograms,
scatter plots, line plots, box plots or pie charts.
Figure 21: The most popular diagrams at a glance


65
8.1 Bar chart
Bar charts are probably the most commonly used charts in statistics. Bar
charts are usually used to show the frequency of different categories, but
also to visualize numerical data, such as sales figures or population statistics.
In a bar chart, the length of each bar is proportional to the value it
represents. The bars are usually arranged horizontally or vertically.
âª Horizontal bar chart: Frequencies are represented by horizontal bars
and the y-axis plots the characteristic values.
âª Vertical bar chart: Frequencies are represented by vertical bars and
the characteristic values are plotted on the x-axis.
Figure 22: Horizontal and vertical bar charts


66
8.2 Bar chart for frequencies
In a bar chart, absolute and relative frequencies are displayed on a two-axis coordinate system.
Figure 23: Bar chart for frequencies
â¢ Horizontal bar chart: Frequencies are represented by horizontal bars and the y-axis plots the characteristic values.
â¢ Vertical bar chart: Frequencies are represented by vertical bars and the x-axis plots the characteristic values.
Due to the simplicity of bar charts, they are often created in descriptive statistics. They provide a very quick overview of the ranking and frequencies of characteristic values.


67
Figure 24: Bar chart falls in the hospital
8.3 Grouped bar charts
If two categorical variables are present, grouped bar charts can be created. In a grouped bar chart, the bars are grouped together. The groups result from the categories of one of the two variables, the categories of the other variable are represented by different colors.
Figure 25: Grouped bar charts


68
In the example above, the groups are formed with the help of the variable fall sequence and the categories of the variable location are highlighted with different colors. This can of course also be reversed.
In grouped bar charts, either the frequency, the percent, or the percent in each group can be specified.
8.4 Bar chart for mean values
Of course, bar charts can be used to display not only frequencies, but also other characteristic values. In addition to frequencies, mean values are very often displayed. For this, a categorical and a metric variable must be present.
Figure 26: Bar chart for mean values
8.5 Error bar
Error bars are a graphical representation of the scatter of data. With error bars you can see how accurate your measurement is and get an overview of the range of your data!
Error bars are drawn in graphs as vertical lines above and below the measured value. The error bar is usually calculated using the standard error, standard deviation, or 95% confidence interval.


69
Figure 27: Error bar
8.6 Example bar chart
The procedure for creating a bar chart with DATAtab will now be explained in
more detail using an example. In the example it is assumed that in a statistics
course, the participants are asked which make of car they drive. The results
can be displayed clearly and easily in a bar chart:
This is how it works in DATAtab:
To create a bar chart online, go to "Charts" and simply click on the variables
you want to evaluate. The appropriate charts will then be created
automatically. If you want to use your own data, just copy it into the table on
DATAtab.


70
Figure 28: Example of a bar chart
If there is an additional categorical variable, this information can be
represented with DATAtab by additional bars with a different coloring. For
example, if gender is also known, the results can be displayed as follows. The
blue bars referring to the "male" and the orange bars describing the "female".
Figure 29: Nested bar chart


71
8.7 Histogram
A histogram is a graphical representation of the frequency distribution of a
metric variable.
To display a distribution of data in a histogram, the data must first be divided
into classes, also called bins. These classes or bins are then represented by
rectangles that lie directly next to each other.
Figure 30: Example of a histogram
This is also the main difference to bar charts; in a bar chart, the data are
already grouped from the outset and do not have to be divided into groups
first as in a histogram. This is graphically illustrated by the fact that in a bar
chart there is a space between the bars.
Accordingly, histograms are used for metric variables such as salary or age,
and bar charts for ordinal or nominal variables such as gender or school
grades.


72
8.8 Histogram example
We would like to display the frequency distribution of the results of a statistics
exam graphically. For this we use a table of test scores of 12 students. You can
find it here:
Student Score
1 28
2 29
3 35
4 37
5 32
6 26
7 37
8 39
9 22
10 29
11 33
12 34
Thatâs how it works with DATAtab:
â¢ Simply copy the above table into the statistics calculator, click on
"Descriptive Statistics" and finally select the variable "Score".
â¢ You will then get the following graph in DATAtab, which provides a
visualization of the results of the statistics exams of 12 students.


73
Figure 31: Example of a histogram
8.9 Bar chart vs. Histogram
A bar chart and a histogram are both types of graphical representations of
data, but they are used to display different types of information.
A bar chart is used to represent discrete data, where the data is divided into
separate categories. The height of each bar represents the frequency or
quantity of the data that falls into that category.
A histogram, on the other hand, is used to represent continuous data, where
the data is divided into a set of bins or intervals. The height of each bar
represents the frequency or quantity of the data that falls into that bin or
interval. The bars in a histogram are usually adjacent and there is no space
between them.
In summary, the main difference between a bar chart and a histogram is the
type of data they represent and the way the data is divided and displayed.


74
8.10 Scatter plot
Scatter plots are used in statistics to visualize correlations in data. In a
scatterplot always two variables can be plotted, this is done by representing
each pair of values of a case as a point in a coordinate system. If, for example,
10 persons are asked for their weight and height, the scatterplot shows 10
points.
Figure 32: Example of a scatter plot
With the help of the scatterplot you get a first indication of the correlation
between the two visualized variables. If high values of one variable are
associated with high values of the other variable, there is a positive
correlation. If high values of one variable are associated with low values of the


75
other variable, there is a negative correlation. If the points are randomly
distributed, there is no correlation.
Figure 33: Interrelationships in the scatter diagram
Furthermore, there can also be a nonlinear relationship; in this case, there is
a pattern in the distribution of the points, but no straight line can be drawn
through the points.
8.11 Line charts
A line chart is a graph consisting of a series of data points connected by a line.
It is used, for example, to show a continuous change of data over time.
In a line chart, time or the other continuous variable are plotted on the
horizontal axis, while the values of the data to be illustrated are plotted on
the vertical axis.
Line charts are particularly useful for visualizing trends and changes over
time, and they are often used to represent economic and financial data,
weather data, or scientific data.


76
8.12 Boxplot
What is a boxplot? With a boxplot you can graphically display a lot of
information about your data. Among other things, the median, the
interquartile range (IQR) and the outliers can be read in a boxplot.
The data used are mostly metric scaled, such as a person's age, annual
electricity consumption, or temperature.
Figure 34: Boxplot example
Often a boxplot is created to compare two or more groups. For example, the
salary of men and women.


77
Interpretation of a boxplot
The box itself indicates the range in which the middle 50% of all values lie. The
lower end of the box is therefore the 1st quartile and the upper end the 3rd
quartile.
Therefore below Q1 lie 25% of the data and above Q3 lie 25% of the data, in
the box itself lie 50% of your data.


78
Figure 35: Interpret boxplot
Let's say we look at the age of individuals in a boxplot, and Q1 is 31 years, then
it means that 25% of the participants are younger than 31 years. If Q3 is 63
years, then it means that 25% of the participants are older than 63 years, 50%
of the participants are therefore between 31 and 63 years old. Thus, between
Q1 and Q3 is the interquartile range.
In the boxplot, the solid line indicates the median and the dashed line
indicates the mean.


79
Figure 36: Interpret boxplot part 2
For example, if the median is 42, this means that half of the participants are
younger than 42 and the other half are older than 42. The median thus divides
the individuals into two equal groups.
The T-shaped whiskers go to the last point, which is still within 1.5 times the
interquartile range. What does it mean? The T-shaped whisker is either the
maximum value of your data but at most 1.5 times the interquartile range.
Any observations that are more than 1.5 interquartile range (IQR) below Q1
or more than 1.5 IQR above Q3 are considered outliers. If there are no
outliers, the whisker is the maximum value.
So the upper whisker is either the maximum value or 1.5 times the
interquartile range. Depending on which value is smaller. The same is true for
the lower whisker, which is either the minimum or 1.5 times the interquartile
range.


80
Figure 37: Interpret boxplot part 3
Points that are further away are considered outliers. If no point is further away
than 1.5 times the interquartile range, the T-shaped whisker indicates the
maximum or minimum value.
Thatâs how it works with DATAtab:
â¢ With DATAtab you can easily create a boxplot online.
â¢ To do this, click on the statistics calculator,
â¢ copy your own data into the table,
â¢ select the tab "Descriptive" or "Charts" and
â¢ click on the variables for which you want to create a boxplot.


81
Figure 38: Interpret boxplot part 4
In the upper boxplot created with DATAtab online, the location of falls in a
hospital was contrasted with the age of the persons who fell.
8.13 Bland-Altman plot
Bland-Altman plots, also known as difference plots, are a powerful graphical
tool for comparing two measurement techniques and assessing the
agreement between two sets of data.
The plot provides a visual representation of the difference between two
measurements on the y-axis and the average of the two measurements on
the x-axis.


82
Figure 39: Bland-Altman Plot
In essence, a Bland-Altman plot is a scatter plot where the differences
between two measurements are plotted against their averages. This helps to
visualize the degree of agreement between the two raters and identify any
systematic bias.
Example of Bland-Altman plot
Bland-Altman plots are widely used in medical research, industrial quality
control and other fields where comparing two measurement methods is
required.
In the medical field, for example, it is often necessary to compare the results
of a new measurement technique with a gold standard.
The Bland-Altman plot is a powerful tool for this purpose, as it allows for the
visualization of the agreement between the two methods and any systematic
bias or random error.


83
Example: An example of a Bland-Altman plot is to compare the measurement
of blood sugar using two different measuring systems. In this case the x-axis
is the mean of the two measurements and the y-axis is the difference between
the two measuring systems. The plot would show the agreement or
disagreement between the two measurement techniques.
Structure of a Bland-Altman plot
First, let's take a look at the basic structure of a Bland-Altman plot. The plot
consists of a scatter plot of the differences between the two measurements
against the averages of the two measurements.
A horizontal line is also included on the plot, representing the mean difference
between the two measurements.
The plot also typically includes lines that represent the standard deviation,
typically Â±1.96 standard deviations of the differences, from the mean
difference, which is used to identify any outliers in the data.
How can a Bland-Altman plot be used?
The Bland-Altman plot can be used to Evaluate agreement, Identify any
systematic bias and Find outliers in the data.
â¢ Evaluate agreement
One of the key advantages of Bland-Altman plots is that they can be used to
evaluate the agreement between two measurement techniques.
â¢ Identify any systematic bias
The plot can be used to identify any systematic bias or random error in the
data. For example, if the mean difference between the two measurements is
consistently positive or negative, this may indicate a systematic bias in one of
the measurement techniques. Additionally, if the scatter of the points on the
plot is greater than the standard deviation, this may indicate the presence of
random error in the data.


84
â¢ Find outliers in the data
Another important aspect of Bland-Altman plots is that they can be used to
identify outliers in the data. Outliers can have a significant impact on the
results of a study, and it is important to identify them in order to understand
the overall agreement between the two measurement techniques. Outliers
can be identified by looking for points that fall outside of the lines
representing the standard deviation from the mean difference.
Thatâs how it works with DATAtab:
You can easily create a Bland Altman plot online with DATAtab.
â¢ To do this, simply copy your data into the table in the statistics
calculator.
â¢ Click on the tab Charts (create charts online) or Reliability (Reliability
analysis calculator)
â¢ Then select the desired variables for which you want to create the
Bland-Altman plot online


85
8.14 Create charts online with DATAtab
Charts are a valuable tool for visually presenting information. With Datatab
you can create your own charts online and free of charge.
To create a chart, simply select the type of chart you want to create and copy
your data into the data table. The graphic below illustrates the chart creation
procedure:
Figure 40: Create charts with DATAtab


86
To create a chart with DATAtab, simply copy your data into the table on the
statistics calculator and select the variables for which you want to create a
chart.
Using DATAtab you can create various charts, such as:
â¢ Barchart
â¢ Boxplot
â¢ Histogram
â¢ Scatter Plot
â¢ Creating Violin plot
â¢ Create Raincloud Plot
â¢ Create Bland-Altman plot
â¢ Create Sankey diagram
â¢ and many more...
Which diagram you should create depends on the information you want to
convey and the scale level of your data.
In statistics it is advisable to first examine the data by means of the created
diagrams, this already gives an indication whether there are differences in the
individual groups, for example. Subsequently, the visual results can be verified
with hypothesis tests.


87
9. Inferential Statistics
In contrast to descriptive statistics inferential statistics want to make a
statement about the population. In statistics, different types of hypotheses
are distinguished and there are rules that must be observed when formulating
hypotheses. These topics will now be discussed in more detail in the following
chapter.
Figure 41: Aim of inferential statistics
9.1 Hypotheses
A hypothesis is an assumption that is neither confirmed nor disproved. In the
research process, a hypothesis is made at the very beginning and the goal is
to either reject or maintain the hypothesis. n order to reject or or not reject a
hypothesis, data, e.g. from an experiment or a survey, are needed, which are
then evaluated using a hypothesis test.
Usually, hypotheses are formulated starting from a literature review. Based
on the literature review, you can then justify why you formulated the
hypothesis in this way.
An example of a hypothesis is: "Men earn more than women in the same job
in Germany. "


88
Figure 42: Properties of hypotheses
To test this hypothesis, you need data, e.g., survey data, and a suitable
hypothesis test such as the t-test or correlation analysis. Don't worry,
DATAtab will help you choose the right hypothesis test.
Formulating a hypothesis
In order to formulate a hypothesis, a research question must first be defined.
A precisely formulated hypothesis about the population can then be derived
from the research question, e.g., men earn more than women in the same job
in Germany. Based on the hypothesis, a suitable hypothesis test is chosen to
test the assumption.


89
Figure 43: Hypotheses in the research process
Hypotheses are not simple statements; they are formulated in such a way that
they can be tested with collected data in the course of the research process.
To test a hypothesis, it is necessary to define exactly which variables are
involved and how the variables are related. Hypotheses, then, are
assumptions about the cause-and-effect relationships or the associations
between variables.
What is a variable?
A variable is a property of an object or event that can take different values.
For example, the eye color is a variable, it is the property of the object eye
and can take different values (e.g. blue or brown).
If you're doing research in the social sciences, your variables may be:
âª Gender âª Income âª Attitude towards environmental protection


90
If you are researching in the medical field, your variables may be:
âª Body weight âª Smoker status âª Heart rate
9.2 Null and alternative hypothesis
There are always two hypotheses that exactly oppose each other or assert the
opposite. These opposing hypotheses are called null and alternative
hypothesis and are abbreviated as H0 and H1.
The definition of the null hypothesis is: "The null hypothesis assumes that
there is no difference between two or more groups with respect to a
characteristic. "
â¢ Example:
The salary of men and women does not differ in Germany.
In contrast, the alternative hypothesis can be described as follows:
"Alternative hypotheses assume that there is a difference between two or
more groups. "
â¢ Example:
The salary of men and women differs in Germany.
The hypothesis that you want to test or that you have derived from the theory
usually states that there is an effect e.g. gender has an effect on salary. This
hypothesis is called an alternative hypothesis.
The null hypothesis usually states that there is no effect e.g. gender has no
effect on salary. In a hypothesis test, only the null hypothesis can be tested;
the goal is to find out whether the null hypothesis is rejected or not.


91
9.3 Difference and correlation hypotheses
What types of hypotheses are there? The most common distinction is
between difference and correlation hypotheses as well as directional and
non-directional hypotheses.
Difference hypotheses are used when different groups are to be
distinguished, e.g., the group of men and the group of women. Correlation
hypotheses are used when the relationship or correlation between variables
is to be tested, e.g., the relationship between age and height.
Difference hypotheses
Difference hypotheses test whether there is a difference between two or
more groups.
Examples of difference hypotheses:
âª The "group" of men earns more than the "group" of women.
âª Smokers have a higher risk of heart attack than nonsmokers.
âª There is a difference between Germany, Austria, and France in terms of
hours worked per week.
Thus, one variable is always a categorical variable, e.g., gender (male,
female), smoking status (smoker, nonsmoker), or country (Germany, Austria,
and France); the other variable is at least ordinally scaled, e.g., salary, percent
risk of heart attack, or hours worked per week.
Figure 44: Difference hypotheses


92
Correlation hypotheses
Correlation hypotheses test correlations between at least two variables.
Here you can find some examples:
âª The taller a person is, the heavier he or she is.
âª The more horsepower a car has, the higher its fuel consumption.
âª The better the math grade, the higher the future salary.
As can be seen from the examples, correlation hypotheses often take the form
"The more..., the higher/lower.... ". Thus, at least two ordinally scaled
variables are being examined.
Figure 45: Correlation hypotheses


93
9.4 Directional and undirectional hypotheses
Hypotheses are divided into directional and undirectional (non-directional)
or one-sided and two-sided hypotheses. If words such as "better than" or
"worse than" occur in the hypothesis, the hypothesis is usually directed.
In the case of an undirectional hypothesis, one often finds building blocks such
as "there is a difference between" in the formulation, but it is not stated in
which direction the difference lies.
â¢ With an undirectional hypothesis, the only thing of interest is whether
there is a difference in a value between the groups under consideration.
â¢ In a directional hypothesis, what is of interest is whether one group has
a higher or lower value than the other.
Figure 47: One-sided and two-sided testing
Figure 46: Directional and undirectional hypotheses


94
Undirectional hypotheses
Undirectional hypotheses test whether there is a relationship or a difference,
and it does not matter in which direction the correlation or difference goes.
In the case of a difference hypothesis, this means there is a difference
between two groups, but it does not say whether one of the groups has a
higher value.
âª There is a difference between the salary of men and women (but it is
not said who earns more!).
âª The risk of heart attack differs between smokers and non-smokers (but
it is not said who has the higher risk!).
In terms of a correlation hypothesis, this means there is a relationship or
correlation between two variables, but it does not say whether this
relationship is positive or negative.
âª There is a correlation between height and weight.
âª There is a correlation between horsepower and fuel consumption in
cars.
In both cases it is not said whether this correlation is positive or negative!


95
Directional hypotheses
Directional hypotheses additionally indicate the direction of the correlation
or the difference. In the case of the difference hypothesis a statement is
made as to which group has a higher or lower value.
âª Men earn more than women.
âª Smokers have a higher risk of heart attack than nonsmokers.
A correlation hypothesis indicates whether the correlation is positive or
negative.
âª The taller a person is, the heavier he is.
âª The more horsepower a car has, the higher its fuel consumption.
A one-sided or directional alternative hypothesis includes only values that
differ in one direction from the value of the null hypothesis.
The p-value for directional hypotheses
Usually, statistical software always calculates the non-directional test and then also outputs the p-value for this.
To obtain the p-value for the directional hypothesis, it must first be checked whether the effect is in the right direction. Then the p-value must be divided by two. This is because the significance level is not split on two sides, but only on one side. More about this in the tutorial about the p-value.
If you select "one-tailed" in DATAtab for the calculated hypothesis test, the conversion is done automatically and you only need to read the result.


96
9.5 Hypothesis Testing
Now that the basic rules of hypothesis formulation and the different types of
hypotheses have been discussed, the next section deals with the goals of
hypotheses, the basics of hypothesis testing, and the logic of statistical
inference.
The starting position of hypothesis testing is that in your thesis you have
formulated several hypotheses that you want to test in order to make a
statement about the population.
A hypothesis test is used whenever you want to test a hypothesis about the
population with the help of a sample. So, whenever you want to prove or say
something about the population with a sample, hypothesis tests are used.
A possible example is that the company My-Muesli would like to know
whether their produced muesli bars really weigh 250g. For this purpose, a
random sample is taken, and a hypothesis test is then used to draw
conclusions about all the muesli bars produced.
Figure 48: Logic of statistical inference


97
Figure 49: Hypothesis testing and research process
9.5.1 Hypothesis testing and the null hypothesis
As we know from the previous tutorial on hypotheses, there is always a null
and an alternative hypothesis. In "classical" inferential statistics, the null
hypothesis is always tested using a hypothesis test. The hypothesis is tested
to see if there is no difference or no relationship.
If you want to be 100% accurate (what is not always the case in practice), the
null hypothesis H0 can only ever be ârejectedâ or ânot rejectedâ using a
hypothesis test. The non-rejection of H0 is not a sufficient reason to conclude
that H0 is true. Thus, it must always be "H0 was not rejected" and not "H0 was
retained."


98
9.5.2 The uncertainty in hypothesis testing
Whether an assumption or hypothesis about the population is accepted or
rejected by hypothesis testing can only ever be determined with a certain
probability of error. But why does the probability of error exist at all?
Here is the short answer: Each time you take a sample, of course, you get a
different sample, which means that the results are different every time. In the
worst case, a sample is taken that happens to deviate very strongly from the
population and the wrong statement is made. Therefore there is always a
probability of error for every statement or hypothesis.
9.5.3 Level of significance or probability of error
A hypothesis test can never reject the null hypothesis with absolute certainty.
There is always a certain probability of error that the null hypothesis is
rejected even though it is actually true. This probability of error is called the
significance level or Î±.
The significance level is used to decide whether the null hypothesis should be
rejected or not. If the p-value is smaller than the significance level, the null
hypothesis is to be rejected; otherwise, it is not to be rejected.
Figure 50: Uncertainty in hypothesis testing


99
Usually, a significance level of 5% or 1% is set. If a significance level of 5% is
set, it means that it is 5% likely to reject the null hypothesis even though it is
actually true.
Illustrated by the two-sample t-test, this means: The observed means of two
samples have a certain distance to each other. The greater the observed
distance between the mean values, the less likely it is that both samples come
from the same population. The question now is, at what point is it "unlikely
enough" to reject the null hypothesis? If a significance level of 5% is set, at 5%
it is "unlikely enough" to reject the null hypothesis.
The probability that two samples are drawn from a population and that they
have the observed mean difference, or even a greater one, is indicated by the
p-value. Accordingly, if the p-value is less than the significance level, the null
hypothesis is rejected; if the p-value is greater than the significance level, the
null hypothesis is not rejected.
If, for example, a p-value of 0.04 results, the probability that two groups with
an observed mean distance or an even greater distance come from the same
population is 4%. The p-value is thus smaller than the significance level of 5%
and thus the null hypothesis is rejected.
It is important that the significance level is always determined prior to the
investigation and may not be changed subsequently in order to obtain the
"desired" statement after all. To ensure a certain degree of comparability, the
significance level is usually 5% or 1%.
âª Î± â¤ 1% highly significant (h.s.)
âª Î± â¤ 5% significant (s.)
âª Î± > 5% not significant (n.s.)


100
9.5.4 Example significance level and p-value
The significance of significance level and p-value will now be illustrated by an
example.
H0: Men and women in Austria do not differ in their average monthly net
income.
To test this hypothesis, a significance level of 5% is set and a survey is
conducted asking 600 women and 600 men about their monthly net income.
An independent t-test gives a p-value of 0.04
The p-value 0.04 is less than the significance level of 0.05, thus we rejecting
the null hypothesis. Based on the data collected, we have sufficient evidence
that there is a statistically significant difference in average monthly next
income for the population of men and women in Austria.
9.5.5 Types of errors
Because a hypothesis can only be rejected with a certain probability, different
types of errors occur. Due to the sample selection, it can happen that the null
hypothesis is rejected by chance, although in reality there is no difference, i.e.
the null hypothesis is valid. Conversely, the result of the hypothesis test can
also be that the null hypothesis is not rejected, although in reality there is a
difference and thus the alternative hypothesis is actually true.
Accordingly, two types of errors arise in hypothesis testing:
â¢ Type 1 error: If the alternative hypothesis is accepted although the
null hypothesis is valid.
â¢ Type 2 error: If the null hypothesis is retained although the
alternative hypothesis applies.