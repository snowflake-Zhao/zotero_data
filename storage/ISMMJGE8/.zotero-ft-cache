THE ELSEVIER AND MICCAI SOCIETY BOOK SERIES
Advisory Board
Nicholas Ayache James S. Duncan Alex Frangi Hayit Greenspan Pierre Jannin Anne Martel Xavier Pennec Terry Peters Daniel Rueckert Milan Sonka Jay Tian S. Kevin Zhou
Titles
Balocco, A., et al., Computing and Visualization for Intravascular Imaging and Computer Assisted Stenting, 9780128110188.
Dalca, A.V., et al., Imaging Genetics, 9780128139684.
Depeursinge, A., et al., Biomedical Texture Analysis, 9780128121337.
Munsell, B., et al., Connectomics, 9780128138380.
Pennec, X., et al., Riemannian Geometric Statistics in Medical Image Analysis, 9780128147252.
Trucco, E., et al., Computational Retinal Image Analysis, 9780081028162.
Wu, G., and Sabuncu, M., Machine Learning and Medical Imaging, 9780128040768.
Zhou S.K., Medical Image Recognition, Segmentation and Parsing, 9780128025819.
Zhou, S.K., et al., Deep Learning for Medical Image Analysis, 9780128104088.
Zhou, S.K., et al., Handbook of Medical Image Computing and Computer Assisted Intervention, 9780128161760.


Radiomics and Its Clinical Application
Artificial Intelligence and Medical Big Data
Jie Tian
CAS Key Laboratory of Molecular Imaging, Chinese Academy of Sciences; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, School of Engineering Medicine, Beihang University, Beijing, China
Di Dong
CAS Key Laboratory of Molecular Imaging, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China
Zhenyu Liu
CAS Key Laboratory of Molecular Imaging, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China
Jingwei Wei
CAS Key Laboratory of Molecular Imaging, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China


Academic Press is an imprint of Elsevier 125 London Wall, London EC2Y 5AS, United Kingdom 525 B Street, Suite 1650, San Diego, CA 92101, United States 50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States The Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, United Kingdom
Copyright © 2021 Elsevier Inc. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or any information storage and retrieval system, without permission in writing from the publisher. Details on how to seek permission, further information about the Publisher’s permissions policies and our arrangements with organizations such as the Copyright Clearance Center and the Copyright Licensing Agency, can be found at our website: www.elsevier.com/permissions.
This book and the individual contributions contained in it are protected under copyright by the Publisher (other than as may be noted herein).
Notices
Knowledge and best practice in this field are constantly changing. As new research and experience broaden our understanding, changes in research methods, professional practices, or medical treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating and using any information, methods, compounds, or experiments described herein. In using such information or methods they should be mindful of their own safety and the safety of others, including parties for whom they have a professional responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability for any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or from any use or operation of any methods, products, instructions, or ideas contained in the material herein.
Library of Congress Cataloging-in-Publication Data
A catalog record for this book is available from the Library of Congress
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library
ISBN: 978-0-12-818101-0
For information on all Academic Press publications visit our website at https://www.elsevier.com/books-and-journals
Publisher: Mara Conner Acquisitions Editor: Tim Pitts Editorial Project Manager: Chiara Giglio Production Project Manager: Prasanna Kalyanaraman Cover Designer: Victoria Pearson
Typeset by TNQ Technologies


Preface
With the expeditious growth of medical imaging data and the rapid advancement of artificial intelligence techniques, image-derived diagnosis and prognosis of multifold diseases has broken through the scope of conventional computer-aided diagnosis. Toward the era of intelligent analysis, a new product that combines big data of medical imaging and artificial intelligence, radiomics, has emerged. In 2012, Professor Philippe Lambin and Professor Robert Gillies first proposed the concept of radiomics, which converts medical images such as computer tomography, magnetic resonance imaging, positron emission tomography, and ultrasound into excavable data, mines massive quantitative imaging characteristics related to diseases, and builds intelligent analysis models by artificial intelligence techniques to assist clinical diagnosis and prognosis. Radiomics originates from clinical issues and eventually returns to clinical guidance applications. It is currently one of the most important research hotspots with cutting-edge directions and has definitely shown great clinical application prospects. Up to now, many mainstream international imaging conferences, such as those of the Radiological Society of North America, the International Society for Magnetic Resonance in Medicine, and the World Molecular Imaging Congress, and clinical oncology conferences (such as those of the American Association for Cancer Research, American Society of Clinical Oncology), have set up special sessions for radiomics. There is also a trend of rapid growth in international research papers related to radiomics year by year. We have been following the research hotspot of radiomics for many years and have participated in the international radiomics seminars hosted by Professor Robert Gillies for six consecutive years. While witnessing the rapid development of radiomics and the endless novel methods and clinical applications, we are deeply concerned about the lacunae of books dedicated to radiomics in China. In view of this, we have systematically sorted out the radiomics technique procedures and typical clinical applications and compiled this book, hoping to attract more domestic clinical and scientific researchers to jointly launch radiomics researches and provide a potential technical tool for promoting the precise diagnosis and treatment of cancers and other diseases. The publication of this book has received much help and support. We appreciate the National Science and Technology Academic Publication Fund (2017-H-017), the National Key Research and Development Program of China (2017YFA0205200), the National Natural Science Foundation of China (81930053), and the Science Press for their long-term strong support. This book is based on radiomics research studies accumulated over years by the Key Laboratory of Molecular Imaging of the Chinese Academy of Sciences and the preliminary work of many doctoral students, master’s students, postdoctoral fellows, and young teachers. We are especially grateful to three authoritative experts in the field of radiomics, Robert Gillies, Philippe Lambin, and Sandy Napel, for writing prefaces to this book and supporting the research of radiomics in China. We thank Di Dong, Zhenyu Liu, Jingwei Wei,
xi


Zhenchao Tang, Shuo Wang, Hailin Li, Siwen Wang, Lianzhen Zhong, Mengjie Fang, Lixin Gong, Runnan Cao, Caixia Sun, Kai Sun, Dongsheng Gu, and Shuaitong Zhang for participating in the writing and organization of this book. They contributed a lot to the final completion of this book.
Jie Tian October 2020
xii Preface


Biographies
Dr. Jie Tian received his PhD (with honors) in Artificial Intelligence from the Chinese Academy of Sciences in 1993. Since 1997, he has been a Professor at the Chinese Academy of Sciences. Dr. Tian has been elected as a Fellow of ISMRM, AIMBE, IAMBE, IEEE, OSA, SPIE, and IAPR. He serves as an editorial board member of Molecular Imaging and Biology, European Radiology, IEEE Transactions on Medical Imaging, IEEE Transactions on Biomedical Engineering, IEEE Journal of Biomedical and Health Informatics, and Photoacoustics. He is the author of over 400 peer-reviewed journal articles, including publication in Nature Biomedical Engineering, Science Advances, Journal of Clinical Oncology, Nature Communications, Radiology, IEEE Transactions on Medical Imaging, and many other journals, and these articles have received over 25,000 Google Scholar citations (H-index 79). Dr. Tian is recognized as a pioneer and leader in the field of molecular imaging in China. In the last two decades, he has developed a series of new optical imaging models and reconstruction algorithms for in vivo optical tomographic imaging, including bioluminescence tomography, fluorescence molecular tomography, and Cerenkov luminescence tomography. He has developed new artificial intelligence strategies for medical imaging big data analysis in the field of radiomics and played a major role in establishing a standardized radiomics database with more than 100,000 cancer patients data collected from over 50 hospitals all over China. He has received numerous awards, including 5 national top awards for his outstanding work in medical imaging and biometrics recognition. Dr. Di Dong is currently an Associate Professor at the Institute of Automation, Chinese Academy of Sciences. He received his PhD in Pattern Recognition and Intelligent Systems from the Institute of Automation, Chinese Academy of Sciences, China, in 2013. Dr. Dong is a member of the Youth Innovation Promotion Association of the Chinese Academy of Sciences, an active member of the American Association for Cancer Research (AACR), and a corresponding member of the European Society of Radiology (ESR). Dr. Dong has carried out long-term research work in the field of tumor radiomics and medical big data analysis. In recent years, Dr. Dong has published nearly 50 peer-reviewed papers in SCI journals, e.g., in Annals of Oncology, European Respiratory Journal, Clinical Cancer Research (three publications), BMC Medicine, etc. These articles have received over 1,600 Google Scholar citations (H-index 24). He has 6 ESI highly cited papers. He has applied for more than 20 patents and 10 software copyright licences in China. Dr. Zhenyu Liu is currently a Professor at CAS Key Laboratory of Molecular Imaging, Institute of Automation. He received his PhD in Pattern Recognition and Intelligent Systems from the Institute of Automation, Chinese Academy of Sciences, China, in 2014. Dr. Liu got the outstanding youth fund of the Natural Science Foundation of China (NSFC) and is a member of the Youth Innovation Promotion Association of the Chinese Academy of Sciences. His research focuses on medical imaging analysis, especially radiomics and its applications in oncology research.
ix


In recent years, Dr. Liu has published nearly 30 papers in peer-reviewed journals, e.g., in Clinical Cancer Research, Theranostics, EBioMedicine, Radiotherapy and Oncology, etc. These articles received over 1,300 Google Scholar citations. He also holds more than 10 patents in China. Dr. Jingwei Wei is currently an Assistant Professor at the Institute of Automation, Chinese Academy of Sciences. Her research focuses on radiomics and its clinical application in liver diseases, liver-specific feature engineering, traditional pattern recognition classifiers, and deep learning methods implemented towards liver disease-oriented research. Her primary work includes pre-operative prediction of microvascular invasion in hepatocellular carcinoma (HCC), prognosis prediction in HCC, and non-invasive imaging biomarker development for pathological factors prediction in liver diseases. Dr. Wei has published over 20 peer-reviewed papers in SCI journals, e.g., in Liver Cancer, Liver International, Clinical and Translational Gastroenterology, etc.
x Biographies


Introduction 1
Chapter outline
1.1 Background of medical image analysis in cancer................................................... 2 1.2 Multidimensional complexity of biomedical research ............................................. 4 1.3 Concept of radiomics............................................................................................ 6 1.4 Value of radiomics ............................................................................................... 7 1.5 Workflow of radiomics.......................................................................................... 7 1.5.1 Image acquisition and reconstruction ................................................. 8 1.5.2 Image segmentation ......................................................................... 8 1.5.3 Feature extraction and selection ........................................................ 8 1.5.4 Database and data sharing ................................................................ 9 1.5.5 Informatics analysis.......................................................................... 9 1.5.6 Medical image acquisition................................................................. 9 1.5.7 Segmentation of the tumor.............................................................. 11 1.5.8 Tumor image phenotype.................................................................. 12 1.5.9 Clinical prediction for tumor............................................................ 13 1.5.10 New technology of artificial intelligence ........................................... 15 1.6 Prospect of clinical application of radiomics ....................................................... 16 References ............................................................................................................... 16
Medical imaging began in 1895 when German physicist Wilhelm Konrad Rontgen discovered X-rays. In 1978, G.N. Hounsfield published computed tomography (CT) technology, which is considered to be one of the great achievements of science and technology in the 20th century. Since then, medical imaging has developed rapidly, and various new medical imaging devices and technologies have been continuously presented. Especially medical imaging has played a vital role in cancer screening, diagnosis, and treatment for patients. On the other hand, the rise of modern artificial intelligence in recent years has led to breakthroughs in computer vision and patter recognition, and the growth of massive medical big data provides an excellent opportunity for artificial intelligence applications in medical imaging analysis. Especially, the multiple molecular events represented the intrinsic progress in the micro scale, which can be possibly uncovered by artificial intelligence
CHAPTER
1
Radiomics and its Clinical Application. https://doi.org/10.1016/B978-0-12-818101-0.00004-5 Copyright © 2021 Elsevier Inc. All rights reserved.


analysis on medical imaging. In this context, radiomics emerges as a new research area that integrates artificial intelligence and machine learning to extract pathophysiological information of tumor images, thus enabling tumor staging classification, therapeutic evaluation, and prognosis assessment. The content of this chapter is focused on the background of medical imaging, multidimensional complexity of biomedical research, concept of radiomics, value of radiomics, workflow of radiomics, and the clinical applications of radiomics.
1.1 Background of medical image analysis in cancer
According to the 2014 World Cancer Report published by the International Agency for Research on Cancer of the World Health Organization on February 3, 2014, there were 14.1 million newly diagnosed cancer cases and 8.2 million cancer-related deaths worldwide in 2012. The incidence of young patients is increasing year by year [1]. Wanqing Chen and Jie He of the National Cancer Center of China estimated there were 4.292 million newly diagnosed cancer cases and 28.14 million cancerrelated deaths in China in 2015 [2]. Cancer has become a major disease that seriously affects the quality of human life and threatens human life. Early diagnosis of cancer and accurate prognosis assessment via imaging plays an important role in providing personalized treatment plans. CT, magnetic resonance imaging (MRI), positron emission tomography (PET), and ultrasound are widely used in clinical practice. Thus, the results of relevant examinations provide assistance and reference for cancer staging and assessment of cancer patients’ prognosis. Medical imaging is an important tool for evaluation of the tumors and the curative effects. The diagnostic value of CT for cancer is that it can observe the changes of morphology and density of cancerous tissues and the extent of tumor invasion from CT images, thus achieving staging of the tumor. But it is difficult to diagnose early tumors with inconspicuous morphology. MRI has a high advantage in multidirectional imaging and soft tissue contrast ability, which can clearly show the size and location of the tumor and the degree of invasion of surrounding tissues. MRI has good clinical value for the diagnosis and staging of cancer, and can also provide information on the spread of tumors to help develop surgical plans. As a noninvasive method for tumor diagnosis, medical imaging has been widely used in the auxiliary diagnosis of various cancers: Firstly, the use of image information for clinical diagnosis often relies on the subjective experience of doctors, and imaging features are connected to corresponding diagnosis. However, medical imaging contained valuable information to reveal both intra- and intertumor heterogeneity. For example, based on standard medical images (e.g., CT, MRI, and PET), clinicians can only obtain concise diagnostic information related to tumor shape, size, image contrast, and tumor metabolism. However, these information may not fully reflect the pathophysiology or diagnosis and treatment of the entire tumor, and thus it is impossible to provide an effective means to quantify
2 CHAPTER 1 Introduction


the imaging finding soft tumors’ pathological staging or tumor changes after treatment [3]. Medical images are not just images, in addition to providing visual information, they also contain a large amount of potential information related to tumor pathophysiology and tissue cell microenvironment [4]. Such information has not been effectively utilized for a long period of time in clinics. The deep exploration of medical imaging data will provide more information, including tumor morphology, potential pathological mechanism, and tumor heterogeneity toward the precise diagnosis and personalized treatment of patients. Secondly, medical imaging has evolved from single X-ray imaging to a multimodal medical imaging technique. Currently, medical imaging is routinely used for clinical evaluation of tumors, diagnosis of tumor staging, and evaluation of the therapeutic effect [5,6]. However, most of the applications of these medical images only focus on the evaluation of tumor anatomical structure (e.g., tumor size) in the standardized clinical diagnosis. This measurement limits the application of medical imaging in the study of extensive tumor heterogeneity. The advantage of medical imaging is that the appearance phenotype of the tumor can be obtained in a noninvasive way, such as macroscopic intratumoral heterogeneity. Alternatively, tumor-related information can be obtained by invasive biopsy by extracting selected cancer tissue, therefore the heterogeneity of the tumor tissue cannot fully reflect the internal pathological information of the tumor. Furthermore, repeated invasive biopsies are a heavy burden for high-risk patients. Conversely, the imaging phenotype of the tumors provided by medical imaging provides a wealth of information on tumor genotypes, tumor microenvironments, and potential therapeutic effects [4]. At the same time, the information provided by the images can complement the genetic information. Therefore, the role of the tumor image phenotype based on medical imaging in precision medicine opens new possibilities for clinical research. Moreover, the clinician can quantitatively evaluate the patient’s tumor phenotype at each follow-up. Therefore, analyzing tumor heterogeneity by quantitative imaging has a great potential for precision oncology applications. Thirdly, in current clinical practice, physicians from the radiology department use quantitative indicators for tumor evaluation. In axial CT imaging, tumor size can be described by one- or two-dimensional methods. In molecular imaging, there is less quantitative information extracted and used for clinical application. In PET imaging, only the maximum or average intake is used to quantify the metabolism. Although these indicators are highly meaningful biomarkers, a large number of potential imaging characteristics were yet to be elucidated from quantitative tumor imaging. In addition, semantic features can also be obtained from medical imaging. The semantic features refer to the tumor characteristics obtained by visual evaluation of medical images via radiologists. By definition, semantic features are qualitative judgment of the tumor in clinical practice. The extraction of semantic features highly depends on professional medical imaging knowledge and is subject to subjective influence by different human evaluators. The benefit of establishing a unified terminology standard is that the terminology defined by
1.1 Background of medical image analysis in cancer 3


experienced radiologists can establish a uniform measure of tumor characterization. Moreover, radiologists can evaluate low-quality or low-resolution medical images and give diagnostic results.
1.2 Multidimensional complexity of biomedical research
Multidimensional complexity is an important problem in biomedical research, which is mainly divided into spatial complexity and time complexity. In terms of spatial complexity, human cells contain about 20,000 protein-coding genes, about 360,000 mRNAs, and about 1,000,000 protein molecules [x]. Their biological functions and activities are difficult to measure and estimate. The human body contains about 30 trillion cells, 79 key organs, 13 major organ systems, and their phenotype and function are still difficult to estimate. The composition of the human body has a strong spatial complexity from micro to macro levels. The existing data processing methods are difficult to effectively analyze such a large amount of biomedical data. In terms of time complexity, the interaction of each person’s behavior from birth to death to the external environment is incalculable. The tissues, organs, proteins, RNA, and DNA of an organism change over time, and are also affected by the external environment and various behaviors. Similarly, existing data processing methods are difficult to make an effective analysis for such a large amount of biomedical data. For the above reasons, analyzing the multidimensional and complex biomedical data needs to overcome many challenges, and biomedical research usually needs to choose a simplified and controllable research direction. For example, medical big data come from various sources including gene sequencing, protein sequencing, electron microscopy, optical microscopy, etc. At the molecular cell level, medical data are generated by clinical examination, medical imaging, surgery, autopsy, etc. At the tissue and organ level, medical big data are generated by environmental monitoring, smart cities, smart homes, wearable devices, etc. At the behavioral level, we can solve specific problems in a local dimension. However, the local dimension study based on medical imaging big data cuts in from the macroscopic tissue organ dimension, and can reflect the microscopic dimension information through image features. A patient’s CT image can contain 52,428,800 voxels, which contain between 1000 and 100,000 image features, while 1000 patients contain between 1 million and 100 million image feature big data. Such a large amount of data lay the foundation for artificial intelligence in medicine. In the spatial dimension, the relationship between medical image features and pathological gene analysis results is constructed from macro to micro. In the time dimension, the association between medical imaging features and treatment follow-up results was constructed from treatment to prognosis. This joint analysis can help achieve assisted precision diagnosis and treatment planning for patients. Molecular events represent the intrinsic
4 CHAPTER 1 Introduction


molecular activities in the micro scope. The massive collection of molecular events in the micro scale is reflected in the macro scale as the medical image. The pathological changes are usually related to multiple abnormal molecular event. The massive amount of multiple abnormal molecular events would display as abnormal regions in the medical images. Though medical images are macro representation of the massive amount of multiple abnormal molecular events, it is still difficult to associate the macro images characteristics with the micro abnormal molecular events. Nevertheless, with the help of artificial intelligence, the potential information in the macro images can be mined in the approach of quantitative analysis. Then, the multiple molecular events can be analyzed by the quantitative information explored by artificial intelligence. In the earlier research, Michael Kuo et al. analyze the diversity of genetic and protein activities with the diverse radiographic features extracted from CT images [7]. They found that the combination of 28 radiographic features can reconstruct 78% of the global gene expression profiles. The radiographic features are also related with cell proliferation, liver synthetic function, and patient prognosis. The finding of Michael Kuo and his colleges indicate the substantial relation between the macro quantitative information and the multiple molecular events. Furthermore, Sun et al. extracted quantitative features from contrast-enhanced CT images to investigate the relationship with the CD8 cell tumour infiltration [8]. It was found that the quantitative features combined with RNA-seq genomic data can be used to assess the tumourinfiltrating CD8 cells and predict the response to anti-PD-1 or anti-PD-L1 immunotherapy. The macro image biomarker can directly reflect the micro changes of CD8 cell tumour infiltration, which is an important marker for tumor response. Sun’s work shows that the relationship between the macro images and the micro multiple molecular events cannot only be explored but also can be used to assess the clinical manifestation of the patient, such as the therapy response. Their findings are also validated in three independent cohorts, which indicating that the image biomarker is promising in predicting the immune phenotype of tumours. Similarly, Haruka et al. extracted quantitative image features capturing the shape, texture, and edge sharpness information of the glioblastoma [9]. They found three distinct phenotypic “clusters” are related to unique set of molecular signaling pathways, which were directly related to differential probabilities of survival. So, the macro image phenotypes are related to different prognosis by associating with the micro multiple molecular events. Thus, the distinct phenotypes of the images can provide as a noninvasive approach to stratify GBM patients for different targeted therapy and personalized treatment. In a recent study, Mu et al. analyze the relationship between the quantitative F-18-FDG-PET/CT image features and the EGFR mutation status, which is related to the longer progression free survival in patients treated with EGFR-TKIs [10]. They construct an EGFR deep learning score significantly and negatively associated with higher durable clinical benefit, reduced hyper progression, and longer PFS.
1.2 Multidimensional complexity of biomedical research 5


1.3 Concept of radiomics
In recent years, due to the advancement of storage and information technology, the medical image information of patients has been well preserved digitally. Compared with previous simple image processing based on small samples, the ever-growing number of medical images brings new research opportunities: (1) Based on a large amount of imaging data, a more accurate statistical model can be established to improve the level of diagnosis and detection of computer-aided diagnosis systems, so that its accuracy is comparable to human-level diagnosis; (2) More complex and expressive machine learning, pattern recognition, and statistical methods can play a better role with the big data, thus mining more potential laws and information from massive imaging data. The accumulation of medical image big data and the rapid development of artificial intelligence technology directly promote the new comprehensive analysis method in medicine. Radiomics generally refers to the use of CT, PET, MRI, or ultrasound imaging as input data, extracting expressive features from massive image-based data, and then using machine learning or statistical models for quantitative analysis and prediction of diseases [4,11e13]. Compared with the traditional practice of using only manual viewing, radiomics analysis can extract high-dimensional features that are difficult to quantitatively describe in human visuals from massive data and correlate them with clinical and pathological information of patients to achieve the prediction of certain diseases or genes. Using advanced bioinformatics tools and machine learning methods, researchers are able to develop potential models that improve the prediction accuracy of diagnostic and prognostic approaches [4]. Radiomics is an emerging medical image analysis proposed by Lambin et al. [12] in 2012, which refers to the high-throughput extraction of a large number of image features from radiological images. In the same year, Kumar et al. supplemented the definition of radiomics to high-throughput extraction and analysis of a large number of advanced quantitative image features from CT, PET, and MRI [11], expanding the imaging modality and adding quantification analysis. In 2014, Aerts et al. published a breakthrough application in Nature Communications, pointing out the prognostic ability of radiomics [13], which caused widespread concern in the scientific research community. In general, radiomics refers to the extraction and analysis of highly representative quantitative image features from clinically large-scale imaging data, that is, using a large number of automated data feature description algorithms, the imaging data were transformed into high-dimensional extensible feature space, and the disease diagnosis and prediction of the case data were completed by comparing and analyzing the imaging data with clinical information. Furthermore, the radiomics analysis is based on the assumption that the quantitative imageebased parameters have a certain correlation with the molecular phenotype or genotype of the tumor. Radiomics postprocesses the medical images that need to be collected in clinical diagnosis and treatment, extracts information that is difficult to see with the naked eye, and combines with other genomic data, metabolic data, and protein data to improve the efficacy prediction and prognosis of the tumor. Thus, personalized treatment of the patient is achieved.
6 CHAPTER 1 Introduction


1.4 Value of radiomics
Due to the pathological characteristics of different tumor types, they have different imaging performances. Different tumor image features also indicate completely different treatment methods, which directly affect the prognosis. At present, based on the subjective clinical experience of the doctor, prejudgment of the tumor is achieved through medical images. Based on the existing medical image feature analysis, some multidimensional texture features can accurately reflect the pathological information of the diseased tissue, which has important research value for the realization of individualized medical treatment. Therefore, a complete feature database can filter the subsequent key features and provide more comprehensive data support. The use of radiomics methods to assist in the predictive analysis of tumors and to give credible recommendations is of great practical significance. Radiomics analysis mainly extracts and quantifies the features associated with a diagnosis from images. For example, in the CT images of tumors, there are differences in the tumors’ shape, size, and texture with different pathological grades. These features are often used by doctors as a basis for manual diagnosis, but the diagnostic results are subjective and relevant to the radiologists’ experience, which makes it difficult to realize objective and repeatable diagnosis results. However, in the analysis of radiomics, the features of these doctors’ qualitative descriptions can be quantitatively described by mathematical expressions from the perspective of images, thus providing an objective and repeatable diagnosis. Radiomics attempts to extract image features associated with diagnostic results through a large number of medical images, and directly analyzes results via the images, rather than just simple image processing. For example, researchers extracted high-dimensional image features by radiomics and found that these features were highly correlated with the prognosis survival of lung cancer, patients could be divided into different risk groups according to different values of these features, and different treatment plans could be implemented [13]. In addition to the use of imaging data, radiomics has introduced genetic analysis to improve diagnostic accuracy. In the traditional genetic analysis, whether a gene is mutated is determined by gene sequencing of tumor tissue sampled from a certain location of the tumor. However, due to the heterogeneity of the tumor, genetic mutations may occur in other parts of the tumor that are not sampled. Therefore, traditional genetic analysis may have sampling errors [4]. The mutated gene affects the growth of the tumor and is thus expressed in the imaging data [14]. Radiomic features can be extracted from the overall tumor image, which contains complete information. The genetic analysis in radiomics can complement the traditional genetic analysis to improve diagnostic accuracy.
1.5 Workflow of radiomics
Radiomics includes the following steps of data acquisition, lesion detection, lesion segmentation, feature extraction, and information mining for clinical decision
1.5 Workflow of radiomics 7


support. An automatic algorithm can be used to detect the lesion area after acquiring the imaging data. Manual or automatic segmentation is performed for the detected lesion area to obtain an accurate tumor area. High-dimensional features can be extracted for the tumor area by an image processing algorithm. Finally, the relationship between the feature and the pathological result is analyzed by machine learning or statistical methods to predict the pathological result through the imaging data. Radiomics analysis mainly includes the five key technologies [11] described in 1.5.1e1.5.5.
1.5.1 Image acquisition and reconstruction
Due to the different reconstruction methods and slice thickness of different scanners, the parameters (such as resolution) of images are also greatly different. In order to reduce the differences, on the one hand, it is possible to reduce the difference in parameters as much as possible by formulating a series of clinical medical image acquisition specifications. On the other hand, it is possible to select features that have high reproducibility between different patients, sufficient dynamics range [15], and are insensitive to image acquisition protocols and reconstruction algorithms in the analysis process.
1.5.2 Image segmentation
Radiomics analysis extracts features from the region of interest (ROI), including tumors, normal tissues, and other anatomical structures. The accurate segmentation of the ROI is very important for feature quantification, feature extraction, and statistical analysis. The manual segmentation results of experienced radiologists are often used as the gold standard, but this method is very time-consuming and has very large subjectivity. Therefore, it is not suitable for radiomics analysis based on large data volume medical images. The research of robust automatic or semiautomatic segmentation methods is a very important part of radiomics analysis.
1.5.3 Feature extraction and selection
Radiomic features extraction can be performed after the tumor area is determined. Classical features include the description of characteristics of tumor gray histograms (high or low contrast), tumor shape (circular or needle-like), texture features (homogeneous or heterogeneous), and relationship between tumor location and surrounding tissue. A large number of radiomic features can usually be extracted based on the extraction methods described above. The dimension of the features may be much larger than the sample size, but for a specific analysis target, not all features are valuable, so reducing the feature dimension and selecting the most valuable features is very important. Feature selection can be implemented either using machine learning or statistical analysis. In the selection process, in addition to considering high information volume and nonredundancy, high repeatability is also an indicator that needs to be considered.
8 CHAPTER 1 Introduction


1.5.4 Database and data sharing
The ultimate goal of radiomics is to establish the relationship between image features and tumor phenotypes or molecular features. It is necessary to establish a complete database of images, features, clinical data, and molecular data (Fig. 1.1). In the establishment of this database system, it is necessary to protect the patient’s privacy and hide the individual information of the patient in the Digital Imaging and Communications in Medicine (DICOM) data header file.
1.5.5 Informatics analysis
One of the most important aspects of the radiomics analysis is to propose appropriate, identifiable, reliable, and reproducible features that have potential applications in clinical diagnostics. The employment of existing bioinformatics analysis tools can be the first step in the analysis of the features, which can reduce the trouble of developing new analytical methods, as well as use widely accepted methods. On this basis, the improved methods specifically for the radiomics analysis mainly involve the following: (1) Multivariate validation problem; (2) Supervised or semisupervised analysis; (3) Classification for biomarker verification, etc. It is also very important to combine clinical and patient risk factors, as these risk factors may be related to medical image characteristics, or they may be statistically significant for the prediction analysis. Therefore, the combination of biostatistics, epidemiology, and bioinformatics is necessary to establish robust clinical predictive models that relate high-dimensional features to tumor phenotypes or gene protein markers. It is believed that when the database shown in Fig. 1.1 is established, the development of targeted analysis methods will be further promoted. In addition, due to the rapid development of natural image processing and artificial intelligence, deep learning models with feature self-learning capabilities have also been broadly applied in medical image analysis. Unlike the traditional radiomics process, deep learning does not require precise tumor boundary segmentation and artificially defined feature extraction but automatically learns the features associated with the pathological diagnosis from imaging data by self-learning. This is an end-to-end learning approach that has achieved remarkable success in the field of artificial intelligence and has also been gradually accepted in medical imaging processing. Deep learning techniques will also be explained in detail in this chapter as a new radiomics technique.
1.5.6 Medical image acquisition
A large amount of data are the consolidated foundation for reliable conclusions, especially in the field of radiomics, because radiomics seeks to mine the correlation between clinical data and high-dimensional quantitative features extracted from a great number of medical image data. However, collecting and integrating largescale medical image data are difficult. Firstly, most medical images are scattered at institutions or hospitals in various regions, and the number of medical images
1.5 Workflow of radiomics 9


FIGURE 1.1
Framework for the radiomics database. Mainly composed of four parts: patient/clinical data (blue), medical image type (orange), Radiomic
features (purple), and molecular data (green).
10 CHAPTER 1 Introduction


depends on the size of the local patient population. All of these factors make data resources markedly scattered. Secondly, it is very time-consuming to collect medical images. For example, it often takes several years to follow up on patients to determine their survival time when performing a prognostic analysis. Lastly, the instruments and scanning parameters and methods used by various hospitals or institutions are different, which may result in inconsistent or incomplete image formats, which would affect the accuracy of the analysis. Therefore, it is our priority to allow institutions and hospitals to start to creating large medical image datasets with consistent and rich diagnostic information. Recently, The Cancer Imaging Archive (TCIA) established a medical image data sharing platform, providing publicly available medical images and metadata for specific cancers from various institutions and hospitals around the world [14]. Based on these datasets, researchers and engineers can develop new medical image analysis methods and tools to validate their hypotheses or assist radiologists in making clinical decisions. For example, a large dataset that constitutes TCIA is the Lung Image Database Consortium image collection (LIDC-IDRI) [16,17]. The dataset contains the original CT image and four radiologists’ markers on the tumor boundary. It also provides some diagnostic information and detailed annotations of the lung nodules, including the location, benign and malignant information of the nodules, which provide good resources for verification of nodule detection and segmentation methods [18e21]. In addition, many researchers use this database for classification of benign and malignant pulmonary nodules. Since these medical images are obtained from different institutions and hospitals, different software and coding protocols are required for the reconstruction. Researchers using these datasets need to consider these differences to avoid potential unexpected effects.
1.5.7 Segmentation of the tumor
Tumor segmentation is a fundamental step in the radiomics analysis because it converts the original medical image into an image that can be extracted. Although the segmentation algorithm has been studied for a long time, the fully automatic segmentation algorithm still needs to be improved, especially in the field of medical image analysis. This is mainly due to the following difficulties in the existing segmentation algorithm: (1) There is no consented gold standard for nodules or tumor boundaries because there is adhesion between the tumor and surrounding tissues, and the definition of tumor boundaries has certain subjectivity. Due to subjective varieties, it is difficult to obtain consistent segmentation results [16]. This poses a challenge to the machine learningebased segmentation method because the segmentation labels of the training set are not necessarily true or robust. (2) It is very time-consuming for doctors to manually segment the tumors. Doctors must mark the tumor slice by slice, while larger nodules are usually distributed over many slices. Finally, nodules are not in regular geometric shapes, so it is difficult to model them. (3) Due to the influence of adhesions and other conditions, the tumor boundary
1.5 Workflow of radiomics 11


will be unclear, which also brings difficulties to segmentation. (4) The repeatability of segmentation is also very important because the following feature extraction procedure is performed on the segmented tumors, and repeatable segmentation ensures that the extracted features are stable and reliable. Despite the above difficulties and problems in the segmentation algorithm, researchers are still perfecting the segmentation method for specific situations [22,23]. At present, the segmentation methods of tumor images can be divided into three categories: manual segmentation, semiautomatic segmentation, and automatic segmentation. The commonly used segmentation algorithms mainly include threshold segmentation, image segmentation based on fuzzy theory, region-based image segmentation, and edge-based image segmentation. Although there have been a large number of tumor image segmentation algorithms, there has been no consensus on how to choose the right algorithm in different situations, how different segmentation algorithms affect postfeature quantization and feature extraction, what kind of segmentation method will be more consistent with the gold standard, and whether there is a general segmentation method.
1.5.8 Tumor image phenotype
The most crucial part of radiomics is to extract high-dimensional features to quantify the tumor. Based on the automatic feature extraction algorithm, radiomics can extract high-dimensional features from medical images [4,11e13]. Feature extraction is a bridge connecting images and clinical results, which can be divided into semantic features and nonsemantic features. Semantic features refer to the characteristics of qualitative descriptions by doctors, such as uniformity and edge spurs, which often lack effective mathematical expressions. Nonsemantic features can be quantitatively described as mathematical expressions. Currently, the general method for generating nonsemantic features is extracting them from segmented nodules based on mathematical expressions. Firstly, segmented markers can be used as a mask to extract shape-related features such as volume, surface area, and compactness. Secondly, the gray intensity of each pixel can be used to construct a large number of features, such as firstorder, second-order, and high-order statistical features [4]. The first-order features are mainly related to the gray distribution in the tumor area, such as the minimum value of the grayscale, maximum value, median value, entropy of the gray histogram, and the characteristics of kurtosis and skewness; the second-order features mainly describe the statistical correlation between the current pixel and surrounding pixels, such as texture features, which can be used to describe the heterogeneity of the umor. In the radiomics analysis, texture analysis can employ texture analysis to reflect intratumoral heterogeneity by a gray run-length matrix and gray function matrix. In addition, heterogeneity is often seen as a feature of malignant tumors, and it is reflected in many levels of genes to the macroscopic level. As an effective means to evaluate heterogeneity, using texture parameters to evaluate the treatment output or assess prognosis has been valued by researchers [24]. Texture analysis
12 CHAPTER 1 Introduction


has been used as a new tool for assessing intratumoral heterogeneity in medical images, especially in PET images. Moreover, since it reflects the relative change between adjacent pixels, the disadvantages of not being robust in SUV analysis can be overcome. Although texture analysis has not been widely used in clinical practice, more and more studies have demonstrated its important role in prediction of diagnosis or evaluation of prognosis. Some studies have demonstrated that texture parameters are superior to gray values in predicting treatment outcomes and overall survival in many cancers. Higher-order features such as wavelet and spectral analysis are often used to characterize repeated or nonrepetitive potential patterns [4]. In addition, there are shapes, positional features, and features that reflect the three-dimensional features of the tumor, such as sphericity, spikiness, position, and attachment characteristics. There are still many studies devoted to extracting more quantitative features from medical images. In an earlier study [11], 182 texture features and 22 semantic features were extracted from CT images of lung cancer; in recent research [13], researchers extracted 662 radiomic features based on the Laplace transform analysis of the Gaussian kernel, including 522 texture features and fractal features. Obviously, the number of radiomic features can be very high, so there is a risk of overfitting in the subsequent modeling analysis. Thus, a reasonable feature reduction must be performed. The common method is to first remove the redundant features. A highly correlated cluster of features can be dimensioned to a more representative feature. This feature usually requires the largest interindividual variability and dynamic range. After reducing the correlation between features, feature selection can be performed by some statistical methods, such as a two-sample T-test or an F-test, to select features that are statistically different between different patient populations. In addition, the features’ selection approach, such as Lasso [25] or Elastic-net [26] regression, can be used to select features useful to predicting benign and malignant tumors, staging, and prognosis. In general, feature extraction can produce hundreds of features; however, most of the features are correlated, so there is redundancy between the features, and feature selection is required to remove redundant features. In Ref. [15], the authors first selected features that were highly reproducible in multiple experiments and then used hierarchical clustering to extract highly dependent features. They also proposed a “dynamic range” metric function to characterize changes between different patients to rule out small variations between different patients. In the article [13], the author used a similar procedure to construct a radiomics label from four types of features. The feature dimension after feature selection is usually less than 100, which is much less than the feature dimension before selection.
1.5.9 Clinical prediction for tumor
The clinical prediction of tumors is also the process of knowledge mining of medical images, where the goal is to find the correlation between radiomics and clinical information. In many applications, this is converted into classification problems.
1.5 Workflow of radiomics 13


For example, the association between images and gene mutations can be converted into image classification problems (gene mutations and gene nonmutation). This step trains the classifier to learn the difference of the features between the two types. Feature mining related to patient survival is also an application of data mining. Machine learning and artificial intelligence algorithms play a very important role in this step. Since the tumor area and the infected tissue have similar gray values on the tumor image, it has always been a challenging clinical problem to distinguish them from each other. Yu et al. used the K-nearest neighbor classification (KNN, K-nearest neighbor) to classify the head and neck cancer from infected tissue based on texture features, which provide a basis for quantifying infection in future cancer diagnosis [27]. In addition, some research work focuses on assessing the malignant degree of the tumor. Chen et al. used image retrieval methods to extract texture features from breast cancer ultrasound images and predicted the benign and malignant tumors [28]. Acharya et al. extracted texture features from breast cancer thermal imaging and used the support vector machine method in classifying benign and malignant tumors [29]. Although many studies have shown that the texture features have significant differences between the different prognoses of cancer, it has not yet been established as a perfect prediction system. In addition, various prediction methods have their own advantages and disadvantages, and there is currently no universal method. This requires us to compare different classification methods to get the best effect on the specific classification prediction of certain types of cancer. The combination of radiomic features related to tumor diagnosis is similar to the cohort combination of biomarkers, based on which quantification and validation can be performed. Quantification is used to show that the data have a predictive effect on the predictor; and verification is used to demonstrate whether the predictive performance of the features is stabilized. That is, we need to identify the distinctive features, and verify whether they have predictive effects in the independent validation set. In the construction of the model, features are entered into the model to predict the patient’s diagnosis or prognosis. The choice of model depends on the label to be predicted. Generally, multiple models can be used to verify performance, and then a model with the best predictive performance will be selected. A simpler model can be more easily applied to an independent verification set, so it will also be the first choice for model construction. After the model is built, model performance verification must be performed on an independent validation set. When the number of subjects is small, cross-validationebased internal verification are also acceptable [30]. In addition, it is also necessary to judge the benefit from the decisionmaking of the model, evaluating whether the radiomics model can provide more clinical benefits than the judgment of the clinicians. Although the output of the radiomics model can determine the final predictive performance, successful establishment of the model lies in the availability and interactivity of the model, which can decide whether the model can be clinically accepted. If the model’s predictions cannot be reasonably explained, even if the radiomics model is established by a large patient population and successfully
14 CHAPTER 1 Introduction


validated on an independent validation set, this model still cannot be clinically accepted. In addition, the radiomics model needs to be visualized. A typical visualization method is the nomogram. Nomogram is a graphical display method proposed in the early 20th century. It can easily apply the radiomics model to clinical practice for decision-making assistance.
1.5.10 New technology of artificial intelligence
Artificial intelligence, especially deep learning, has the potential to study clinical tumor imaging. Artificial intelligence algorithms can quantify data patterns that have made significant advances in autonomous driving and speech recognition [x]. Radiomics is the application of artificial intelligence in medical imaging, which can provide quantitative imaging characteristics for tumor tissues. This information can be used to predict diagnosis and assist clinical decision-making, as well as to evaluate treatment efficacy. Radiomics has attracted wide attention in recent years due to its huge potential for clinical application, and more research results are expected in the near future. At present, many studies have made robust and meaningful findings, but there are still some studies that have problems with unreasonable laboratory design, such as lack of model validation, failure to perform good variable control, etc. These problems not only lead to unreasonable results but also cause other studies to make the same mistakes. The enormous potential of artificial intelligence has led to many related studies on medical imaging. With some research on abnormal situation monitoring or automatic disease quantification, others focus on the mapping between imaging phenotypes and genes. Given the positive results of the current study, radiomics research will continue to make progress in the near future. Deep learning is under rapid development recently with various applications. Convolutional neural network (CNN) is one of the most widely used models in the image applications. The first successful application of CNN dates back to the 1990s, when LeCun used the LeNet-5 model for handwritten digit recognition [31]. The second wave of development is that the CNN model won the first place in the Imagenet Large-Scale Visual Identity Competition in 2012 [32]. Due to the development of hardware technology (such as the graphics processor GPU), researchers can now design larger networks to solve more complex problems. Many applications in medical image analysis focused on improving the performance of existing CAD systems, such as improving nodule detection accuracy [33] and diagnostic accuracy [34]. In recent years, Kumar et al. introduced the concept of “exploration Radiomics” for lung cancer detection of CT images [35]. Their proposed radiomics framework uses a deep CNN model to determine the phenotype of the tumor, which extracts abstract imageebased features. There are two main advantages in using the CNN model: (a) Since CNN is an end-to-end machine learning architecture, its input can be the original segmented images [36]; (b) discriminative features are automatically learned during training. These advantages are very important to improve the classification accuracy of the model. As mentioned earlier, there
1.5 Workflow of radiomics 15


are many difficulties in segmentation, but it is inevitable for the current radiomics process operation. Unreliable segmentation can affect feature extraction and even produce wrong predictions. In the feature selection step, the features associated with the disease or prognosis remain unknown. Therefore, a lot of unrelated features are extracted before feature selection, and deep learning can greatly accelerate this time-consuming feature extraction process. Although deep learning has many advantages, there are still many difficulties in related research. For example, the features selected by deep learning are not directly interpretable, and the association of these features with diagnostic data is unknown. In other words, the deep learning features are conceptually similar to black boxes for researchers. Making deep learning features more interpretable and friendly useable for clinicians will be the center for deep learning applications in medicine. In the field of natural image processing, there is a good research environment for everyone to share their well-trained deep neural network. In this way, other researchers do not need to spend a long time to retrain the model. In the field of medical imaging, deep learning development is still in its early stages. Many researchers prefer to use existing models for reproduced results and rapid development. Therefore, creating open source communities is of great significance for the application of deep learning in the field of medical imaging.
1.6 Prospect of clinical application of radiomics
Radiomics is a rapidly developing field in radiology and oncology. In the past few years, extensive studies have shown that image features are associated with cancer diagnosis and gene expression [13,37e39]. At present, the clinical application of radiomics mainly includes auxiliary diagnosis, treatment evaluation, and prognosis prediction. With the development of medical image big data, more abundant image data will enable the application of artificial intelligence methods. At the same time, excellent clinical manifestations obtained by various advanced pattern recognition methods also enable the application of radiomics in auxiliary diagnosis, treatment evaluation, and prognosis prediction. Now, more researchers are entering the field of radiomics clinical applications [39e44]. The current research results show the promise that radiomics can be used as an auxiliary and complementary tool for doctors’ diagnosis. More radiomics applications, implementation details, new opportunities, and challenges are covered in the following chapters.
References
[1] Stewart BW, Wild CP, editors. World cancer report 2014. Cancer worldwide. International Agency for Research on Cancer; 2014. [2] Chen W, et al. Cancer statistics in China, 2015. CA Cancer J Clin 2016;66(2):115e32. [3] O’Connor JPB. Rethinking the role of clinical imaging. eLife 2017;6:e30563.
16 CHAPTER 1 Introduction


[4] Gillies RJ, Kinahan PE, Hricak H. Radiomics: images are more than pictures, they are data. Radiology 2016;278(2):563e77. [5] Buckler AJ, et al. A collaborative enterprise for multi-stakeholder participation in the advancement of quantitative imaging. Radiology 2011;258(3):906e14. [6] Kurland BF, et al. Promise and pitfalls of quantitative imaging in oncology clinical trials. Magn Reson Imag 2012;30(9):1301e12. [7] Segal E, et al. Decoding global gene expression programs in liver cancer by noninvasive imaging. Nat Biotechnol 2007;25(6):675e80. [8] Sun R, et al. A radiomics approach to assess tumour-infiltrating CD8 cells and response to anti-PD-1 or anti-PD-L1 immunotherapy: an imaging biomarker, retrospective multicohort study. Lancet Oncol 2018;19(9):1180e91. [9] Itakura H, et al. Magnetic resonance image features identify glioblastoma phenotypic subtypes with distinct molecular pathway activities. Sci Transl Med 2015;7(303). [10] Mu W, et al. Non-invasive decision support for NSCLC treatment using PET/CT radiomics. Nat Commun 2020;11(1). [11] Kumar V, et al. Radiomics: the process and the challenges. Magn Reson Imag 2012; 30(9):1234e48. [12] Lambin P, et al. Radiomics: extracting more information from medical images using advanced feature analysis. Eur J Cancer 2012;48(4):441e6. [13] Aerts HJWL, et al. Decoding tumour phenotype by non-invasive imaging using a quantitative radiomics approach. Nat Commun 2014;5:4006. [14] Gatenby RA, Grove O, Gillies RJ. Quantitative imaging in cancer evolution and ecology. Radiology 2013;269(1):8e15. [15] Balagurunathan Y, et al. Reproducibility and prognosis of quantitative features extracted from CT images. Transl Oncol 2014;7(1):72e87. [16] Armato III SG, et al. The Lung Image Database Consortium, (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans. Med Phys 2011;38(2):915e31. [17] McNitt-Gray MF, et al. The Lung Image Database Consortium (LIDC) data collection process for nodule detection and annotation. Acad Radiol 2007;14(12):1464e74. [18] de Carvalho Filho AO, et al. Automatic detection of solitary lung nodules using quality threshold clustering, genetic algorithm and diversity index. Artif Intell Med 2014;60(3): 165e77. [19] Diciotti S, et al. Automated segmentation refinement of small lung nodules in CT scans by local shape analysis. IEEE Trans Biomed Eng 2011;58(12):3418e28. [20] Orban G, Horvath G, IEEE. Lung nodule detection on digital tomosynthesis images: a preliminary study. In: 2014 IEEE 11th international symposium on biomedical imaging; 2014. p. 141e4. [21] Song J, et al. Lung lesion extraction using a toboggan based growing automatic segmentation approach. IEEE Trans Med Imag 2016;35(1):337e53. [22] Gu Y, et al. Automated delineation of lung tumors from CT images using a single click ensemble segmentation approach. Pattern Recognit 2013;46(3):692e702. [23] Song J, et al. Lung lesion extraction using a toboggan based growing automatic segmentation approach. IEEE Trans Med Imag 2015;35(1):337e53. [24] Tomasi G, Turkheimer F, Aboagye E. Importance of quantification for the analysis of PET data in oncology: review of current methods and trends for the future. Mol Imag Biol 2012;14(2):131e46.
References 17


[25] Friedman J, Hastie T, Tibshirani R. Regularization paths for generalized linear models via coordinate descent. J Stat Softw 2010;33(1):1e22. [26] Tibshirani R. Regression shrinkage and selection via the Lasso. J R Stat Soc Series B Stat Methodol 1996;58(1):267e88. [27] Yu H, et al. Automated radiation targeting in head-and-neck cancer using region-based texture analysis of PET and CT images. Int J Radiat Oncol Biol Phys 2009;75(2): 618e25. [28] Chen D-R, Huang Y-L, Lin S-H. Computer-aided diagnosis with textural features for breast lesions in sonograms. Comput Med Imag Grap 2011;35(3):220e6. [29] Acharya UR, et al. Thermography based breast cancer detection using texture features and support vector machine. J Med Syst 2012;36(3):1503e10. [30] Moons KGM, et al. Risk prediction models: II. External validation, model updating, and impact assessment. Heart 2012;98(9):691e8. [31] LeCun Y, et al. Gradient-based learning applied to document recognition. Proc IEEE Inst Electr Electron Eng 1998;86(11):2278e324. [32] Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems; 2012. [33] van Ginneken B, et al. Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomography scans. In: Biomedical imaging (ISBI), 2015 IEEE 12th international symposium on 2015. IEEE; 2015. [34] Shen W, et al. Multi-scale convolutional neural networks for lung nodule classification. Inf Process Med Imag 2015;24:588e99. [35] Kumar D, et al. Discovery radiomics for computed tomography cancer detection. arXiv preprint arXiv:1509.00117. 2015. [36] Shen W, et al. Multi-scale convolutional neural networks for lung nodule classification. In: Information processing in medical imaging. Springer; 2015. [37] Ozkan E, et al. CT gray-level texture analysis as a quantitative imaging biomarker of epidermal growth factor receptor mutation status in adenocarcinoma of the lung. Am J Roentgenol 2015;205(5):1016e25. [38] Coroller TP, et al. CT-based radiomic signature predicts distant metastasis in lung adenocarcinoma. Radiother Oncol 2015;114(3):345e50. [39] Parmar C, et al. Radiomic feature clusters and prognostic signatures specific for lung and head & neck cancer. Sci Rep 2015;5:11044. [40] Leijenaar RT, et al. The effect of SUV discretization in quantitative FDG-PET radiomics: the need for standardized methodology in tumor texture analysis. Sci Rep 2015;5:11075. [41] Grove O, et al. Quantitative computed tomographic descriptors associate tumor shape complexity and intratumor heterogeneity with prognosis in lung adenocarcinoma. PLoS One 2015;10(3):e0118261. [42] Cameron A, et al. MAPS: a quantitative radiomics approach for prostate cancer detection. 2015. [43] Parmar C, et al. Robust radiomics feature quantification using semiautomatic volumetric segmentation. 2014. [44] Cook GJ, et al. Radiomics in PET: principles and applications. Clin Transl Imag 2014; 2(3):269e76.
18 CHAPTER 1 Introduction


Key technologies and software platforms for
radiomics 2
Chapter outline
2.1 Tumor detection ................................................................................................. 20 2.1.1 Data preprocessing ........................................................................... 20 2.1.2 Detection of candidate nodules.......................................................... 22 2.2 Tumor segmentation ........................................................................................... 24 2.2.1 Segmentation of pulmonary nodules based on the central-focused convolutional neural network ............................................................. 25 2.2.2 Segmentation of brain tumor based on the convolutional neural network33 2.2.3 Fully convolutional networks.............................................................. 34 2.2.4 Voxel segmentation algorithm based on MV-CNN................................. 35 2.3 Feature extraction .............................................................................................. 39 2.3.1 The features of artificial design.......................................................... 40 2.3.2 Deep learning features ...................................................................... 41 2.4 Feature selection and dimension reduction .......................................................... 43 2.4.1 Classical linear dimension reduction .................................................. 43 2.4.2 Dimension reduction method based on feature selection ...................... 43 2.4.3 Feature selection based on the linear model and regularization ............ 46 2.5 Model building................................................................................................... 52 2.5.1 Linear regression model .................................................................... 52 2.5.2 Linear classification model................................................................ 57 2.5.3 Tree models ..................................................................................... 61 2.5.4 AdaBoost......................................................................................... 62 2.5.5 Model selection................................................................................ 64 2.5.6 Convolutional neural network............................................................. 65 2.5.7 Migration learning ............................................................................ 71 2.5.8 Semisupervised learning ................................................................... 76 2.6 Radiomics quality assessment system ................................................................. 83 2.7 Radiomics software platform............................................................................... 85 2.7.1 Radiomics software .......................................................................... 85 2.7.2 Pyradiomicsdradiomics algorithm library ........................................... 86 References ............................................................................................................... 97
CHAPTER
19
Radiomics and its Clinical Application. https://doi.org/10.1016/B978-0-12-818101-0.00003-3 Copyright © 2021 Elsevier Inc. All rights reserved.


The key technologies of radiomics include the automatic segmentation of tumors, the extraction of radiomic features, and the construction of radiomics models. In order to measure the performance of the radiomics model, a unified quality assessment system is also needed to measure the clinical value of the developed model. In radiomics research, it is necessary to use artificial or automatic algorithms for tumor segmentation (detection); then massive radiomics feature extraction for the tumor ROI region; since the extracted features contain high-latitude irrelevant information, feature dimensionality reduction of radiomic features is needed to identify key radiomic features; and finally, a small number of key radiomic features are used for the modeling analysis to provide clinically available radiomics models. At the end of this chapter, we will introduce the radiomics platform and software developed by relevant teams to facilitate radiomics research.
2.1 Tumor detection
For the past decades, researchers have been developing computer-aided diagnostic systems (CAD) to improve the diagnostic efficiency and accuracy. As the most common cancer, early screening for lung cancer is of great significance in CAD development. Due to a large number of CT image sequences and relatively small pulmonary nodules, it is common to have missed diagnosis frequently. Therefore, the CAD system for the detection of pulmonary nodules is of great clinical value. A typical pulmonary nodule detection system consists of three parts: (1) data preprocessing, (2) detection of candidate pulmonary nodule, and (3) reduction of false-positive rate. Data preprocessing is used to standardize data, limit nodule search space in the lungs, and reduce noise and artifacts. In the detection of candidate pulmonary nodules, the algorithm will detect suspected pulmonary nodules with a high sensitivity to improve recall rate, which usually has a high false-positive rate. Subsequently, a large number of false-positive nodules will be removed during the phase of reducing false positives. A typical CAD system for detecting pulmonary nodules is described in detail as follows.
2.1.1 Data preprocessing
Data preprocessing plays an important role in many deep learning algorithms. In an actual situation, many algorithms likely achieve optimal performance after data are normalized and whitened. Thus, finding suitable parameters of prepressing analysis becomes an important focus for medical image analysis. The scanning parameters of medical image data such as layer thickness and layer spacing have great influence on the algorithm. Medical images usually were generated from different parameters and protocols in different medicine centers, and algorithms should design for mitigating the difference of multicenter data variance. Reproducibility and quality of the extracted features of CT, PET, MRI, and US, caused by variant image acquisition parameters, are crucial for radiomics research. It was essential for researchers to overcome the interference of variant image
20 CHAPTER 2 Key technologies and software platforms for radiomics


acquisition parameters toward building reliable and accurate radiomics models. Standard imaging protocols or algorithms for overcoming interference of acquisition parameters were necessary to be considered. Therefore, before target detection, data resampling is needed to keep all the data resolution consistent. At the same time, the pixel value of CT images needs to be preprocessed to ensure that it is within a reasonable range, for example: (1) The lung window or mediastinal window is used to limit the amplitude, and only the lung tissue is retained. (2) Perform z-score normalization on CT images to make the mean value and standard deviation of the images 0 and 1, so as to facilitate subsequent algorithm processing. (3) Suppress outliers in CT images, such as removing pixels, which are 3 times the size of the standard deviation except for the mean value and removing noise.
1. Data normalization In data preprocessing, the first step of the standard is data normalization. While there are a number of possible approaches, this step is usually chosen based on the specific situation of the data explicitly. Common methods of feature normalization include the following: (1) Simple scaling (2) Per-example mean subtraction (3) Feature standardization (making all features in the dataset have zero mean and unit variance) In simple scaling, the goal is to resize the values of each dimension of the data (which may be independent of each other) so that the final data vector falls between [0, 1] and [ 1, 1]. This is important for subsequent processing because many default parameters (such as epsilon in principal component analysis (PCA) whitening) assume that the data have been scaled to a reasonable range. In dealing with natural images, for example, we obtain the pixel values in the interval [0, 255], the commonly used treatment is the pixel values divided by 255, making them zoom in [0, 1]. In per-example mean subtraction, if your data are smooth (each dimension data statistics are subject to the same distribution). Example: Normalization removes the average intensity of the image. In many cases we are more interested in the image contents than the illumination of the image, so it makes sense to subtract the mean values of pixels for each data point. Note: although this method is widely used in dealing with images, extra care should be taken when processing color images because pixels in different color channels do not all possess specific smooth properties. 2. Feature standardization Feature standardization is the most common approach to normalization, which enables each dimension of data has a zero mean and unit variance. In practice, feature standardization is done by calculating the mean value of the data on each dimension (using the whole data) and then subtracting the mean value on each dimension. The next step is the data of each dimension divided by the standard deviation of the data in that dimension.
2.1 Tumor detection 21


3. PCA/ZCA whitening After simple normalization, whitening is usually used as the next step of preprocessing, which makes our algorithm work better. In fact, many deep learning algorithms rely on whitening to get good features. In PCA/ZCA whitening, Zero-centered is necessary which makes the 1
m
P
i
xðiÞ 1⁄4 0:
It should be noted that this step needs to be done before calculating the covariance matrix (the only exception is that the per-example mean subtraction has been done and the data are flat across dimensions or pixels). Next, in PCA/ZCA whitening, appropriate parameters need to be selected. Selecting appropriate parameter values plays an important role in feature learning.
2.1.2 Detection of candidate nodules
Candidate nodule detection is the use of a rectangular frame in CT images to detect possible nodule areas. This stage is designed to ensure that a nodule is not missed with high sensitivity. At present, the commonly used region detection algorithm of interest can be realized by image processing. The lung area was first segmented using CT thresholds and potential candidate nodules were then detected in the lung area. In this method, the window width of 300HU to 750HU was first used to remove the CT values of blood vessels and trachea. Then, morphological corrosion operations were used to remove the noise generated by blood vessels and other tissues. Finally, all voxels were clustered using connected component analysis to obtain regions of interest. These steps result in a large number of regions of interest, most of which are not pulmonary nodules. Therefore, feature extraction is required for all regions of interest to screen for true pulmonary nodules. Commonly used features include the following categories: intensity features, texture features, and morphological features [1]. Intensity features: for voxels in the candidate nodule region, using 50HU group spacing to calculate the normalized histogram of image intensity, and calculating the following statistics: entropy, mean, average histogram spacing, kurtosis, skewness, peak, and histogram spacing of 5%, 25%, 50%, 75%, and 95%. In addition, standard deviations, minima, and maxima can also be used to describe the intensity distribution. Texture features: texture features mainly use a local binary pattern [2] (LBP) and 2D Haar wavelet. Both describe local spatial texture information and have been widely used in CT image texture analysis. These features can help eliminate false positives in ground glass areas caused by motion artifacts. Shape features: the shape characteristics of most pulmonary nodules are ellipsoid but not tumor areas such as blood vessels, which may appear as a strip. Thus, shape becomes an important feature in distinguishing true and false positives. Shape features mainly include spherical degree, compactness, radius, and so on.
22 CHAPTER 2 Key technologies and software platforms for radiomics


After extracting the above features for all candidate nodules, we used a classifier to classify each candidate nodules and finally determined whether it was a real pulmonary nodule. Because different machine learning algorithms have their own advantages and disadvantages, it is necessary to choose a classifier that matches the features. In the process of classifier selection, 5-fold cross-validation or 10fold cross-validation is often used on the training set to select the best parameter for the classifier. Here are some common classifiers and their advantages and disadvantages.
1. Naive Bayes algorithm: The principle of Bayes classifier is to use the prior probability of each category, then use Bayes formula and independence assumption to calculate the likelihood and posterior probability category of the object, namely the probability category of the object to which the item belongs, and then select the object with the highest posterior probability as the category of the object. Its advantages are as follows: (1) Solid mathematical foundation, stable classification efficiency, easy to explain; (2) Estimation requires few parameters and is not sensitive to the lost data; (3) There is no need for a complex iterative solution framework for large datasets. However, it has the following disadvantages: (1) It may not satisfy the assumption of independence between attributes; (2) It is necessary to know prior probability, and the classification decision has a specific error rate. 2. Logistic regression algorithm: Binomial logistic regression model is a classification model, expressed by conditional probability distribution P(Y|X) in the form of parameterized logistic distribution. The random variable X is a real number, and the random variable Y is 1 or 0. Model parameters can be estimated by supervised learning. The model has the following advantages: (1) low computational cost, easy to understand and implement; (2) applicable to numeric and subtype data. But it also has the following disadvantages: (1) easy to underfit; (2) classification accuracy may not be high. 3. Support vector machine (SVM) algorithm: For two types of linearly separable learning tasks, the SVM finds a hyperplane with the most considerable interval to separate the two types of samples. The most considerable break can ensure that the hyperplane has the best generalization ability. In the case of small samples, the model shows good performance, but it is sensitive to missing data and depends on the choice of specific and function of nonlinear problems. 4. Decision tree: Decision tree is a heuristic algorithm, in which each node selects features by using criteria such as information gain, and then constructs a tree classifier recursively. The computational complexity of the model is not high, and it is easy to understand and explain. It can handle both data type and subtype characteristics. However, for the data with an inconsistent number of samples, the result of information gain tends to the class with more samples, thus reducing the classification performance. In addition, too deep trees are prone to overfitting problems.
2.1 Tumor detection 23


5. AdaBoost algorithm: AdaBoost algorithm is from a weak learning algorithm, gets a series of weak classifier (namely basic classifier) by learning repeatedly, and then combines these weak classifiers to be a strong classifier; the most ascending method is changing the probability distribution of the training dataset (the weight distribution of training data) according to different training data distributions, and the weak learning algorithm is used to learn a series of weak classifiers. This kind of integrated classifier usually has high classification accuracy. Moreover, various methods can be used to construct subclassifiers and is not easy to cause overfitting.
2.2 Tumor segmentation
When the location of the tumor is detected, accurate tumor segmentation is further essential for quantitative/pixel-wise analysis of tumor images, which includes manual, semiautomatic, and automatic segmentation methods. However, it was difficult to segment tumors because of the extensive heterogeneity of tumors and their similarity of the visual features to the surrounding environment. Firstly, there are no golden standard methods for accurate tumor segmentation. Each slice of each imaging sequence was inevitable to be checked and sketched by radiologists, and a tumor mask for a large tumor usually contains dozens of slices. Secondly, it was difficult to overcome differences between existing numerous morphological variations and geometric objects during segmentation modeling. Thirdly, tumor margins could be blurred by the partial volume effect or noise, which was not precisely defined in medical images. The following sections will detail the relevant technologies of tumor segmentation, such as pulmonary nodules segmentation, pulmonary nodules segmentation based on the central pooling convolutional neural network [3] (CF-CNN), and tumor segmentation based on full convolution. Tumor segmentation determines the region of interest that will be analyzed in subsequent steps of radiomics. Since feature extraction is based on segmented tumors, the segmentation method can ensure the reproducibility and reliability of radiomics features. Intraclass correlation coefficient (ICC) is used to assess inter-reader and internal agreement. Some studies recommend the use of acceptable ICC to detect ROI so that further extracted features can be used. Many automatic or semiautomatic segmentation methods have been developed to reduce labor costs and increase the reproducibility of tumor segmentation. In CT image analysis of a tumor, the cost of obtaining clinical labels or labeling data is relatively high, which leads to a relatively small amount of normalized tumor CT image data. If it is directly analyzed in hundreds of samples, it is difficult to train the deep learning model and give full play to the advantages of deep learning. Although the volume of CT data for patient-grade tumors is small, a CT image of a tumor typically contains a large number of voxel points. For example, a CT image of a typical lung tumor may contain 10 10 5 1⁄4 500 voxels. If every voxel point
24 CHAPTER 2 Key technologies and software platforms for radiomics


in the CT image of tumor is taken as a training sample, a large number (tens of thousands) of training samples can be generated, and then more complex deep learning models can be trained. This is the idea of a voxel-level classification algorithm. In the task of medical image segmentation with a small sample size, it is a good method to transform the segmentation problem into a voxel-level classification problem. By transforming the image segmentation problem into a classification problem of every voxel point, every voxel point (pixel) can be taken as a training sample to expand the dataset. Although the above studies generated a large number of training samples by converting the segmentation problem into the voxel-level classification problem, the CNN network they used lacked specific improvements for the voxel-level classification problem. According to the characteristics of voxel-level classification, if a CNN network structure matching the voxel-level classification can be designed, this will make the voxel-level classification method work better. In order to solve the problem of segmentation of lung tumors in CT images, the voxel-level classification method was used to expand the training samples. At the same time, according to the characteristics of the voxel-level classification method, the structure of the CNN network and the selection method of training samples are improved, and a new CNN model and training sample sampling method are proposed.
2.2.1 Segmentation of pulmonary nodules based on the centralfocused convolutional neural network
Before deep learning was widely applied in the field of computer vision, TextonForest and a classifier based on random forest were generally used for tumor semantic segmentation. The Texton-based texture analysis method expresses the image as the histogram distribution of the texture image. The Texture dictionary can be a set of filter responses or neighborhood brightness. The Texton classification based on neighborhood brightness includes four parts: Texton dictionary construction, Texton histogram generation, classifier training, and classifier result prediction. The algorithm of image semantic segmentation based on random forest randomly samples a fixed-size window from the training library image as a feature, and compares the pixel values of two pixels randomly selected in the window, and quantizes these features into a numerical vector. The vector set is used to train the random forest classifier. Convolutional neural network (CNN) is not only beneficial to image recognition but also successful in image semantic segmentation. A common semantic segmentation method in deep learning is used to transform image segmentation into pixel classification. For each pixel in the image, we take the image with a fixed neighborhood size around it as the center and classify it. Finally, the label of neighborhood classification is assigned to the label of the central pixel. In other words, the image blocks around the pixel are used to classify each pixel. When the tumor is segmented by voxel-level classification, the target voxel is always located in the center of the neighborhood image. At this time, the importance of images at different spatial positions in the neighborhood image is also different. For example, the image
2.2 Tumor segmentation 25


at the center of the neighborhood image is closer to the target voxel and is more important. However, the image at the edge of the neighborhood image is far away from the target voxel and has less useful information. In view of the phenomenon that images in different spatial positions have different importance in the voxellevel classification method, researchers proposed the central-focused convolutional neural network (CF-CNN) to segment lung tumors. For each voxel in the CT image, we extracted a 3D neighborhood image centered on the voxel and a 2D multiscale neighborhood image as the input of the CF-CNN model, and predicted whether the voxel belonged to the tumor or normal tissue. The CF-CNN model proposed by researchers is a classical segmentation algorithm of pulmonary nodules based on pixel classification [3]. The CF-CNN model uses the 2D and 3D network structure to segment pulmonary nodules. For a voxel in a CT image, the model takes this voxel as the center and extracts the 3D neighborhood and the multiscale 2D neighborhood (patch) as the input of the CNN model. A classification label is utilized to determine whether this is a pulmonary nodule or a normal tissue as the output. The model includes two training network branches with identical image structure but different image scales. Each branch consists of six convolutional layers, two central pooling layers, and a full connected layer. The six convolution layers of the CNN model are divided into three computing units, each of which is composed of two convolution layers with 3 3 convolution kernels. These layers convolve all the input feature graphs and use the activation function parametric rectified linear unit (PReLU) to obtain the corresponding output feature graphs. The structure of the CF-CNN model is shown in Fig. 2.1. The CF-CNN network includes two branches with the same structure, which, respectively, use 2D and 3D neighborhood images for training. Each branch of CF-CNN includes six convolutional layers, two central pooling layers, and a full connected layer. The six convolution layers in each branch are divided into three convolution blocks, each of which uses the exact same structure, including two convolution layers with a convolution kernel size of 3 3. These layers convolve the input image to learn the mapping from the image to the voxel point category label. In order to speed up the training process, Batch Normalization is carried out after each convolution layer to eliminate the mean and variance drift of its output values. After batch normalization, the PReLU function is used as the activation function for nonlinear transformation. Between each convolution block, a new pooling method called central pooling layer is proposed by researchers to select a more effective subset of features from the convolution layer. After the last convolutional layer, a fully connected layer F7 is used to capture the relationship between the features of the different convolutional layers. This network consists of six convolution layers (C1eC6), two central pooling layers (central pooling layer 1 and central pooling layer 2), and two full connected layers (Fig. 2.1, F7, F8). The size of the convolution kernel is the filter width the filter height of the convolution filter (for example, 36@3 3 represents 36 convolution kernels with a size of 3 3). The number below each convolution layer
26 CHAPTER 2 Key technologies and software platforms for radiomics


represents the size of the convolved feature graph. After the neighborhood images of all voxel points were input into the CNN model, a probability graph of the same size as the input image was obtained, in which the value of each voxel point represented the probability that it belonged to a lung tumor. The output of the model is the probability graph of the image, in which each value is the probability that the pixel in the corresponding position belongs to the tumor. Each convolutional layer of the input image carries out the operation of
f j 1⁄4 PReLU P
i
cij f i þb j in which f i and f j represent the feature graph of ith
channel and jth channel, respectively. bj is the offset of the jth channel. To speed up the training process, the model uses batch normalization to normalize the output after each convolution operation. After the convolution layer calculation, the model uses the nonlinear activation function for nonlinear mapping. The nonlinear activation function is a PReLU [4], whose formula is expressed as follows:
PReLU f j 1⁄4
( f j if f j > 0
aj f j if f j 0
(2.1)
In this equation, aj is a training parameter, and j represents the jth feature graph in the convolution layer. In this model, aj is initialized to 0.25. When the input is a negative number, PReLU contains a nonzero slope parameter aj, and has been shown
FIGURE 2.1
CF-CNN structure.
2.2 Tumor segmentation 27


to be more effective than the traditional ReLU function. In the middle of each block, a method called central pooling is used to select a subset of features from the convolution layer. After the last convolution layer, a full connected layer is adopted, with each output unit connected to all the input units. The full connected layer can obtain the correlation of different features. To obtain nonlinear transformations, PReLU is used as the activation function after the full connected layer. At the end of the model, the outputs of the two CNN branches are combined by the full connected layer to describe the correlation between the features of the two branches. CF-CNN uses a two-branch network structure to learn both 3D and 2D information. 3D branching uses a voxel size of 3 35 35 3D neighborhoods image as input. Specifically, given a voxel point, we select a 3 35 35 cuboid centered on the voxel as the 3D neighborhood image of the voxel and input it into the 3D branch. The 3D neighborhood image includes the CT section where the target voxel is located and the upper and lower CT sections of the target voxel. The rectangular region of 35 35 is taken from each section. The images of three CT sections are fed into the 3D branch as three-channel images. Due to the large gray value distribution range of CT images, researchers used the z-score to standardize the three-channel images. At the same time, researchers designed a 2D branch to focus on learning more fine-grained features from axial images because axial images have a higher resolution in CT images than the other two directions (coronal and sagittal). First, two 2D neighborhood images with dimensions of 65 65 and 35 35 were selected with the target voxel as the center. They are then rescaled to the same size (35 35) using a cubic spline interpolation algorithm to form a two-channel image and feed into a 2D CNN branch. This multiscale neighborhood image strategy can use only one network to learn multiscale features without training multiple individual networks. In addition, compared with traditional maximum pooling, the model proposes a central pooling layer. Unlike the maximum pooling layer, the central pooling layer uses nonuniform pooling operations. In the central pooling operation, a smaller pooling window is used for the image center, and a larger pooling window is used for the image edge so as to ensure that more information from the center of the image is retained after the pooling operation, while less information from the edge of the second image is retained. In the classification of pixel points, the target pixel is always in the center of the image, while the background pixel is around the image. This nonuniform central pooling operation can ensure that the target pixel of the central point is enlarged while the surrounding redundant information is taken out, so as to prevent the target information from being lost caused by multiple pooling (Fig. 2.2). The central pooling method contains two parameters: (1) the size of each pooling window; (2) the number of each pooling window. In this study, we used three kinds of maximum pooling Windows (pooling window size s 1⁄4 1, 2, 3). The number of pooling Windows of each type can be determined by the following rules: (a) similar to the traditional maximum pooling method, we guarantee that the output image after the central pooling operation is half the size of the input image in each
28 CHAPTER 2 Key technologies and software platforms for radiomics


FIGURE 2.2
Schematic diagram of central pooling. (A) Traditional maximum pooling layer; (B) central pooling layer.
2.2 Tumor segmentation 29


direction. (b) In order to avoid image distortion caused by nonuniformly distributed pooling Windows, we keep half of the pooling Windows at the size of 2 2, which is often used in a traditional maximum pooling operation. After determining the number of three kinds of pooling Windows, we make a symmetrical distribution of all pooling Windows. For example, small pooling Windows (s 1⁄4 1) are distributed in the center of the image, while large pooling Windows (s 1⁄4 2, 3) are symmetrically distributed near the edge of the image. Given an input image of size O O, the following equation can be used to determine the number of three types of pooling Windows n1, n2, and n3. In the process of central pooling, the amount of the pooling window size can be calculated automatically through the following constraint conditions:
8>>>>>><
>>>>>>:
n1 þ 2n2 þ 3n3 1⁄4 O
n1 þ n2 þ n3 1⁄4 O
2
n1 þ 3n3 1⁄4 n2
(2.2)
In order to simplify the central pooling operation, the method stipulates that only three types of pooling Windows with sizes of 1 1, 2 2, and 3 3 are used, the number of them is n_1, n_2, and n_3 and O is the image size. After determining the number of pooling Windows of three sizes, the 1 1 pooling window is located in the center of the image, the 2 2 pooling window is located in the middle of the image, and the 3 3 pooling window is placed at the outer edge of the image to complete the central pooling operation. Fig. 2.3 shows the contrast between central pooling and traditional maximum pooling: When using the CF-CNN model, cuboid ROI is needed to select the region of the lung tumor, and then voxel points are classified in this region to the segment. Because lung tumors are typically distributed on multiple sections, the process of
FIGURE 2.3
Central pooling effect. Note: (A) input image, (B) feature map, (C) traditional maximum pooling, and (D) Central pooling.
30 CHAPTER 2 Key technologies and software platforms for radiomics


delineating rectangular ROI regions on each section is cumbersome. So we simplify user interaction through 3D processing steps. The user only needs to specify a rectangular ROI on a CT section to mark the tumor location. The user-tagged section is defined as the starting section (S1 in Fig. 2.4). Subsequently, user-specified ROI will be applied iteratively to the sections above and below the initial section until at least one of the following two termination conditions is satisfied: (a) CF-CNN does not segment the tumor in this section (Fig. 2.4, Section S6); (b) in this section, the tumor area segmented by CF-CNN was less than 30% of the tumor area in the previous section. For example, Section S3 in Fig. 2.4 was finally removed because, in this section CF-CNN segmented the tumor to contain only four voxels, which was only 10% of the size of the previous section (Section S2). In addition, in order to eliminate noise voxels such as isolated tiny regions (R1, blue region in Section S5) in the 3D segmentation process, we carried out the following connected domain selection operation: (a) when the noise appeared in the initial section, we selected the connected domain closest to the ROI center; (b) when the noise appeared in other sections, we selected the region with the center of mass closest to the center of mass of previous sections. For example, CF-CNN segmented two independent candidate regions R1 and R2 in Section S5. The distance between the centroid of these two regions and the centroid of the tumor in the previous section (S4) is d1 and d2, respectively. Since d2<d1, region R2 is retained and region R1 (noise) is removed. The ROI of the rectangle is first specified on the initial section and then applied in turn to the CT slice images above and below the initial slice. The number on the right of each section image is the number of voxel points contained in the segmented tumor region. The image in the left column shows the original CT slice, and the image
FIGURE 2.4
3D segmentation process.
2.2 Tumor segmentation 31


in the middle column shows the segmentation results of the CF-CNN model, where the red and blue areas represent the tumor and false-positive noise, respectively (false-positive noise marked in blue is eventually removed by the algorithm). The image of the right column is the 3D visualization of the final segmentation result. When training model, all pixels can be used as training samples. However, some pixels with lower difficulty in segmentation do not need to be selected too much, while pixels with higher difficulty in segmentation, such as those at the edge of nodules, need to be sampled massively to ensure that the model can better classify these pixels with higher difficulty in segmentation. This paper proposes a new weighted sampling method based on the difficulty of pixel segmentation. For all the pixels inside and outside the nodules, the paper proposed the following sample segmentation difficulty calculation formula:
PWi 1⁄4 expð minj ̨N dði; jÞÞ
Z (2.3)
NWi 1⁄4 Iiexpð minj ̨Pdði; jÞÞ
Z (2.4)
In this formula, PW and NW, respectively, represent the weight of the representation segmentation difficulty of nodule pixels and background pixels. D(I, j) represents the Euclidean distance between voxel point I and voxel point j. Set N and set P represent the background pixels and node pixels, respectively. Z is a normalized factor, ensuring that the weight sum of all pixel points is 1. After using this new sampling method, the sampling results are shown in Fig. 2.5 below. Using this method makes the dice of 495 cases of pulmonary nodules (82.15%) in the open dataset LIDC-IDRI, and 74 cases of pulmonary nodules (80.02%) in Guangdong Provincial People’s Hospital. The effect of partial segmentation results is shown in Fig. 2.6.
FIGURE 2.5
Rendering of the sampling method based on the difficulty of pixel point segmentation. Note: (A) input image, (B) positive sample sampling weight graph, (C) negative sample sampling weight graph, (D) random sampling results, (E) sampling results based on the difficulty of pixel segmentation.
32 CHAPTER 2 Key technologies and software platforms for radiomics


CF-CNN model is superior to the Graph Cut algorithm and level set algorithm in the LIDC dataset. In addition, CF-CNN also achieves the best results when tested on an independent GDGH dataset, and achieves good segmentation for a variety of different types of lung tumors. In order to prove the advantages of the central pooling layer proposed in this paper, researchers compared the CF-CNN and CF-CNN-MP models. Among them, CF-CNN-MP replaces the central pooling layer in CF-CNN with the traditional maximum pooling layer, and other parameters and network structure are consistent with the CF-CNN model. The central pooling layer proposed in this paper improves the average DSC value by about 2% on both datasets. In addition, the combination of 3D and 2D branches also improves model performance. By comparing the 3D branch, 2D branch, and CF-CNN, it can be found that the performance of CF-CNN on both datasets is better than the single 3D branch model or single 2D branch model.
2.2.2 Segmentation of brain tumor based on the convolutional neural network
Brain tumor segmentation is a label-intense task that requires a significant amount of time for manual tumor delineation. Thanks to the successful application of deep learning in image segmentation, researchers proposed a CNN based on pixel classification for automated brain tumor segmentation [5]. This model proposes a two-branch cascade network structure, in which one branch is used to process local information and the other branch is used to process global information. For each pixel, the model extracts small blocks of 33 33 size
FIGURE 2.6
CF-CNN renderings.
2.2 Tumor segmentation 33


and extracts local and global information with the following two-branch network. In the fine-grained branch, the 7 7 and 3 3 two-layer convolution layer and the maximum pooling layer are used for local information extraction, while the coarse-grained branch uses the convolution kernel of 13 13 for global information extraction. Finally, the feature graphs of the two branches are spliced together to fuse local and global information. After fusing the feature graph, the network uses the convolution layer of 21 21 to replace the full connected layer, which is used to fit the mapping between features and pixel labels. Meanwhile, the network parameters are reduced to prevent overfitting. In order to further prevent overfitting, the network uses L1 and L2 regularization, as shown below:
loss 1⁄4 log pðYjXÞ þ l1jWj1 þ l2kWk2 (2.5)
Finally, the model obtained the result of dice 1⁄4 0.88 in the brain glue tumor segmentation competition of MICCAI.
2.2.3 Fully convolutional networks
Long et al., from the University of California, Berkeley, proposed a structure named Fully Convolutional Networks (FCNs) in 2004, enabling neural networks to reach dense pixel-level classification without the fully connected layer, thus becoming a very popular CNN structure in pixel-level classification. As the fully connected layer is not required, the image can be semantically segmented much faster than the traditional method, whatever the size. Now, almost all of the advanced methods of semantic segmentation are developed based on this model. By applying convolution to the input image layer by layer and extracting different features which are operated through the activation function for highdimensional nonlinear fusion from multiconvolutional kernels, FCNs are capable of recognition of tumor contour so as to complete tumor segmentation. Each convolution layer of FCN uses fk 1⁄4 ReLUðWk x þbÞ to extract features. W is the learnable convolution kernel, represents convolution, and b is the bias of the model. The parameters of the convolution kernel W are obtained by automatic training of the network, which makes it possible to extract strong features. ReLU is a piecewise linear activation function. The transformation can perform nonlinear changes on convolution features, enhance the expression ability of features, and play the role of sparse features simultaneously. Finally, the features fk are extracted from the convolutional layer. Max-pooling layers are performed after convolutional layers in this research to promote robustness with reduced parameters of the model. After the last layer of pooling, FCN restored the segmented image to the original resolution by means of upper sampling and convolution layer stacking, so as to improve the accuracy in segmentation. The pooling layer is another critical point in CNN semantic segmentation in addition to fully connected layers. The pooling layer is competent to enlarge the
34 CHAPTER 2 Key technologies and software platforms for radiomics


receptive field and discard the information where to aggregate the context information. However, semantic segmentation requires exact alignment of the classifying diagrams, thus reserving position information. Two different classification architectures are described below for this problem. One is the encoderedecoder architecture. The encoder gradually reduces the spatial dimension through the pooling layer, while the decoder gradually recovers the details and spatial dimension of the object. There are usually shortcut connections between the encoder and decoder, which can better recover the details of the object. U-net is one of the most popular models in this kind of architecture. Another type is to use the dilated/atrous convolutions [6] structure to replace the pooling layer. Pooling can increase the receptive field and improve the performance of the classification network model. However, it is not the best semantic segmentation method as it reduces the resolution as well. Dilated convolution model uses dilated convolution as the convolution layer to achieve pixel-level prediction. Dilated convolution is able to increase the receptive field exponentially without reducing the spatial dimension.
2.2.4 Voxel segmentation algorithm based on MV-CNN
According to the characteristics of tumor CT image segmentation and voxel classification, researchers proposed the multiview convolutional neural networks (MVCNN) model, which utilizes multiview CT images and multiscale strategy to segment lung tumors. For each voxel of CT images, we extract the multiscale neighborhood image from three orthogonal perspectives of the image as input of the MV-CNN model, then predict whether the voxel belongs to the tumor or normal background tissue. Compared with the existing lung tumor segmentation methods, this model harbors the following advantages: (1) No needs to introduce any shape assumption or complicated parameter settings by users, which is easier to apply and is more robust; (2) In each perspective of the CT image, using a multiscale input image not only is the detailed texture information provided by the small-scale image but also the global shape information from large-scale image can be considered; and (3) Using three network branches to learn the information from three perspectives of the CT image, it’s feasible to observe the tumor from three orthogonal perspectives at the same time, which is more conducive to segmentation of tumors with adhesion to surrounding tissues. In addition, as data-driven and automatic learning to distinguish the features of lung tumor voxels and background voxels, the extracted features from this model fit lung tumor segmentation better than the artificially defined features. For each voxel in the CT image, researchers extract a multiscale 2D neighborhood image centered on it from three orthogonal image directions (axial, coronal, sagittal) as the input of three branches of MV-CNN, thus predicting whether the voxel belongs to the tumor or normal tissue.
2.2 Tumor segmentation 35


The three branches of the MV-CNN model process CT images from axial, coronal, and sagittal positions, respectively. The three branches own the same structure and are composed of six convolutional layers (C1 to C6), two max-pooling layers (max-pooling 1 and 2), and a fully connected layer (F7). Six convolutional layers of each branch are divided into three blocks, each block containing two 2D convolutional layers with a convolutional kernel size of 3 3. The convolutional layers perform convolution on the input to learn the mapping between the images and the voxel category labels. The operation process of the convolution layer is as follows:
fj1⁄4X
i
cij f i þ b j (2.6)
where f i and f j represent the i and j feature maps, respectively; cij represents the convolution kernel between f i and f j (* is a 2D convolution); bj is the bias of the convolution kernel cij. Feature maps refer to the output or input image of the convolution layers in CNN. Between each block, a max-pooling layer with the window of 2 2 and step of 2 is applied to reduce the dimensions of the feature image after convolution. After the last convolution layer, there is a fully connected layer F7, in which each neuron remains connected to all input neurons. The fully connected layer is capable of capturing the correlations between different features from convolution layers. In aims to realize the nonlinear transformation, the PReLU is applied as the nonlinear activation function after the fully connected layer F7 and each convolution layer. It is defined as follows:
PReLU f j 1⁄4
( f j if f j > 0
aj f j if f j 0
(2.7)
In this equation, aj is a trainable parameter, and j represents the j-th characteristic graphs in the convolution layers. For this experiment, it is initialized to 0.25. When the input is less than 0, the output of the PReLU function is controlled by parameter aj. This way of introducing a nonzero slope to the negative input is proved to be more effective than the traditional ReLU function in the classification tasks of ImageNet datasets. Previous research revealed that multiscale features can boost the classification performance of CNN. Therefore, the input of each CNN branch of MV-CNN is a two-channel multiscale neighborhood image. Here we use the multiscale input image to extract multiscale features, instead of building two CNN models to process two scale images, which can reduce the computation and network parameters. During the selection of multiscale neighborhood images, we set the target voxel as the center, and chose two rectangular images with a size of 65 65 and 45 45, respectively. Then, the cubic spline interpolation algorithm is applied to scale the two-scale images to 45 45, thus constituting the CNN branch corresponding to the two-channel image input. As the gray value
36 CHAPTER 2 Key technologies and software platforms for radiomics


distribution range of CT image varies, Z-score is applied to standardize the neighborhood images, and the calculation formula is as follows:
f ðxÞ 1⁄4 ðx xmeanÞ
xstd
(2.8)
where xmean and xstd represent the mean and standard deviation of neighborhood images, respectively. The multiscale strategy of MV-CNN on the input enables the model to learn multiscale features, while the CNN branches of three perspectives enable the model to learn 3D information of the tumor, which plays an important role in segmenting the tumor from adhesion to surrounding tissues. For example, it is difficult to recognize whether some voxels are tumor tissues only by axial images, but by coronal and sagittal images. At the end of the CNN model, the features extracted by these three branches are fused by the fully connected layer F8, hence predicting the class labels of the input voxel with two neurons in the output layer. The output layer of MV-CNN consists of two neurons whose output is transformed into the probability distribution of the two kinds of labels in the input image by the binary softmax function. Assuming that ok is the output of the kth neuron in the output layer, the probability that the input image belongs to the kth category is calculated by the softmax function:
pk 1⁄4 expðokÞ
P
h4f0;1gexpðohÞ (2.9)
where k 1⁄4 0 and k 1⁄4 1 represent background voxels and lung tumor voxels, respectively. After the MV-CNN network is constructed, it can be trained through the training set data to get good prediction performance. The goal training is to maximize the probability of correct classification, which can be achieved by minimizing the cross-entropy loss function of each training sample. Assuming that yn represents the real label (with a value of 0 or 1) of the nth input image, the cross-entropy loss function is defined as follows:
LðWÞ 1⁄4 1
N
N X
n1⁄41
ynlogybn þ ð1 ynÞlog 1 ybn þ ljWj (2.10)
where ybn is the predicting probability of MV-CNN and N is the number of samples. To avoid overfitting, we apply L1 norm to regularize the model parameter W. The intensity of regularization is controlled by l, which is set to 5 10 4. In the process of training, by calculating the gradient of loss function L on network parameter W, the loss function can be minimized gradually to promote performance. During the process, the model parameter W is initialized by the Xavier algorithm and updated by the SGD algorithm, as shown in the following formula:
Wtþ1 1⁄4 Wt þ Vtþ1 (2.11)
2.2 Tumor segmentation 37


Vtþ1 1⁄4 mVt aVLðWtÞ (2.12)
where T represents the number of iterations of training, and V represents the updated amount of network parameters in each iteration, with an initial value of 0. When calculating the gradient VLðWÞ, as it is difficult to store tens of thousands of training samples in memory simultaneously, this study applied 128 samples (batch size) for the batch operation. m is the learning momentum of the SGD algorithm, set to 0.9, and a is the learning rate. The initial learning rate is set as a0 1⁄4 6 10 5 at the beginning of training, which speeds up the learning process of the network and makes the network parameters approach the optimal solution quickly. After every four epochs of the training process, the learning rate is reduced by 10 times, since it needs the lower learning rate to find a more precise optimal solution to help the network converge when near the optimal solution. If the learning rate keeps getting larger, it may cause network learning process concussion leading to nonconvergence. As MV-CNN automatically learns CNN-based features from images in a datadriven way, many neighborhood images are needed as training samples for model training. In the chest CT image, the tumor area only accounts for a very small part of the whole lung tissue, and the number of voxels in the tumor area accounts for less than 5% of the total in a CT section. Therefore, choosing voxes randomly in the whole CT image of the lung as training samples will produce a large number of voxels (negative samples) from the background tissue, resulting in the imbalance of positive and negative samples. To solve this problem, researchers first determined the external rectangular ROI of each lung tumor in the training set when generating training samples, and expanded the ROI on each axis by eight voxels to get an extended ROI. Then, voxels are selected as training samples in the way of even sampling at intervals in the extended ROI, which accounts for one voxel being selected as training samples after skipping one voxel in the row and column directions. Hence, a quarter of the voxels in the ROI are selected as training samples during this process. Because the number of positive and negative samples in the training samples is prone to be inconsistent, in order to avoid the influence of unbalanced data distribution, we guarantee the same number of training samples in the two categories by random sampling. Finally, about 340,000 training samples were selected. For each voxel as a training sample, multiscale neighborhood images that are centered are extracted in axial, coronal, and sagittal positions, respectively, to train the MV-CNN network. At the end of each training stage, the MV-CNN model was tested on the verification set and evaluated by DSC. After 20 epochs of training, DSC on the verification set becomes stable. Therefore, the model stops training after 20 epochs of training. Finally, the performance of the model is evaluated on the independent test set. To compare the segmentation performance of the MV-CNN model, two widely used methods, including the level set method based on the active contour and graph cut algorithm, are implemented. The level set method and graph cut algorithm are
38 CHAPTER 2 Key technologies and software platforms for radiomics


implemented by Fiji software, and the parameters are optimized by the grid search. In the level set method, the first step is to apply fast marching to generate the initial lung tumor contour. Then, the active contour model is used to refine the segmentation results delicately, and parameters are set as the gray threshold value is 50, the distance threshold value is 0.1 when fast matching, while the advection value of the active contour model is set to 2.20, the curvature is set to 1.00, the grayscale tolerance is set to 30.00, and the convergence value is set to 0.005. Fiji software involves two parameters in the graph cutting method which are set as follows: Data prior (foreground offset) is 0.86, and edge weight (smoothness) is 0.56. MV-CNN, level set, and graph cut algorithm are all segmented and tested on 2D CT slices during the test. The quantitative results of MV-CNN and various other methods on the test set all have good performance as it’s easy to implement the segmentation relatively on isolated solid tumors (L1). However, when there is adhesion between the lung tumor and surrounding tissue, the segmentation performance of the level set method and graph cut algorithm decreased, revealing the two algorithms are incompetent in separating the lung tumor and surrounding tissue. For example, when there is adhesion between the lung tumor and lung wall (L2), the level set method and graph cut algorithm are prone to mistaking the lung wall for a lung tumor. When there is adhesion between the lung tumor and blood vessel (L3), the level set method and graph cut algorithm are also prone to dividing the blood vessel into the tumor. In contrast, the MV-CNN algorithm still maintains strong robustness in the segmentation of this kind of tumor, which shows that the MV-CNN has learned the critical features that are able to distinguish the lung tumor and adherent tissue. For the lung tumor with an internal cavity (L4), both the level set method and the graph cut algorithm mistakenly identity the cavity regions as background; however, MV-CNN correctly retains the cavity region. For lung tumors with internal calcification (L5), the level set method only segmented the calcified area, but mistakenly considered the noncalcified area as the background because of the large gray variance between calcified and noncalcified tissues. For a GGO lung tumor (L6), the tumor presented as a nonsolid state, with low contrast with the background. The level set method and graph cut algorithm tend to oversegment in this situation, prone to segmenting the nonsolid GGO tumor into many small regions, while MV-CNN maintains strong robustness.
2.3 Feature extraction
Feature extraction is a vital step in radiomics that captures the quantitative characteristics of the tumor. Therefore, there are numerous methods available for extracting rich tumor information with clinical value from medical imaging. Generally, the radiomics features can be divided into the features of the artificial design and the features of deep learning.
2.3 Feature extraction 39


2.3.1 The features of artificial design
The features of artificial design are basically classical algorithms that describe image information. These features include low-level visual features and highdimensional nonlinear features, which can reflect tumor information from multiple scales. The features of artificial design can be roughly divided into (1) intensity features; (2) shape features; (3) texture features; and (4) wavelet features, as shown in Fig. 2.7.
1. Intensity Features The intensity features are obtained by counting the gray value of the tumor image, generally including the first-order gray-scale statistics such as mean value, maximum and minimum value, kurtosis, skewness, entropy, etc. The intensity features reflect the contrast variance between the tumor and the surrounding tissue, and the heterogeneity of the internal tissue of the tumor. 2. Shape Features Shape features are generally applied to measure the shape, size, regularity, and other information of tumors. For example, the longest and shortest diameters and volume of tumor reflect the size information of the tumor. The ellipsoid degree of tumor reflects the shape information of the tumor, which contributes to quantifying whether the tumor tends to be spherical or
FIGURE 2.7
Schematic diagram of manually defined features.
40 CHAPTER 2 Key technologies and software platforms for radiomics


ellipsoid. The compactness of the tumor reflects whether the shape of the tumor has a regular edge, and other shape information. 3. Texture Features Texture features are different from shape and intensity features. Intensity and shape features reflect low-dimensional visual information, such as brightness, shape, uniformity, edge, and other image features that are easy to see. Texture features are used to quantify the texture pattern or tissue distribution inside the tumor and other information difficult to be perceived by vision. Texture features can be obtained by a variety of texture calculation matrices, such as gray-level run length matrix, which can be used to quantify gray-level run information in an image. The number of pixels with the same gray value in a certain direction is defined as a gray run. This means, in the gray run matrix Mði; jjqÞ, the times the element at position ði; jÞ repeats in the q direction. Gabor texture features are widely used to extract texture information of fixed patterns in some directions. Gabor filter, also known as Dennis Gabor, is a linear filter for edge detection. Gabor filter is capable of extracting multifrequency edge information in different directions. Since it contains different frequency edge information, Gabor texture shows excellent performance in most situations. 4. Wavelet Features The features of intensity, shape, and texture reflect the low-order visual information and high-dimensional texture information of the tumor, but the amount of information is limited. In order to extract tumor information in different frequency domains [7], wavelet transform is applied to decompose tumor images into different frequency domains and then extract tumor shape, intensity, texture, and other information. Wavelet transform is capable of decomposing the tumor image into four different frequency domain images: horizontal, vertical, diagonal, and low frequency. Wavelet features reflect the multiband and multiscale information of the tumor because of the different frequency domain images generated by wavelet decomposition. In many studies, the relationship between tumor images and clinical labels is difficult to be described by simple visual features. In this situation, wavelet features, a high-dimensional abstract feature, function in capturing features that are difficult to be perceived by vision, but contain clinical information. 5. Semantic features Semantic features could be defined as the qualitatively described empirical features proposed by radiologists. These features are beneficial for clinical settings of radiomics analysis, which cannot be related to an efficient mathematical expression directly.
2.3.2 Deep learning features
The features of artificial design have a clear calculation formula and strong interpretability but are difficult to design and lack the adaptability of different
2.3 Feature extraction 41


investigations. For example, the features of artificial design need the definition formula to design new features, which leads to inflexibility, dependence of expert knowledge, and weak scalability. In addition, for different clinical research, it is difficult to take different clinical labels into account. In the design of artificially designed features, research generally focuses on depicting tumor information from multiple perspectives, but designing features consistent with clinical labels or designing different features for different topics are unfeasible sometimes. In contrast, deep learning is competent to learn features automatically from data by virtue of its powerful feature self-learning ability, rather than relying on manual feature design. Deep learning represents a class of algorithms that use the stacked neural network structure. The deep learning feature extraction commonly includes two ways: (1) training CNN model, regarding the output of its fully connected layer as a deep learning feature; (2) training self-encoder with unlabeled data, regarding the output of the encoder as a deep learning feature. When there is label data to train the supervised learning model, researchers usually train a small CNN, applying the first method to extract the deep learning feature and collect the output of its fully connected layer as the deep learning feature. In the previous study [8], researchers constructed a three-layer CNN to predict EGFR gene mutations in lung cancer. After the CNN is trained, the author extracted 30 dimensional deep learning features of the fully connected layer and trained the SVM classifier separately. In the test set, the deep learning feature reached 0.668 area under curve (AUC). When the set of labeled data is extremely small to train the supervised CNN, the selfencoders help to learn the features of deep learning automatically by unsupervised learning. In the previous study [9], a typical model of self-encoders is constructed to extract deep learning features. The self-encoder training only needs the tumor image instead of the clinical labels of the tumor. Unlike supervised learning, self-encoders implement the encoder network to extract deep learning features; then, the decoder network is used to reconstruct tumor images from deep learning features for testing the amount of information in deep learning features. When the deep learning features reconstruct the original input images, the deep learning features contain most of the tumor image information already, therefore the deep learning features function for the construction of the model in clinical prediction. Compared with the features of artificial design, deep learning features are acquired from data by supervised or unsupervised learning. Therefore, these features are naturally related to the clinical labels or tumor images, which is better than the artificially defined features. In addition, deep learning features do not need to define the calculation formula of features manually. Therefore, some features are difficult to be formulated but are meaningful, and they can be obtained automatically through the self-learning ability of the deep learning network. On the other hand, the design of the deep learning network structure is flexible. Only changing the network structure or training mode will realize different deep learning features.
42 CHAPTER 2 Key technologies and software platforms for radiomics


2.4 Feature selection and dimension reduction
In order to describe the data more comprehensively, a large number of features will be generated from many aspects. However, too many features are not necessarily conducive to the analysis of the results. On the one hand, a large number of features need sufficient data to support training a robust and effective model; otherwise, the calculation of high complexity will lead to dimensional disaster. On the other hand, there will be interfered, redundant, and irrelevant features. Through feature selection and dimension reduction, we will reduce overfitting, promote the accuracy of the model, and reduce the training time of the model. In this section, we will introduce some widely used feature selection or dimension reduction methods in radiomics, and analyze their application scenarios.
2.4.1 Classical linear dimension reduction
Among machine learning algorithms, there are many basic linear dimension reduction methods including PCA, ICA, linear discriminant analysis (LDA), LFA, and LPP, but most of them are based on the projection of data, which makes data from the dimension reduction difficult to interpret. Now there are two widely used dimension reduction methods, PCA and LDA, which will be introduced in this section.
1. Principal Component Analysis PCA [10] is the most widely used linear dimension reduction method in machine learning with the main conception of the least square method, which is equivalent to the method of keeping the error of the least square method in high-dimensional space. The goal of PCA is to reduce dimensions with minimal loss. For example, it is equivalent to compressing points of a plane into a line. In order to minimize the loss, a line with the minimum distance from all points is the ideal model. This line is the result of compression, and the distance of each point is the error. This is shown in the Fig. 2.8. 2. Linear Discriminant Analysis LDA, also known as Fisher linear discriminant, is a common way of dimension reduction in machine learning. Unlike PCA, it needs to enter known labels, which means it needs supervised conditions. The concept is to find a low-dimensional space in a high-dimensional space, which can make the mean (center points) distance of two or more types of data be the farthest and the intraclasses variance of each type to be as small as possible. As shown in the figure Fig. 2.9, m represents the center of the data classes, while the size of the red and blue circles represents the size of variance. It can be seen that by projecting the data onto the diagonal on the left, the centers of red and blue datasets can be separated.
2.4.2 Dimension reduction method based on feature selection
There are classical feature dimension reduction methods of machine learning such as PCA and LDA, with projecting the original data and losing some data, which outputs
2.4 Feature selection and dimension reduction 43


FIGURE 2.8
PCA diagram. Note: The red dots show the original data, and the green dots are the corresponding data after dimension reduction.
FIGURE 2.9
LDA diagram. Note: Red and blue represent the distribution of the original two datasets, m represents the center of the data class, and the left slash is the projection target.
44 CHAPTER 2 Key technologies and software platforms for radiomics


features with noninterpretability and weak correspondence to the ones before dimension reduction. Therefore, these algorithms have to be bound with the classification models, hence limiting applications in radiomics. We will introduce the method of dimension reduction through the selection of original features, and evaluate the importance of each original feature in this section.
1. Feature Analysis Based On a Statistical Test
In common cases, evaluating each feature information by simple statistical or test methods is helpful, so as to obtain a certain understanding of each feature. For example, in the classification cases what we are most concerned about is which features are most effectively helpful to separate the two categories. In the regression cases, what we will be concerned about is which features are most relevant. Proper features selection for specific cases promotes the performance and helps to analyze better. For the regression cases (the target is a continuous value), correlation analysis, ANOVA, linear regression, and other methods help to evaluate the features and target value. Pearson correlation coefficient, the covariance of two sets divided by the standard deviation of two sets, is commonly applied in a correlation analysis. The absolute value of the correlation coefficient also expresses the correlation degree between the two features.
r 1⁄4 N P xiyi
P xi
P yi
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
N P xi2 ðP xiÞ2
q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
N P yi2 ðP yiÞ2
q (2.13)
where r is the correlation coefficient, xi represents the ith sample of dataset X, and N is the number of samples. Linear regression is similar to a variance analysis to measure the effectiveness of the features by the error of linear fitting between the features and the target values. For classification problems, LDA can be used, or t-test or chi-square test can be used to determine whether a significant difference exists between the two categories of data. The idea of LDA is similar to LDA, but without practical projection. The mean and variance of each category is a calculated analysis through the labels of different categories in a LDA. The degree of feature separation is evaluated by the variance of all kinds of means. T-test and chi-square test are used to test the degree of feature separation of the two categories.
t 1⁄4 X1 X2
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ðn1 1ÞS2
1 þ ðn2 1ÞS2
2
n1 þ n2 2
1
n1
þ1
n2
s (2.14)
The formula above shows the t-test formula, where X1 X2 represents the difference between the mean values, n represents the number of samples, and s represents the standard deviation.
2.4 Feature selection and dimension reduction 45


2.4.3 Feature selection based on the linear model and regularization
It is also mentioned above that the coefficient of the equation obtained by linear regression or classifier can judge the validity of the feature. The feature with too small of a coefficient has little effect on the overall equation. Removing such features can improve the robustness of the model and reduce the complexity of the model. However, this screening modus also requires some standards and methods. In this section, we introduce the linear model dimensional reduction through two regularization methods. Assume that the linear regression of normal use is as follows:
y 1⁄4 a0 þ X
aixi (2.15)
In the above formula, y is the target value, ai is the coefficient of the equation, and xi is the i-th feature. The commonly used linear regression or classification models use the least squares rule for iterative convergence. The objective function of each iteration is as follows:
J 1⁄4 y a0 þ n X
i1⁄41
aixi
!!2
(2.16)
where J is the objective function, n is the number of features, y is the true value of
the target, ða0 þ P aixiÞ represents the value obtained by the regression equation, and J is equivalent to the mean square error of the equation.
1. L1 norm regularization (LASSO) LASSO (least absolute shrinkage and selection operator) [11] method is an efficient feature selection algorithm, which is based on compression estimation. The LASSO method compresses some coefficients and sets some coefficients to zero by building a penalty function to get a more refined model. The LASSO method minimizes the sum of squares of residuals under the constraint, and the sum of the absolute values of the regression coefficients is less than a constant. Therefore, regression coefficients strictly equal to 0 is able to be generated, and an interpretable model can be obtained. LASSO regression is sometimes called L1 regularization of linear regression, that is, by adding the L1 norm in the objective function of a linear regression, which is the sum of the absolute values of the coefficients of the original equation.
J 1⁄4 y a0 þ Xn
i1⁄41
aixi
!!2
þ l Xn
i1⁄41
jaij (2.17)
l Pn
i1⁄41
jxij added in the equation is the L1 norm, and l is a parameter used to adjust the
proportion of the L1 norm. Due to the existence of the regular term, the constraint
46 CHAPTER 2 Key technologies and software platforms for radiomics


that minimizes nP
i1⁄41
jaij is also added, so that some ai are reduced to 0, and finally, the
purpose of dimension reduction is achieved. As shown in Fig. 2.10, when the LASSO program is called on in general, the corresponding effect is tested according to different l(Lambda), and the penalty coefficient that can effectively classify and effectively cancel the coefficient is selected. 2. L2 norm regularization (ridge regression) Ridge regression is similar to LASSO except that the L1 norm is replaced by the L2 norm, which is the sum of the squares of the coefficients. Since the L2 norm is a square term, the curve is smoother than L1, and there is no LASSO strict coefficient compression. The obtained coefficients are closer to zero but not zero, and the amount of information stored is greater.
J 1⁄4 y a0 þ Xn
i1⁄41
aixi
!!2
þ l Xn
i1⁄41
aij2 (2.18)
Fig. 2.11 is a graphical diagram of the L1 and L2 norms. It can be seen that L2 is much smoother than L1, which leads to more points to select when returning. The shape of the L1 norm is similar to a diamond, and the regression is more likely to approach the position of the four corners farthest from the center resulting in more characteristic coefficients being reduced to zero. So, L1 will be more effective than L2 for effect, while L2 will retain more detail. 3. Elastic net Elastic net regression analysis is an extension of LASSO regression analysis. Elastic net was proposed by Hui Zou in 2005. It overcomes the shortcomings of LASSO and ridge regression. The degree of compression coefficient is between LASSO and ridge regression. Although LASSO regression analysis can solve the feature selection problem when the feature number is much larger than the sample number, LASSO randomly selects one of the features and sets the others to zero when the correlation between multiple features is strong. In medical image processing, there is a strong correlation between the extracted features. In some case, LASSO regression analysis will eliminate much useful feature information, which is not conducive to the later construction of clinical auxiliary diagnostic models. Compared with the LASSO regression analysis, the L2 norm is the penalty term, which makes the coefficients between related features closer to each other and is not compressed to 0. But ridge regression analysis cannot select the feature effectively. In the elastic net regression analysis, the penalty term is a linear combination of the LASSO penalty term and the ridge penalty term, so that the elastic net regression analysis can solve the feature selection problem with strong correlation between features. The shape of the elastic net constraint domain is between a square and a circle. It is characterized by the trade-off between arbitrarily selecting an independent
2.4 Feature selection and dimension reduction 47


FIGURE 2.10
Recursive schematic of LASSO penalty factor coefficient [12]. Note: The left side is the change of the residual term with Lambda change, and
the right picture shows the change of the coefficient.
48 CHAPTER 2 Key technologies and software platforms for radiomics


variable or a set of independent variables. It shows the advantages of LASSO and ridge regression, and can realize variable selection. It can avoid the influence of a strong correlation between independent variables. LASSO, ridge, and elastic net is implemented in Matlab 2016a (MathWorks, Natick, MA, USA) via the Glmnet toolkit. 4. Feature selection using a tree model and min-redundancy and max-relevance The decision tree model and the min-redundancy and max-relevance (mRMR) method are different from the linear analysis. The feature selection is based on the change of the degree of chaos in the system. The existing features are sorted by the entropy information nonpurity evaluation index, and finally the characteristics of the model penetration are selected. (1) Tree model The tree model currently includes decision trees, random forests, gradient trees, etc. The evaluation indicators also include Gini factors, entropy gains, residuals, etc., but the basic idea is to expand the direction of the model improvement, and gradually make the model more refined and effective. This section mainly introduces common criteria information gain and information gain ratio. Entropy is a measure of disorder. In information theory and statistics, entropy represents the measure of the uncertainty of a random variable. Suppose X is a discrete random variable with a finite value, its probability distribution is as follows:
PðX 1⁄4 xiÞ 1⁄4 pi; i 1⁄4 1; 2; .; n (2.19)
Then, the entropy of the random variable X is defined as follows:
HðXÞ 1⁄4 n X
i1⁄41
pi log pi (2.20)
FIGURE 2.11
Norm diagram. Note: The left picture is the L1 norm, and the right picture is the L2 norm.
2.4 Feature selection and dimension reduction 49


Among them, it can be seen from the above formula that entropy only depends on the distribution of X, and has nothing to do with the value of X. The greater the entropy, the greater the uncertainty of the random variable. With a random variable (X, Y), its joint probability distribution is as follows:
P X 1⁄4 xi; Y 1⁄4 yj 1⁄4 pij; i 1⁄4 1; 2; .; n; j 1⁄4 1; 2; .; m (2.21)
The conditional entropy H(Y|X) represents the uncertainty of the random variable Y under the condition that the random variable X is known.
HðYjXÞ 1⁄4 n X
i1⁄41
piHðYjX 1⁄4 xiÞ (2.22)
The information gain is the importance of feature X, in which the information uncertainty of class Y is decreased. The information gain g(D, A) of A to the training data D is defined as the inconsistence between the empirical entropy H(D) of data D and the empirical conditional entropy H(D|A) of D under the given condition A:
gðD; AÞ 1⁄4 HðDÞ HðDjAÞ (2.23)
Features with large information gains have stronger classification capabilities. For the training dataset D, calculating the information gain of each feature, comparing their sizes, sorting all the features, and selecting the top features. When selecting features by information gain, it is preferred to select features with more values. Using an information gain ratio to correct this problem, the information gain ratio gRðD; AÞ of feature A to the training dataset D is defined as the ratio of the information gain g(D, A) to the entropy HAðDÞ of the training dataset D with respect to the value of feature A:
gRðD; AÞ 1⁄4 gðD; AÞ
HAðDÞ ; HAðDÞ 1⁄4 n X
i1⁄41
jDij
jDj log2
jDij
jDj (2.24)
As shown in Fig. 2.12, the higher the general entropy gain, the higher the point splitting number and the splitting depth, which indicates the degree of improvement of the classification effect after obtaining the information. (2) mRMR, min-redundancy and max-relevance The min-redundancy and max-relevance method is a filtering feature selection method. A common feature screening method is utilized to maximize the relevance between features and labels. That is, to select the top k variables with the highest relevance of the categorical variables. However, in feature selection, the integration of a single great feature depends on the high relevance of features, which may cause redundancy of the feature variables. So eventually, mRMR is employed to maximize the correlation between features and labels, and minimize the correlation between features and other features.
50 CHAPTER 2 Key technologies and software platforms for radiomics


The common measurement method in the mRMR method is mutual information:
Iðx; yÞ 1⁄4
ZZ
pðx; yÞlog pðx; yÞ
pðxÞpðyÞ dxdy (2.25)
This can be seen as a knowing y, and x is the loss in entropy, and p is the probability of events:
Iðx; yÞ 1⁄4 HðXÞ HðXjYÞ (2.26)
Then, the maximum correlation can be defined as follows:
maxDðS; cÞ; D 1⁄4 1
jSj
X
xi εS
Iðxi; cÞ (2.27)
where xi is the i-th feature, c is the categorical variable, and S is the feature subset. Min-redundancy is as follows:
maxRðSÞ; R 1⁄4 1
jSj2
X
xi ;xj εS
Iðxi; xjÞ (2.28)
When using mRMR, D and R are generally combined by addition, subtraction, or multiplication and division into an equation as an objective function to optimize and obtain valid feature ordering. 5. Recursive feature elimination, RFE The main idea of recursive feature elimination is to repeatedly build models (such as LR, SVM of linear models, random forests of tree models, gradient trees, etc.) to evaluate the features that have the worst (or best) performance
FIGURE 2.12
Schematic diagram of the tree model.
2.4 Feature selection and dimension reduction 51


improvement effect on the model. The resulting feature takes the dataset and repeats the process on the remaining features until all features are taken out. The order in which features are eliminated in this process is the ordering of features. Therefore, this is a greedy algorithm for finding the optimal subset of features. The specific algorithm is as follows: (1) Input the feature vector and target value. (2) Use the selected classifier to rank the states in which each feature is eliminated. (3) Take the feature with the lowest score. (4) Repeat steps 2 and 3 until the features are removed. (5) Determine the importance of a feature in the order in which it is taken out, the more backward the more important. The stability of the RFE depends to a large extent on which model is used at the bottom of the iteration, since it evaluates the features, especially the models used. It inherits the characteristics of the model selection features. For example, the regression coefficient of a linear model requires a penalty term or a different loss function to maintain the stability of the model, while the tree model needs to select a corresponding function for evaluation, etc.
2.5 Model building
After the completed feature selection and dimensionality reduction, a radiomics model needs to be established to describe the correlation between the feature and the predicted tag. This section focuses on the models commonly used in radiomics and the application of models. The models commonly used in radiomics include basic linear classification models, tree models, linear regression models, and deep learning models developed in recent years. Next, each type of model will be introduced.
2.5.1 Linear regression model
Linear regression models are generally used to fit a regression curve. Unlike a classifier, the objective function of a regression model can be a continuous value, not necessarily a categorical variable. Therefore, linear regression models are often applied to the prediction of continuous values, such as survival, risk index, etc.
1. Linear regression Linear regression is the most common regression model. Its formula is very simple. The relationship between each feature and the target value is obtained by multiple features. Finally, the offset is added.
y 1⁄4 a0 þ X
aixi (2.29)
52 CHAPTER 2 Key technologies and software platforms for radiomics


where y is the target value, xi is the ith feature, a0 is the offset, and ai is the coefficient of xi. There are usually two ways to solve linear regression, least squares fitting or iterative functionebased iteration. The least squares method refers to finding a set of parameters to minimize the square of the difference between the target value and the regression value. When it is iterated as the objective function, the function is as follows:
J 1⁄4 y a0 þ n X
i1⁄41
aixi
!!2
(2.30)
J is the objective function, n is the number of features, y is the target real value, and
ða0 þ P aixiÞ represents the value obtained by the regression equation. 2. Proportional hazards model (Cox) Cox regression analysis is a semiparametric analysis method for survival analysis. It proposes a concept of a proportional hazard model, which can effectively deal with the problem of multiple factors affecting survival time. The advantage is that it can ignore the distribution of survival time and effectively use censored data. Its formula is as follows:
hðtÞ 1⁄4 h0ðtÞe
P aixi # (2.31)
where ai is the regression coefficient and hðtÞ is the risk rate, which represents the instantaneous mortality of the patient who survived at time t and after time t. h0ðtÞ indicates the risk rate when xi is all zeroes. Due to the existence of the hidden variable h0ðtÞ, the formula is difficult to calculate. Therefore, when calculating the objective function of the Cox function, you need to go through the h0ðtÞ term. The objective function takes into account the ratio of the risk of a certain characteristic population at t time to the death risk of all people at time t, obtained by the maximum likelihood formula:
J 1⁄4 Yn
j1⁄41
2
4e
P aixi;j
P
kεRðtj Þe
P aixi;k
3
5
dj
(2.32)
where tj represents the jth time, xi;j represents the jth xi, and dj is whether the data are censored, that is, whether the follow-up is interrupted. By this formula, ai can be estimated without knowing h0ðtÞ. The Cox regression application mainly analyzes various factors in the aspect of survival analysis. The regression coefficient ai indicates the degree of danger of the factor, ai > 0 indicates that the covariate is a risk factor, and the larger it is, the shorter the survival time, and ai < 0 indicates that the covariate is a protective factor, and the larger it is, the longer the survival time. The standardized ai absolute value reflects the relative magnitude of this factor’s impact on lifetime. After finding the valid features, the Cox model is also used to evaluate the sample risk. The Fig. 2.13 shows the comparison of the survival curves of
2.5 Model building 53


the high and low risk groups by the median score of the Cox regression risk coefficient score. After training the parameters in the Cox proportional hazard model, the survival probability of the patient at the specified time point can also be calculated by the following formula:
SðtjXÞ 1⁄4 e
R
t
0
hðsjXÞds
1⁄4e
R
t
0
h0ðsÞebT X ds
1⁄4 e ðH0ðtÞÞebT X
1⁄4 S0ðtÞebT X (2.33)
where H0ðtÞ 1⁄4 R t
0 h0ðsÞds, S0ðtÞ 1⁄4 e H0ðtÞ. S0ðtÞ is the baseline survival function, and the baseline survival function can be estimated by the following formula:
S0ðtÞ 1⁄4 Y
ti <t
ni ri
ni (2.34)
FIGURE 2.13
Schematic diagram of the survival curve.
54 CHAPTER 2 Key technologies and software platforms for radiomics


where ni is the number of patients at risk of survival before time t; ri is the number of patients who occurred (Fig. 2.13) at time t. For a patient p, given its characteristic representation Xp, the probability of survival at time point tp can be expressed as 1-SðtpjXpÞ. For example, when calculating the patient’s 3-year survival probability, tp 1⁄4 3 year can be set, at which point the patient’s 3-year survival probability is 1 Sðt 1⁄4 3 yearjXÞ.
Common evaluation indicators for survival analysis are as follows:
(1) Harrell’s Concordance Index For survival data with Censored, the standard evaluation method does not apply. Harrell’s Concordance Index (C-index) is a commonly used evaluation method, and its calculation method is as follows: (1) Pair all samples in pairs to form a total of N (N 1)/2sample pairs. (2) Eliminate the pairing that cannot determine the order of the end events, and set the number of remaining pairs to be M. (3) Among the remaining M pairs, the number of pairs whose prediction result is consistent with the actual result is K. (4) C-index 1⁄4 K/M. C-index calls the rcorrcens function of the rms package (Frank E Harrell Jr 2018. rms: Regression Modeling Strategies) in R language (Version 3.5.1, R Core Team 2018. R Foundation for Statistical Computing, Vienna, Austria. www.R-project.org). Confidence interval (CI) is achieved by 1000 times of Bootstrap. (2) Time-Dependent ROC Survival analysis is different from the classification problem. Survival data include survival time and status. Therefore, the receiver operating characteristic curve (ROC) cannot be directly generated. It is necessary to determine a time point and draw ROC at this point in time, called time-dependent ROC. The Coef parameter value in Cox is the regression coefficient of the corresponding feature, through which the risk function can be calculated, and then each sample can calculate its predicted value of the survival probability at a certain point in time. The subsequent steps are the same as the ROC drawing method of the classification problem. Finally, the ROC was evaluated by AUC. The statistical difference between the two ROCs is calculated using the DeLong’s test. Time-dependent ROC calls the val.surv function of the rms package (Frank E Harrell Jr 2018. rms: Regression Modeling Strategies) and the survival ROC function of the survival ROC package (Patrick J. Heagerty and packaging by Paramita Saha-Chaudhuri 2013. survival ROC; Time-dependent ROC curve estimation from censored survival data) in the R language (Version 3.5.1, R Core Team 2018. R Foundation for Statistical Computing, Vienna, Austria. www.R-project.org); DeLong’s test is implemented by calling the roc.test function of the pROC package.
2.5 Model building 55


(3) KaplaneMeier The KaplaneMeier (K-M) survival curve is a one-factor nonparametric analysis method that estimates survival probability from observed survival time. Survival analysis studies, in addition to Cox for univariate or multivariate analysis, also used K-M survival curves to demonstrate the impact of single factors on prognosis. For the n-th time point tn in the study, calculate the survival probability. Log-rank test is widely used in survival analysis to compare two or more K-M survival curves, which is a nonparametric test, so there is no assumption about the distribution of survival probability; the null hypothesis states that there is no statistical difference in survival between the various curve groups. The comparison is the number of events observed in each group. The statistics are similar to the Chi-square test statistic. The K-M survival curve and log-rank test are implemented in the R language (Version 3.5.1, R Core Team 2018. R Foundation for Statistical Computing, Vienna, Austria. www.R-project.org) by calling the survfit function and the survdiff function of the survival package (Therneau, 2015) [23]. The K-M survival curve is drawn by calling the ggsurvplot function of the survminer package (Alboukadel Kassambara and Marcin Kosinski 2018. survminer: Drawing Survival Curves using ggplot2). (4) Calibration Curve The Calibration curve is implemented in the R language (Version 3.5.1, R Core Team 2018. R Foundation for Statistical Computing, Vienna, Austria. www.Rproject.org) by calling the val.surv function of the rms package (Frank E Harrell Jr 2018. rms: Regression Modeling Strategies) and the groupkm function. The HosmereLemeshow test is implemented by calling the hoslem.test function of the ResourceSelection package (Subhash R. Lele, Jonah L. Keim and Peter Solymos 2017. ResourceSelection: Resource Selection Probability Functions for Use-Availability Data). (5) Nomogram Nomogram was originally proposed by French engineer Philbert Ocagne in 1884, and it was originally used in engineering, which can describe the relationship between different variables. Since the nomogram can be used to calculate the survival of cancer patients, it is widely used in the medical field. For detailed mapping methods, see the paper published in the 2008 Journal of Clinical Oncology (JCO), How to build and interpret a nomogram for cancer prognosis (DOI: 10.1200/Jco.2007.12.9791). A typical nomogram can be divided into four areas: (1) score; (2) variable; (3) total score; and (4) prediction results. Its method of use is (1) The point at which the eigenvalues of the patient’s imaging ensembles are found on the target axis of the prediction result, and then the vertical line is drawn up to the coordinate axis of the score, that is, the score value of the radiomic feature is obtained. Repeat the process for each variable to get the score value for each variable.
56 CHAPTER 2 Key technologies and software platforms for radiomics


(2) Add the sum of the score values of the individual variables and find the point of the sum value on the total score axis. (3) Draw the vertical line down to the prediction result area to get the survival probability at the corresponding time point. Nomogram is implemented in the R language (Version 3.5.1, R Core Team 2018. R Foundation for Statistical Computing, Vienna, Austria. www.Rproject.org) by calling the nomogram function of the rms package (Frank E Harrell Jr 2018.rms: Regression Modeling Strategies).
2.5.2 Linear classification model
The linear classification model is basically the same as the linear regression, but the target value is generally 1/0 dichotomous or discrete (Fig. 2.14), rather than outputting continuous values like regression. A linear classifier is an algorithm that separates two types of objects by a line or a hyperplane. Commonly used are logistic regression, SVM, and a series of variants of them. Their formula base structure is similar to linear regression, and different classification effects are achieved by changing the objective function and the objective function form.
1. Logistic regression Logistic regression is a generalized linear model. It is more suitable for classification tasks than direct linear regression model logistic regression. It introduces the sigmoid function as an activation function to control the original output based on the linear regression model.
FIGURE 2.14
Schematic diagram of linear classification.
2.5 Model building 57


f ðxÞ 1⁄4 1
1 þ e x (2.35)
The above formula is a sigmoid function, the domain is in ( ∞, þ ∞), and the value range is between (0, 1). In the neural network, it is also often used as an activation function to constrain the output, so that the original continuous and infinite range of the output is limited to a fixed small range (Fig. 2.15). Consider a vector xiði 1⁄4 1; 2; .; nÞ with n independent variables, and let the conditional rate Pðy 1⁄4 1jxÞ 1⁄4 p be the probability that the observation will occur relative to an event x. Then, the logistic regression model can be expressed as follows:
Pðy 1⁄4 1jxÞ 1⁄4 1
1 þ e ða0þP aixiÞ (2.36)
where ða0 þ P aixiÞ is the same as the formula for linear regression, and the probability that the relative observation does not occur is as follows:
Pðy 1⁄4 0jxÞ 1⁄4 1 1
1 þ e ða0þP aixiÞ (2.37)
Compare the odds of experiencing an event, abbreviated as odds:
Pðy 1⁄4 1jxÞ
Pðy 1⁄4 0jxÞ 1⁄4 eða0þP aixiÞ (2.38)
A logarithmic version of the linear regression model is also obtained. From this equation, the maximum likelihood function of the logistic regression formula can be obtained. Let the observation value be pi 1⁄4 Pðyi 1⁄4 1jxiÞ, then the maximum likelihood function is as follows:
FIGURE 2.15
Schematic diagram of the sigmoid function.
58 CHAPTER 2 Key technologies and software platforms for radiomics


J1⁄4Y
i1⁄41
n
pyi
i ð1 piÞ1 yi (2.39)
2. SVM
SVM is a popular supervised learning-based method. It is a classification technology proposed by Vanpik’s AT&T Bell laboratory research team in 1963. It is a pattern recognition method based on statistical learning theory and shows many unique advantages in solving small sample, nonlinear and high-dimensional pattern recognition problems. Based on the structural risk minimization theory, the optimal hyperplane is constructed in the feature space, so that the learner is globally optimized, and the expectation of the entire sample space satisfies a certain upper bound with a certain probability. The difference between SVM and ordinary linear regression lies in the objective function. By using the hinge loss function, it uses only the sample (support vector) closest to the separation interface to evaluate the interface. The idea of SVM is to find a subinterface that perfectly separates the two categories and is equidistant from the two categories. This formula is also based on a linear regression model, and in order to facilitate the calculation, the linear regression is rewritten as
gðxÞ 1⁄4 wx þ b (2.40)
w denotes a coefficient matrix, x denotes a set of feature vectors, and b denotes a bias. Through the formula, the vertical direction w
jjwjj of the classifier can be obtained,
and the closest distance from the classification surface is as follows:
d 1⁄4 im ̨inn
jjwxi þ bjj
jjwjj (2.41)
Here we set jjwxi þbjj 1⁄4 1, so there is a distance between the sides of the m 1⁄4
2d 1⁄4 2
jjwjj, and get the following formula:
( wx þ b > 1 ; when yi 1⁄4 þ1
wx þ b < 1; when yi 1⁄4 1
(2.42)
In this way, we can calculate the target we need, minimize 1
2 w 2, and guarantee
yiðwx þbÞ > 1, thus establishing the objective function:
J1⁄41
2jjwjj2 n X
i1⁄41
aiðyiðwx þ bÞ 1Þ; ai > 0 (2.43)
Deriving the objective function to obtain two conditions for minimizing the function:
8>>>>>><
>>>>>>:
vJ
vW 1⁄4 Xn
i1⁄41
aiyix 1⁄4 0
vJ
vb 1⁄4 n X
i1⁄41
aiyi 1⁄4 0
(2.44)
2.5 Model building 59


Substituting the objective function to get the simplified objective function:
J 1⁄4 n X
i1⁄41
ai
1 2
n X
i1⁄40
n X
j1⁄40
aiajyiyjxT
i xj; ai > 0; n X
i1⁄41
aiyi 1⁄4 0 (2.45)
The classifier iterated according to this objective function is the SVM. As shown in the Fig. 2.16, the triangles and circles represent the support vectors of the two types of data. It can be seen from both the formula and the graph that the iterative process of the SVM only needs the data features of the support vector, and other data points basically do not directly contribute to the classifier, so the SVM can also effectively perform the classification task for small data volumes, and at the same time, it is also more susceptible to singular support vector effects resulting in overall offset. However, in practice, it is difficult to strictly guarantee the distance between the two sides, so there is an SVM using slack variables, and the formula changes as follows:
8>>>>>><
>>>>>>:
minimize 1
2jjwjj2 ; yiðwx þ bÞ > 1
Y
minimize 1
2jjwjj2 þ C
X z; yiðwx þ bÞ > 1 z; z > 0
(2.46)
FIGURE 2.16
Schematic diagram of SVM classification.
60 CHAPTER 2 Key technologies and software platforms for radiomics


In this way, the slackness of the SVM, that is, the degree of strictness to the two boundaries, can be controlled by any small slack variable z and parameter C. The larger the C, the higher the fit to the data, and the easier it is to overfit. Conversely, the smaller the C, the lower the degree of fit and the more robust it is.
2.5.3 Tree models
There are many tree models commonly used in machine learning tasks, including decision tree, random forests, gradient tree, and so on. The basic theories of them are similar. Based on information gain or residual, the target data are classified and distinguished step by step in detail to get the final classification result.
1. Decision tree Decision tree is a simple, efficient, and highly explanatory model, and in the field of data analysis, it is widely used. Its essence is a tree composed of many judgment nodes. The core of the algorithm is to choose the judgment node and to construct an appropriate decision tree through data learning. In general, we use GINI impurity or entropy gain on each node as we introduced in the previous section to determine whether a split is needed. The general process of the algorithm is as follows: (1) The input is m samples, the sample output is D, each sample has n discrete features, the feature set is X, and the output is decision tree T. (2) Initialize the threshold value of information gain. (3) Information gain of each feature in x is calculated to output D, and feature a with the largest information gain is selected. (4) If the information gain of a is less than threshold value, then return the single-node tree, and mark category as the category that outputs the first sample in sample D as the category with the most instances. (5) Repeat steps 2 and 3 until the information gain threshold cannot be reached or there are no other features. The advantages of the decision tree lie in its fast speed, relatively small computation, easy conversion to rules, easy understanding, and clear display of which attributes are more important. 2. Random forests and gradient tree Both random forest and gradient tree can get more comprehensive results by combining multiple tree models. Random way is used to establish random forest. Many decision trees make up the forest and they have no correlation; that is, different feature ranges are selected. After obtaining the random forest, when a new sample is input, judgment is made by each decision tree in the random forest. According to the decision made by each decision tree, the final prediction result is obtained through voting or other combined ways. Gradient tree boosting is a combination algorithm, also called a gradient boosting regression tree. The basic classifier of gradient tree boosting is a
2.5 Model building 61


decision tree, which can be used both in regression and classification. It typically consists of multiple regression trees, the core of which is that each tree learns from the previous residual. Regression tree is similar to a decision tree; besides the classification target value is changed from dispersed to continuous, and the evaluation method is changed from entropy gain to a residual of the regression function. It is equivalent to having done the first regression tree, and finding the direction with the greatest residual to build the next tree. Unlike the parallel structure of a random forest, the gradient tree is equivalent to tandem learning, which reduces the residual step by step and finally combines the result by weight. According to the results, random forest improves performance by reducing model variance, while the gradient tree reduces model deviation.
2.5.4 AdaBoost
AdaBoost algorithm is a lifting method, which integrates several weak classifiers by bagging and combining them into strong classifiers. AdaBoost is the abbreviation of Adaptive Boosting (Adaptive enhancement), proposed by Yoav Freund and Robert Schapire [22]. Boosting integrated classifier contains many very simple member classifiers. These member classifiers perform better in random guesses and are often referred to as weak learning machines. The typical example of weak learning machines is a single-layer decision tree. The Boosting algorithm mainly aims at samples difficult to distinguish, and the weak learning machines improve classification property of classifiers by learning on the samples with incorrect classification. Boosting differs from bagging in that the initial stage of boosting uses a nonreturn sampling to randomly extract a subset from the training samples, while bagging uses a retrieving extraction. The boosting process consists of four steps:
1. A training subset d1 is randomly selected from training set D by the no-return sampling method for the training of the weak learning machine C1. 2. A training subset d2 is randomly selected from training set D by the no-return sampling method, is added to the 50% incorrect samples of c1, and then trained to get the weak learning machine C2. 3. Training samples D3 with inconsistent C1 and C2 classification results are extracted from training set D to generate training sample set d3, and d3 is used to train the third weak learning machine C3. 4. Combine weak learning machines C1, C2, and C3 by majority vote.
The AdaBoost algorithm has been improved on the Boosting algorithm. It uses the entire training set to train the weak learning machine. The training samples are reassigned a weight in each iteration, and build a more powerful classifier based on the previous weak learning machine error. Its adaptation is to strengthen the weight of the sample of the previous weak classifier (the weight corresponding to the sample), and the updated sample of the weight is used again to train the next new weak classifier. The new weak classifier is trained with the population (sample population)
62 CHAPTER 2 Key technologies and software platforms for radiomics


in each round of training, generating new sample weights and weak classifier weights. Until the predetermined error rate is reached or the specified maximum number of iterations is reached, the iteration stops. Fig. 2.17 explains the working process of the AdaBoost algorithm: The training sample consists of two different species. All the samples in Fig. 2.17 are given the same weight. Through the training of the training set, we can obtain a single-layer decision tree (dashed line in the figure), which divides two different types of samples by minimizing the cost function (sample impurity), in which two blue balls and one red ball are incorrectly divided. In the second training process, two mistakenly divided blue balls and one misclassified red are given greater weight (the ball becomes larger), while also reducing the weight of correctly segmented samples. In the course of this training, we will focus more on the sample with the right weight, that is, the sample that divides the error. By repeating this process, the sample is correctly divided. Then, a combination of weak learning machines is obtained, and through majority voting, the final prediction result is determined. The specific calculation process is as follows:
FIGURE 2.17
Adaboost classification diagram.
2.5 Model building 63


1. Initialize the weight u of the sample with the same initial value, and the sum of the sample weights is 1.
n X
i1⁄41
ui 1⁄4 1 (2.47)
2. In the j-th round boosting operation, train a weighted weak classifier:
Cð jÞ 1⁄4 trainðX; y; uÞ (2.48)
3. Forecast sample:
pred 1⁄4 predictðCð jÞ; XÞ (2.49)
4. Calculate the weight error rate:
ε 1⁄4 u ðpred 1⁄41⁄4 yÞ (2.50)
5. Calculate the correlation coefficient:
aj 1⁄4 0:5 log 1 ε
ε (2.51)
6. Update sample weights:
u 1⁄4 u eð aj pred yÞ (2.52)
7. Normalize sample weights to ensure that the sum of all sample weights is 1:
u1⁄4 u
Xn
i1⁄41ui (2.53)
8. Complete the final forecast:
pred 1⁄4
0
@ Xm
j1⁄41
ðaj predðCð jÞ; XÞÞ
1
A (2.54)
2.5.5 Model selection
The above introduces the more commonly used machine learning classifiers. Each type of classifier has its own applicable range and the appropriate data format. It
64 CHAPTER 2 Key technologies and software platforms for radiomics


is necessary to construct different forms of data structures for each data and application scenario in order to effectively play the effect of the classifier. For example, the linear model is derived from a classification surface whose weight is obtained by weighting multiple features. Each feature is used as the basis vector for constructing the classification coordinate system, so nonorthogonal features and complex features are less effective. That is to say, the linear model is not good for the correlation between features that are high or the variables that are controlled by multiple variables. It is more suitable for the case where each feature and objective function value are linearly related, and it is difficult to distinguish multiple related or unrelated features. Generally, the influence of these features on the model is reduced by means of deleting features and dimensionality reduction, or these features can be further processed by feature engineering, and the feature is decomposed by label coding by OneHot Encoding. Otherwise, the new feature is combined by dot product and multiplication to explore whether multiple factors work together. Overfitting problems can also be mitigated by different levels of feature grading, segmentation, and different degrees of data differentiation. Through the understanding of the data, it should be judged whether the feature should be analyzed differently as tag data or continuous data. In addition to the improvement of features and models, it also can change the learning algorithm to avoid model bias caused by extreme eigenvalues by replacing the original iterative learning with a random gradient descent method. For example, use the method of random gradient descent instead of the original iterative learning. Compared to linear models, the tree model is less sensitive to anomalies (outside the gradient tree) and can be better handled against the label features, but is less sensitive to the relative size information in the continuous features. Therefore, the tree model is also difficult to control the degree of fitting, and it is difficult to separate the complex features. It is more suitable for the feature that can effectively separate the feature from the target value through the truncation point. It is also usually through the screening and dimension reduction of features to improve the performance of the model.
2.5.6 Convolutional neural network
The CNN is a feed-forward neural network whose artificial neurons can respond to edges of the image and have great performance for large image processing. Neurons of it include learnable weights and biases. Each neuron receives some input and performs a dot product calculation, and the output is the score for each category. CNN usually contains the following layers:
1. Convolutional layer The statistical properties of a part of an image are the same as other parts, which is the inherent characteristics of natural images. This also means that the features we learn in this part can also be used in another part, so the same learning characteristics for all positions on this image can be used. A more appropriate
2.5 Model building 65


explanation is that when we randomly select a small block from a large-sized image, such as 8 8 as a sample, some features are learned from this small sample. The features learned in the process are applied as detectors to any part of the image. Furthermore, the features learned from the 8 8 samples can be used to convolve with the original large-size image, then we can obtain an activation value for a different feature for any position on this large-size image. Each convolutional layer in a CNN consists of several convolutional units, and the parameters of each convolutional unit are optimized by a back propagation algorithm. Different types of features of the input can be extracted from the convolution operation. The first layer of the convolutional layer may only extract some low-level features such as edges, lines, and corners. Deep layers of networks can extract more complex features from low-level features. The following formula explains the detailed calculation process for convolution operations:
Given input image matrix I 1⁄4
0
B@
I11 I12 I13
I21 I22 I23
I31 I32 I33
1
CA and convolution kernel
1⁄4 k11 k12
k21 k22
. Convolutional layer output F 1⁄4 conv(I, K), where conv is the
convolution operation. The convolution operation can be expressed by the following formula:
F 1⁄4 convðI; KÞ
1⁄4 I11 k11 þ I12 k12 þ I21 k21 þ I22 k22 I12 k11 þ I13 k12 þ I22 k21 þ I23 k22
I21 k11 þ I22 k12 þ I31 k21 þ I32 k22 I22 k11 þ I23 k12 þ I32 k21 þ I33 k22 (2.55)
A specific example is given below: Suppose you have learned the characteristics of an 8 8 sample from a 96 96 image, and this is done by self-encoding with 100 hidden units. The convolutional feature was obtained through a convolution operation, which is required for each 8 8 small block image region of the 96 96 image. That is to say, 8 8 small block areas are extracted, and are marked as (1, 1), (1, 2), . from the starting coordinates, up to (89, 89), and then to the extracted area. The trained sparse self-encoding is run one by one to obtain the activation value of the feature. In this example, it is clear that you can get 100 sets, each containing 89 89 convolution features. Assuming a large-size image of r c is given, it is defined as xlarge. By training sparse self-encoding from a small a b image sample xsmall extracted from a
large-size image, calculate f 1⁄4 s Wð1Þxsmall þbð1Þ (s is a sigmoid). The type function obtains k features, where W(1) and b(1) are the weights and deviations between the visible layer unit and the implicit unit. For each small image xs of
a b size, calculate the corresponding value fs 1⁄4 s Wð1Þxs þbð1Þ , and convolve these fconvolved values to obtain a k ðr a þ1Þ ðc b þ1Þ matrix of features after convolution.
66 CHAPTER 2 Key technologies and software platforms for radiomics


2. Rectified linear units layer The activation function of this layer of nerves adjusts the output of each layer where
ReLUðxÞ 1⁄4 maxð0; xÞ (2.56)
The nonlinear function performs output value transformation, and the feature extracted by the convolutional layer can be transformed into a high-dimensional nonlinear space; the activation function of ReLU can suppress the output to a negative value, achieve the purpose of feature sparseness, and prevent overfitting. In addition to this, there is also a sigmoid function as an activation function:
f ðxÞ 1⁄4 1
1 þ expð xÞ (2.57)
It can be seen that a logical regression was actually used as the inputeoutput mapping relationship of the single “neuron.” Also, the hyperbolic tangent function (tanh)
f ðxÞ 1⁄4 tanhðxÞ 1⁄4 ex e x
ex þ e x (2.58)
The tanh function is a variant of the sigmoid function, which has a range of [ 1, 1] instead of [0, 1] of the sigmoid function. 3. Pooling layer Usually, after the convolutional layer, features with large dimension are obtained. Cutting the features into several regions and taking the maximum value or the average value of them can greatly reduce the obtained features, and the feature complexity is reduced. After obtaining the features by convolution, in the next step we hope to use these features to do the classification. In theory, all of the extracted features can be used to train classifiers, such as the softmax classifier, but this is a computational challenge. For instance, for a 96 96 pixel image, suppose 400 features defined on the 8 8 input have been learned, and each feature and image convolution will get one (96 8 þ 1) (96 8 þ 1) 1⁄4 7921 dimensional convolution feature, and with 400 features, each sample will get a 7921 400 1⁄4 3,168,400dimensional convolution feature vector. Learning a classifier with more than three million feature inputs is inconvenient and prone to overfitting. In order to solve this problem, first recall that the reason we decided to use the convolutional feature because the image has a “static” property, which means that features useful in one image area are most likely to be equally applicable in another. Therefore, we can aggregate statistics on the features at different locations to describe large images. For example, the average (or maximum) of a particular feature over a region of an image can be calculated. These summary statistical features not only have a much lower dimension (compared to using all extracted features) but also improve the results (not easy to do overfitting). This
2.5 Model building 67


kind of aggregate operation is called pooling, sometimes called average pooling or maximum pooling (depending on the method). If a continuous range in the image is selected as the pooled area and only the features produced by the same (repetitive) hidden unit are pooled, then these pooling units have a translation invariant. This means that even after the image has undergone a small shift, the same (pooled) features will still be generated. In many tasks (such as object detection, voice recognition), we all want to get a feature with translation invariance, because the sample (image) mark obtained in this way will not be affected by image translation. Formally, after obtaining the convolutional features we discussed earlier, we need to determine the size of the pooled area (assuming a b) to pool our convolution features. Then, we divide the convolution feature into a number of disjointed regions of size a b, and then use the average (or maximum) features of these regions to obtain the pooled convolution features. These pooled features can be used for classification.
Such as assumed feature map F 1⁄4
0
BBB@
15 28
39 78
10 26
85 32
1
CCCA, the size is 4 4, given a
window size of 2 2, and the maximum value of the pooling operator is 2, the pooling operation splits F into four adjacent 2 2 small matrices. The maximum value of each of the small matrices will be taken out as the result of the pooling
operation P 1⁄4 9 8
8 6. 4. Fully connected layer All local features are combined into global features. Integrate features in a manner similar to logistic regression, and finally complete classification. 5. Batch normalization layer Since the gradient of the deep network is gradually weakened during training, the batch normalization layer performs batch normalization of features after each layer of convolution, and normalizes the eigenvalues of each layer to 0 mean 1 standard deviation. It ensures that the features and gradients in the training process can be transmitted to the output and input of the network as much as possible, thus speeding up the training process of the network. 6. Output layer The softmax function or the sigmoid function is usually used in the output layer. In fact, the softmax regression model is a generalization of the logistic regression model on multiclassification problems. In the multiclass problem, the class label y can take more than two values.
Our training set consists of n labeled samples in logistic regression: n
xð1Þ,yð1Þ , x2,yð2Þ ,/ xðnÞ,yðnÞ o
68 CHAPTER 2 Key technologies and software platforms for radiomics


Since the logistic regression is for the two-class problem, the class is labeled yðiÞ ̨f0; 1g. The hypothesis function is as follows:
h0ðxÞ 1⁄4 1
1 þ expð qT xÞ (2.59)
We will train the model parameter q to minimize the cost function:
JðqÞ 1⁄4 1
M
" Xm
i1⁄41
yðiÞ log h0 xðiÞ þ 1 yðiÞ log 1 h0 xðiÞ
#
(2.60)
In softmax regression, we solve the multiclass problem (relative to the two-class problem solved by logistic regression); the class Y can take k different values (k > 2). Therefore, for the training set xð1Þ,yð1Þ , x2,yð2Þ ,/ ðxðnÞ,yðnÞ , we have yðiÞ ̨f1; 2; /; kg (Note that the category subscript here starts at 1 instead of 0). Such as in the MNIST digital recognition task, the number of categories is 10.
For a given test input x, we want to estimate the probability value P(y 1⁄4 j|x) for each category j using a hypothesis function. In other words, estimate the probability of the occurrence of each classification result of x. Therefore, a k-dimensional vector (the sum of the vector elements is 1) will be output through our hypothesis function to represent the probability values of the k estimates. As the number of categories decreases, softmax regression degenerates into a logistic regression. This suggests that softmax regression is a general form of logistic regression. The CNN is equivalent to treating each pixel in the image and the points around it as a whole by convolution, so that small images can be learned as features instead of learning from each pixel. More attention is paid to the related issues between different points in the image. With this network structure, it can approximate valid features based on the position of the data and the label oriented from the convolutional block in the picture. This allows the classifier to learn the features and methods from the picture itself, rather than providing features to the classifier by humans. On the one hand, it breaks through the limitations of human design features, and on the other hand, it completes an end-to-end device that can automatically study the problem. Why do we want to use deep networks? The main advantage of using a deep network is that it can express a much larger set of functions in a more compact and concise way. Or, we can find some functions that can be expressed succinctly using the k-layer network (the simplicity here means that the number of hidden layer units only needs to be polynomial with the number of input units). But for a network with only the k-1 layer, these functions cannot be expressed succinctly unless it uses the number of hidden layer units exponentially related to the number of input units. As a simple example, we intend to build a Boolean network to calculate the parity of n input bits (or XOR). It is assumed that each node in the network can perform
2.5 Model building 69


a logical OR operation (or a NAND operation) or a logical AND operation. If we have a network consisting of only one input layer, one hidden layer, and one output layer, then the number of nodes required for the parity function is exponential with the size n of the input layer. However, if we build a deeper network, then the scale of this network can be just a polynomial function of n. When the object is an image, we can use the deep network to learn the “partialtotal” decomposition relationship. For example, the first layer can learn how to combine pixels in an image to detect edges (as we did in the previous exercise). The second layer can combine edges to detect longer contours or simple “target parts.” At a deeper level, these contours can be further combined to detect more complex features. The last point to mention is that the cerebral cortex is also calculated in multiple layers. For example, the human brain processes visual images in multiple stages, starting with the “V1” region of the cerebral cortex, followed by the “V2” region of the cerebral cortex, and so on. CNNs have also shown good performance in the study of medical images. For example, Multiscale Convolutional Neural Networks (MCNN) for Lung Nodule Classification is very effective in the identification of benign and malignant lung cancer from unsegmented lung CT [13]. The method first extracts multiple nodule patches to capture nodule variations from the input CT images. When calculating the discriminant features, the resulting valid patch is then sent to the network. In this paper, MCNN is used to describe the feature characteristics from different scales. The experimental results show that the image features obtained by MCNN are much better than the commonly used HOG and LBP features. On this basis, the author also proposed an improved scheme of MCNN [14], which proposed multicrop pooling instead of the original pooling layer, thus effectively characterizing sensitive nodes. It can go out and make multiple choices for the center feature. Based on the maximum pooling layer feature of the original images, the maximum pooling layer feature of the image obtained by two center clippings are added, in which the feature proportion of the central part increases, and a more detailed feature description is obtained. Through this network, the benign and malignant discrimination effect of lung cancer can be further improved. It can be seen from these applications that CNNs have great advantages in the processing of medical images and have great potential for improving clinical outcome prediction. With the further development, this networks design will be more widely used to lead end-to-end medical image diagnosis. Since deep learning is an end-to-end prediction method, the prediction process is not intuitive to the common user. A visualization method is highly valuable to analyze the features extracted by the deep learning model to further understand the prediction process. The convolutional layer is the most important component of the deep learning model. Therefore, the convolutional layer can be visualized from two aspects to understand the inference process: (1) visualize the feature patterns extracted by the convolutional layer; (2) visualize the response of each convolutional layer to different tumors.
70 CHAPTER 2 Key technologies and software platforms for radiomics


Each convolutional layer in the deep learning model consists of a number of convolution filters (convolution kernels), and each of which can extract different features. Through the filter visualization algorithm, the researchers can visualize the feature patterns extracted by the convolution filter. When visualizing a convolution filter, input a random white noise image to the deep learning model and observe the response of the filter. If the filter’s response reaches a maximum value, the input image shows the feature pattern extracted by this filter; otherwise, the back propagation algorithm is used to change the input image. This process is similar to the training process of the deep learning model and will be iteratively iterated until the observed filter response reaches a maximum value. At this time, the input image is the deep learning feature pattern learned by the filter. Through this filter visualization method, the feature patterns extracted by each convolution filter in the deep learning model can be observed, and it is intuitive to understand what features each convolution layer is extracting. Researchers can also observe the response of each convolution filter to different tumor images. For example, given a CT image of a tumor, each convolution filter in the deep learning model can generate a corresponding response map representing the distribution of corresponding feature patterns in the tumor. Define the average of the response map as the response value, where the deep learning feature is produced by the filter. A discriminative convolution filter should have different response values between classification tasks or regression tasks, such as EGFR mutations and EGFR wild-type tumors. Therefore, visualizing the convolution filter’s response to different tumors can help researchers evaluate the performance of convolution filters.
2.5.7 Migration learning
Migration learning (also known as transfer learning) refers to the use of a trained model in one domain to migrate it to another domain for use. When building deep learning models, we often need a lot of data to train a deep CNN to avoid overfitting. However, in medical imagingerelated topics such as radiomics analysis, the amount of data is usually small. Small sample data are difficult to train complex deep learning networks, which is mainly due to the fact that deep learning networks are prone to overfitting in small sample cases. From the perspective of network training, overfitting occurs because the network is stuck in a local extreme point when optimizing the solution. During the training of the deep learning network, the parameters in the network are usually randomly initialized, then the network is trained using the training data. This training process is actually the process of optimizing the parameters of the network. When the network uses random initialization, the solution process starts from a random solution and gradually approximates the approximate optimal solution in the iterative process. However, when network gets deeper or the structure becomes more complex, the parametric surface becomes more rugged, and it is easy to fall into a local extreme point away from the optimal solution during the solution process.
2.5 Model building 71


Therefore, to solve the problem that the network is easy to fall into a bad local extreme in a small sample case, the network training process can be improved: if a better initial solution can be given to the network, the small sample data may also cause the network to converge at a better approximate optimal solution. Migration learning is applied to small sample data by changing the initial state of the network as described above. In migration learning, there are two datasets, one for the initial dataset for pretraining the CNN network and the other for the target dataset for the current problem. In general, the initial dataset has a larger amount of data, while the target dataset has a smaller amount of data. Migration learning first uses a large number of samples on the initial dataset to train the CNN network; at this time, the CNN network has learned meaningful features, and its parameter surface will become smoother. Then, use the parameters of the trained CNN network as the initial state, and perform a refitting on the target dataset (fine tune) to obtain a new CNN model. In order to make the CNN have a better initial state, the initial dataset used for migration learning is generally a natural image dataset with a very large amount of data, such as the ImageNet dataset, which has more than 1 million images that can be used to train the CNN network. Although there are large differences between the initial dataset (such as ImageNet natural image dataset) and the target dataset (medical image dataset), many features of CNN can be shared across domains. The characteristics of CNN learning are hierarchical, and the underlying features are generally image features, such as edges, shapes, etc. These features are versatile and applicable in medical images; CNN’s high-level features are related to datasets, so you can use only the shallow network of CNN for migration during migration learning. According to the different similarities between the initial dataset and the target dataset, migration learning has different ways to refit the CNN network as follows:
(1) Since the shallow features of CNN are general image features such as edges and shapes, they are versatile in different datasets, so the first few parameters of the pretrained network can be fixed during refitting, only training the last layers of the network, which are used to learn the features associated with the target dataset. (2) In order to make the entire network fully adaptable to the target dataset, the entire network can be refitted to train all layers. (3) The last layers of the pretrained CNN are replaced with newly designed randomly initialized layers and then refitted on the target dataset for the newly added layer or the entire network.
In the natural image, the ImageNet dataset has more than 1.28 million training sets, and the smaller cifar10 dataset has more than 50,000 images. Therefore, the subject of natural image processing can construct deep and complex deep learning models to get better performance. However, medical images, especially 3D tumor
72 CHAPTER 2 Key technologies and software platforms for radiomics


images (CT, MRI), have a small amount of data, usually less than 1000, which makes it difficult to directly train large depth learning models, such as CT image datasets for lung nodule segmentation. LIDC-IDRI has only 1010 patients; the MICCAI dataset for brain tumor segmentation has MR data for less than 100 patients. For the problem of having a small amount of medical imaging data, migration learning is a good way to deal with it. In the multiple tasks of diabetic retinopathy, retinal fundus disease diagnosis, and skin cancer diagnosis, migration learning has shown excellent performance [15e17]. In the study of EGFR gene mutation prediction in lung cancer, some researchers constructed a new deep learning network using migration learning, and the training data based on small samples achieved good prediction performance. In natural image processing, DenseNet uses dense connections to improve the problem of gradient dispersion during network training. By introducing a layer jump connection, the gradient of the network output is directly introduced to the input, so that the input layer of the network is well trained [18]. Unlike the “radiomics” analysis method, which requires artificially defined features, deep learning is an end-to-end model whose input is the CT image of the tumor, and the output is the probability that the tumor has a mutation in the EGFR gene. First, the user selects the tumor area using a rectangular frame in each slice of the CT image, that is, it selects the ROI of the tumor. The ROI image is then scaled to a 64 64 voxel size using a cubic spline interpolation algorithm. Finally, the ROI images in adjacent three-layer CT slices are combined into a three-channel image and imported into a deep learning model to predict EGFR gene mutation. Since the CT image of a tumor contains many slice images, researchers input all CT slices of the tumor into a deep learning model for prediction, and the average of the predicted probabilities of all of the slices is taken as the mutation probability of the tumor’s EGFR gene. Drawing on the advantages of DenseNet, researchers used the migration learning method to design a new CNN based on DenseNet [19]. The DL model is based on DenseNet’s network structure. In this model, two convolutional layers and two batch normalization layers make up a stack, which is defined as a group. Specifically, a group is composed of a batch normalization layer, a 1 1 convolution layer, a bulk normalization layer, and a 3 3 convolution layer stacked in order. The first six groups are stacked into the first block. In the first block, the input of each group is connected to the output of all of the groups in front of it, that is, the dense connection (Dense Connection). After the first block, the features are parameter compressed using a bulk normalization layer and a 3 3 convolution layer. For example, the first 256 feature maps are compressed to 128 feature maps using a 128 channel 3 3 convolutional layer. Subsequently, the feature map is reduced to an 8 8 size using a 2 2 average pooling layer. Then, five groups are stacked into a second block, and dense connections are also used within the block. The last convolutional layer of the second block uses the bulk normalization layer to eliminate the mean and variance drift of the feature map, and then uses global mean pooling to change all feature maps from 8 8 to one value. Finally,
2.5 Model building 73


a 352-dimensional feature vector is generated. Next, the 352-dimensional feature vector is connected to the output neuron through the fully connected layer for EGFR gene mutation probability prediction. The network includes subnetwork 1 and 2, wherein subnetwork 1 migrates to the top 20 layers of DenseNet, and its topology and network weight are consistent with the top 20 layers of DenseNet; subnetwork 2 is a redesigned four-layer convolution layer. During network training, subnetwork 1 is first fixed and only subnetwork 2 is trained to ensure that the weights already trained in subnetwork 1 are not disturbed. After subnetwork 2 has been fully trained, the weights of the entire network will be trained together; at this time, the migration learning weight of subnetwork 1 will also be updated, making the entire network more suitable for the current application problem. In order to expand the training samples, each layer of 2D slices in the CT image of the tumor was used as a sample to train the CNN. In the test phase, each layer of CT images of the tumor will obtain a predicted probability value. Finally, the average of the predicted probability values of all CT slices of the tumor image is taken as the final predicted value. Subnetwork 1 has learned general image features by means of migration learning. In order for the network to learn the features associated with mutations in the EGFR gene, we constructed subnetwork 2 behind subnetwork 1 to learn the features associated with the EGFR gene mutation status for this dataset. Since subnetwork 2 is a redesigned structure, it is no longer initialized using migration learning, but is randomly initialized using the Xavier algorithm. Deep learning model training is an iterative process. The purpose of model training is to optimize the parameters in the model and establish the correlation between CT images and EGFR gene mutation status. In each iteration, researchers used cross entropy as the loss function to measure the predictive performance of the deep learning model. Since the parameters of subnetwork 1 are obtained through migration learning, the general image features can already be extracted; but the weight of subnetwork 2 is randomly initialized, and there is no meaning at the beginning of the training, and the randomly initialized network large gradients will be generated at the beginning of the training when performing back propagation, the noise gradient generated by random initialization will destroy the trained weights in subnetwork 1 and lose the meaning of migration learning. Therefore, we first freeze subnetwork 1, and then train only subnetwork 2 with a learning rate of 1 10 3. After training the model for 10 cycles, we trained the entire network (including subnetwork 1 and subnetwork 2) with a small learning rate (1 10 5). At this time, the high-level image features learned by subnetwork 2 and the low-level image features of subnetwork 1 migration learning are simultaneously trained to achieve complementary effects. A total of 14,926 training sample images were generated by using a 2D CT slice image as a training sample. Therefore, subnetwork 1 was pretrained using 1.28 million natural images, and then refitted using 14,926 lung cancer CT images; subnetwork 2 was directly trained using 14,926 lung cancer CT images. Finally, after 30 cycles of training, the model converges.
74 CHAPTER 2 Key technologies and software platforms for radiomics


In order to reduce the difference in image grayscale caused by different devices, we used z-score to normalize the tumor image, adjusted the mean of the tumor CT image to 0, and adjusted the standard deviation to 1. In addition, all tumor images were scaled to the same size (64 64). The migration learning network was AUC 1⁄4 0.88 on the training set and AUC 1⁄4 0.81 on the test set by training on 631 lung adenocarcinoma CT image data. In this study, traditional imaging and clinical models were also used as comparative experiments. The image omics model extracts image morphological features such as shape, intensity, texture, and wavelet, and uses a random forest as a classifier. The clinical model uses age, gender, and tumor TNM staging as features, using SVM as a classifier. On the test set, the AUC of the image ensemble model and the clinical model are 0.64 and 0.61, respectively, so the deep learning model has achieved better performance. Further decision curve analysis shows that the use of a deep learning model for predicting patient EGFR gene mutations can yield good clinical benefits. Since the deep learning model is an end-to-end model, its features are combined with classifiers that cooperate with each other. After using the visualization algorithm to find the predicted focus map of the deep learning model, the deep learning model can locate each tumor in a suspicious area, providing a reference puncture location for the doctor doing the clinical puncture. The advantage of deep learning is mainly due to its powerful feature of selflearning ability. Through migration learning and 14,926 tumor CT images, the deep learning model unearthed CT image features closely related to the EGFR gene mutation status. To better understand the deep learning features, researchers visualized several representative convolution filters in the deep learning model. It can be seen from the figure that the shallow convolution layer has learned simple image features such as diagonal edges and horizontal (Conv_2). Deeper convolutions have learned more complex features, such as tumor shape information. For example, the filters in the Conv_13 layer have a strong response to these circular or curved shapes because most lung tumors are elliptical or nearly circular. When entering a deeper convolutional layer, deep learning features become more abstract and their correlation with the mutation state of the EGFR gene is getting stronger, such as the Conv_20 layer and the Conv_24 layer. Since migration learning uses the weights that the network trains on the ImageNet dataset, the extracted features are similar to those of natural images. In the shallow network, the convolution kernel mainly extracts the edge information, and the deeper layer extracts the shape information. As the network deepens, some complex abstract features are gradually extracted, and the visual interpretation of these features is not obvious. However, its gradual association with EGFR mutation information is more conducive to model prediction. The same convolution kernel in a CNN responds differently to different tumors. For example, a convolutional kernel has a strong response to tumors with EGFR mutations, while a convolutional kernel with EGFR nonmutation has a weak response; in contrast, other convolutional nuclei respond more strongly to EGFR nonmutated tumors. However, the tumor response to EGFR mutations is weak. The study quantified the response of
2.5 Model building 75


the convolutional kernel, indicating that there is a significant difference in the response of the two convolutional kernels to EGFR-mutated and nonmutated tumors. In addition, unsupervised clustering of features indicates that deep learning features are highly correlated with EGFR mutations and show significant clustering in a cluster analysis.
2.5.8 Semisupervised learning
Generally, performance of supervised learning is better than unsupervised learning. Since during supervised model training, the clinical label is considered. However, supervised learning’s performance is limited by insufficiently labeled data. As a trade-off between better performance and enough training data, semisupervised learning could be a good choice. A large amount of unlabeled data are utilized by semisupervised learning to mine tumor information, while small amounts of labeled data are utilized to build the relationship between features and clinical labels. In some prognostic studies, patients need long-term follow-up (such as recurrence, disease progression, and overall survival), so labeled data are very difficult to obtain. However, unlabeled data are relatively easy to obtain. For example, the number of patients with follow-up in a study available in a medium-sized hospital is usually several hundred, but the number of patients without follow-up can exceed several thousand. At this point, information extraction from unlabeled data is the key to improve the performance of the radiomics model. Semisupervised learning, as a hybrid model that combines supervised learning with unsupervised learning, can mine information from a large amount of unlabeled data while constructing predictive models with a small amount of labeled data. In general, supervised learning models have good predictive performance because they are trained to take into account the correlation between features and labels. Unsupervised learning is not directly related to clinical predictive labels during training, so its predictive performance is limited, but it can mine the information naturally found in the data. As a compromise between performance and data volume, semisupervised learning can combine the advantages of unsupervised learning to mine unlabeled data information and the predictive ability of supervised learning. In the study of the prediction of ovarian cancer recurrence time, the author proposed a framework for semisupervised learning [20]. The model is divided into two parts: (A) unsupervised feature learning; (B) supervised Cox prognostic prediction model construction. In unsupervised feature learning, the authors designed a convolutional self-encoder to learn the characteristics of ovarian cancer from unlabeled data.
1. Principle of the self-encoding algorithm In supervised learning, training samples are labeled with categories. Now suppose the training sample set is xð1Þ,xð2Þ,xð3Þ/ , which has no category label, where xðiÞ ̨Rn. A self-encoding neural network is an unsupervised learning algorithm and has a target value equal to the input value, such as
76 CHAPTER 2 Key technologies and software platforms for radiomics


yðiÞ 1⁄4 xðiÞ. The aim of the self-encoding neural network is to learn a function of hW;bðxÞ z x. In other words, it attempts to approximate an identity function such that the output xb is close to input x. The learning significance of the identity function is that, when we add some restrictions to the self-encoding neural network, such as limiting the number of hidden neurons, we can find some interesting structures from the input data. For example, suppose that input x of a self-encoding neural network is a 64 64 pixels gray image (a total of 4096 pixels), and there are 100 hidden neurons in hidden layer L2. The output is also 4096 dimensions of y ̨R4096. Since there are only 100 hidden neurons, we force the self-encoding neural network to learn the compressed representation of the input data. It must be from the 50-dimensional hidden neuron activation vector að2Þ ̨R100 reconstructing the 4096-dimensional pixel gray value input x. If the input data of the network are completely random, for example, each input xi is completely independent of other features, then this compressed representation will be very difficult to learn. But if the input data imply some specific structure, such as some input features are related to each other, then these correlations in the input data can be found using the algorithm. In fact, low-dimensional representation of input data can be usually learned from this simple self-encoding neural network, which is very similar to the PCA results. Our previous discussion is based on the number of hidden neurons is small. When the number of hidden neurons is large (possibly more than the number of input pixels), applying other constraints to the self-encoding neural network can still find the structure in the input data. Specifically, if we add sparsity constraints to hidden neurons, the self-encoding neural network can still find some interesting structures in the input data even the number of hidden neurons is large. The concept of “sparseness” is as follows. The neuron is considered to be activated if its output is close to 1, and when the output is close to 0, it is considered to be suppressed. So the limit that causes most of the neuron’s time to be suppressed is called the sparsity limit. Here we assume sigmoid function is the activation function of the neuron. If tanh function is used as the activation function, we think neurons are suppressed when the neuron output is 1.
Note that að2Þ
j represents the degree of activation of the hidden neuron j, but this representation does not explicitly indicate which input x brings this activation.
So we will use að2Þ
j ðxÞ to indicate the degree of activation of the self-encoding neural network hidden neurons j given input x. Define the following formula as
rbj 1⁄4 1
m
Xm
i1⁄41
h
að2Þ
j xðiÞ i
(2.61)
This indicates the average activity of hidden neurons j (average over the training set). We can approximately add a constraint rbj 1⁄4 r, where r is the sparsity parameter, usually a smaller value close to 0 (such as r 1⁄4 0:05). In other words,
2.5 Model building 77


we want to make the average activity of hidden neurons j close to 0.05. The activity of hidden neurons must be close to 0 to satisfy this condition. In order to achieve this limitation, an additional penalty factor will be added to our optimization objective function, which will punish those cases that are significantly different so that the average activity of hidden neurons remains within a small range. There are many reasonable choices for the specific form of the penalty factor. We will choose the following one:
sX2
j1⁄41
r log r
rbj
þ ð1 rÞlog 1 r
1 rbj
(2.62)
Here, S2 is the number of hidden neurons in the hidden layer, and the index j in turn represents each of the neurons in the hidden layer. If you are familiar with relative entropy (KL divergence), this penalty factor is actually based on it. It can also be expressed as follows:
sX2
j1⁄41
KL r jj rbj (2.63)
KL r jj rbj 1⁄4 r log r
rbj
þ ð1 rÞlog 1 r
1 rbj
(2.64)
The function is relative entropy between two Bernoulli random variables, one with r as the mean and one with rbj as the mean. Relative entropy, a standard method for measuring the difference between two distributions. This penalty factor has the following properties: KL rjj rbj 1⁄4 0 when rbj 1⁄4 r, and monotonically increases as the difference between rbj and r increases. For example, when we set r 1⁄4 0:2, the relative entropy value KL rjj rbj increases as rbj decreases first (Fig. 2.18). We can see that relative entropy reaches its minimum value of 0 when rbj 1⁄4 r, and when rcj approaches 0 or 1,the relative entropy tends to be ∞. Therefore, minimizing this penalty factor has the effect of making rbj close to r. Now, our overall cost function can be expressed as follows:
JspareseðW; bÞ 1⁄4 JðW; bÞ þ b sX2
j1⁄41
KL r jj rbj (2.65)
where JðW; bÞ is as defined previously, and b controls the weight of the sparsity penalty factor. The rbj term also depends (indirectly) on ðW; bÞ because it is the average activation of the hidden neuron j, and the activation of the hidden layer neurons depends on ðW; bÞ. To perform the derivative calculation on relative entropy, we can use an easy-toimplement technique that requires only minor changes in your program. Spe
cifically, dð2Þ
i has been calculated when we calculated the second layer ðl 1⁄4 2Þ update in the backward propagation algorithm:
78 CHAPTER 2 Key technologies and software platforms for radiomics


dð2Þ
i1⁄4
0
@ sX2
j1⁄41
W ð2Þ
ji dð3Þ
i
1
Af 0 zð2Þ
i (2.66)
Now we will replace it with
dð2Þ
i1⁄4
0
@
0
@ sX2
j1⁄41
W ð2Þ
ji dð3Þ
i
1
Aþb
0
@r
rbj
þ1 r
1 rbj
1
A
1
Af 0 zð2Þ
i (2.67)
One thing to note is that we need to know rbj to calculate this update. So before calculating the backward propagation of any neuron, you need to calculate the forward propagation for all training samples to get the average activation. If your training samples can be small enough to be stored in memory, you can easily calculate forward propagation on all your samples and store the resulting activation in memory and calculate the average activation. Then, you can use the precalculated activation to calculate the backward propagation of all training samples. If your data volume is too large to be fully stored in memory, you can sweep through your training samples and calculate a forward propagation, then accumulate the results and calculate the average activation rbj
(when the activation degree að2Þ
j in the result of a forward propagation is used to
calculate the average activation degree rbj , the result can be deleted). Then, after completing the calculation of the average activation rbj , it needs to resend each training sample for forward propagation so that it can calculate the backward
Relative entropy
Average activation unit
FIGURE 2.18
Schematic diagram of relative entropy change.
2.5 Model building 79


propagation. For the latter case, it needs to calculate two forward propagations for each training sample, so the computational efficiency will be slightly lower. In this paper [20], two parts make up the self-encoder: an encoder and a decoder. The encoder compresses the image layer-by-layer convolution kernel into a 16dimensional deep learning feature; the decoder uses the 16-dimensional deep learning features to reconstruct the original input image. When the decoder can reconstruct the original input image from the deep learning features, a large amount of tumor information is contained in the deep learning features. When the self-encoder training converges, the encoder converts the tumor CT image into a 16-dimensional deep learning feature; the decoder then reconstructs the original tumor image through the 16-dimensional feature. The reconstructed image is similar to the original input image, indicating tumor information has been completely extracted. After learning the deep learning characteristics of the tumor through unsupervised learning, the supervised Cox proportional hazard model can use the deep learning feature to predict the recurrence time. When training the Cox model, patient follow-up information is required, so it is called a supervised learning model. After unsupervised learning with CT images of 102 patients with highgrade serous ovarian cancer, 49 patients with follow-up information were used to train the Cox prognostic model. Finally, two datasets (49 patients and 45 patients) were used to verify the predictive performance of the semisupervised learning model. This semisupervised learning model achieved a concordance index (C-Index) of 0.700 and 0.729 on both test sets. To further test the performance of the semisupervised Cox model for predicting recurrence risk, researchers used the semisupervised Cox model to predict the 3-year recurrence probability of patients. The semisupervised Cox model had the AUC 1⁄4 0.833 (95% CI: 0.792e0.874) in the training set and an AUC 1⁄4 0.772 (95% CI: 0.721e0.820) in Test Set 1. The predictive probability of this semisupervised Cox model was significantly different between patients with a relapse time of less than 3 years and patients with a relapse time of 3 years or more (P < .0001 in the train set and P 1⁄4 .0010 in test set 1). The calibration curve indicates that the semisupervised Cox model does not systematically underestimate or overpredict the 3-year recurrence probability because no significant difference between the semisupervised Cox model and the ideal model, which is tested by the HosmereLemeshow test (P 1⁄4 .475 in train set, P 1⁄4 .404 in test set 1). The decision curve shows that when the doctor’s decision threshold is greater than 0.3, the clinical benefit obtained by predicting the 3-years recurrence rate using the semisupervised Cox model is more than the two treatment options: treat all patients as relapsed within 3 years (treat-all) and treat all patients as relapsed after 3 years (treat-none). In the independent test set 2, the semisupervised Cox model also had similar performance (AUC 1⁄4 0.825, 95% CI: 0.765e0.893). The AUC of the semisupervised Cox model is higher than the clinical model in two independent test sets (in test set 1: AUC 1⁄4 0.443, 95% CI: 0.381e0.506). In
80 CHAPTER 2 Key technologies and software platforms for radiomics


test set 2, AUC 1⁄4 0.400, 95% CI: 0.268e0.536, and there was a significant difference (P 1⁄4 .0045 in test set 1, P 1⁄4 .0361 in test set 2,by DeLong test). 2. Visualize the self-encoder training results By visualizing the features of unsupervised learning, the intensity information of the tumor is extracted from the first convolution layer of the encoder; the second and the third convolution layers extract edge information; the information extracted by the third and fourth convolutional layers lacks intuitive visual interpretability, but it reflects high-dimensional abstraction of the tumor. Two tumors with different recurrence times were sent to the self-encoder and different responses were obtained. By further reducing the deep learning features to the 2D space, it can be found that in the deep learning feature space, patients with high recurrence and low recurrence show a more obvious clustering phenomenon. After training the self-encoder, we also want to visualize the function to figure out what it has learned. We take the example of training a self-encoder on a 64 64 pixels image. In this self-encoder, each hidden unit i is calculated as follows for the function of the input:
að2Þ
i 1⁄4f
0
@ 40X96
j1⁄41
W ð1Þ
ij xj þ bð1Þ
i
1
A (2.68)
The function we will be visualizing is the above function that takes a 2D image as
input and is calculated by hidden unit i. It is dependent on parameter Wð1Þ
ij
(temporarily ignores the offset bi). It should be noted that að2Þ
i can be regarded as a nonlinear feature of input x. However, there is still a problem: what kind of
input image x can make að2Þ
i get the maximum excitation? Here we have to add constraints to x, otherwise we will get a trivial solution. If we assume that the input has a norm constraint,
jjxjj2 1⁄4 40X96
i1⁄41
x2
i 1 (2.69)
The input that causes hidden unit i to get the maximum excitation should be given by pixel xj ð j 1⁄4 1; 2; 3/4096Þ calculated by the following formula:
xj 1⁄4 Wð1Þ
ij
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
X4096
j1⁄41 W ð1Þ
ij
2
r (2.70)
When we use the above formula to calculate the value of each pixel, compose them into an image, and present the image in front of us, the true meaning of the features sought by the hidden unit is gradually becoming clear. If we train a self-encoder with 100 hidden elements, the visualization will contain 100 such imagesdeach hidden unit corresponds to an image (Fig. 2.19).
2.5 Model building 81


3. The principle of the self-learning algorithm Giving the existing powerful machine learning algorithm more data can make it get better performance. There is even a saying in the machine learning community: “Sometimes the winner does not have the best algorithm, but more data.” More labeled data are always needed, but the cost is often high. For example, researchers have put considerable effort into using tools like AMT (Amazon Mechanical Turk) to get a larger training dataset. Compared to a large number of researchers building features by hand, it is an improvement to use crowd sourcing to manually label data by multiple people, but we can do better. Specifically, if the algorithm can learn from unlabeled data, then we can easily obtain a large amount of unlabeled data and learn from it. Self-learning and unsupervised feature learning are such algorithms. Although a single unlabeled sample contains less information than a labeled sample, if you can get a lot of unlabeled data (such as downloading random, unlabeled images, audio clips, or text from the Internet) and the algorithm can make effective use of them, the algorithm will achieve better performance than large-scale manual build features and labeled data. On the self-learning and unsupervised feature learning problems, the algorithm can be used to learn a good feature description with a large amount of unlabeled data. When attempting to solve a specific classification problem, supervised learning method can be used to complete the classification based on these
FIGURE 2.19
Schematic diagram of visual self-encoding results.
82 CHAPTER 2 Key technologies and software platforms for radiomics


feature descriptions learned from labeled data. But self-learning algorithm may be most effective in scenarios where there is a large amount of unlabeled data and a small amount of labeled data. Even in the case where only the data have been labeled (when we usually ignore the class label of the training data for feature learning), the above ideas can get good results. We have seen how to learn features from unlabeled data using an auto-encoder. Specifically, assume that there is an unlabeled training dataset xð1Þ,xð2Þ,xð3Þ/ . Now using them to train a sparse self-encoder, using the trained model parameters Wð1Þ; bð1Þ, given any input data x, you can calculate the activations a of hidden units. As mentioned earlier, a can be a better characterization than the original input x. The neural network in the figure below describes the calculation of the feature (activation amount a). This is actually the sparse self-encoder obtained before, and the last layer is removed here. Suppose there is a labeled training set xð1Þ,xð2Þ,/xðmÞ of size m. We can find a better characterization for the input data. For example, xð1Þ can be input to the sparse self-encoder to get the hidden unit activation amount að1Þ. Next, you can use að1Þ directly instead of the original data. It can also be combined into one, using new vectors instead of raw data. After transformation, the training set becomes að1Þ,að2Þ,/aðmÞ . Finally, a supervised learning algorithm (such as SVM or logistic regression) can be trained to obtain a discriminant function to predict the y value. The prediction process is as follows: Given a test sample x, repeat the previous process and send it to the sparse auto-encoder to get a. Then, a is sent to the classifier to get the predicted value. After building a radiomic model or deep learning model, the model must be validated. Only the validated model can show its potential value in clinical application. Compared with internally validated models, independent and externally validated models have higher confidence, as results from independently obtained data are usually more robust. In addition, for radiomic studies, prospectively validated models have the most credibility. Performance of radiomic models can be measured by dozens of tools. For discrimination analysis, the ROC curve is the most commonly used evaluation method, and the AUC, sensitivity, and specificity of the model can also be used to evaluate if the model can predict the clinical outcome. For survival analysis, the concordance index (C-index) and time-dependent ROC curve are usually used for validation. In addition, calibration curve is a useful tool for both discrimination analysis and survival analysis, as the agreement between the observed clinical outcomes and model predictions can obtained through it.
2.6 Radiomics quality assessment system
Because radiomics has a wide range of research, including research on different diseases, different diagnosis or prognostic prediction tasks, many scholars have
2.6 Radiomics quality assessment system 83


proposed or improved the new photographic group model, the application scenarios and quality lack a unified evaluation standard. According to Professor Lammin’s point of view on 2017 Nature Reviews Clinical Oncology [21], radiomics research can follow the quality assessment system below to measure the quality of the model. Using the quality assessment system shown in Table 2.1, we can measure the reliability and clinical application value of published work, and help to further improve the quality of radiomics research and provide uniform research standards within the industry.
Table 2.1 Radiomics quality assessment system.
Evaluation standard Points
1 Image imaging parametersdhave detailed imaging parameters (such as whether it is enhanced image, layer thickness, scan voltage, etc.) or have a public scanning protocol to ensure repeatable image acquisition
þ1 (if the imaging parameters are detailed) þ 1 (if using public scanning parameters)
2 Multiple segmentationdsegmentation using different doctors/algorithms/software, random noise is considered in segmentation, segmentation is performed in different respiratory cycles to verify the stability of the extracted features for segmentation
þ1
3 Performing a profiling experiment on all scanning instrumentsddetecting differences between instruments and selecting features that are insensitive to scanning instruments
þ1
4 Imaging at multiple time pointsdcollecting images of individual patients at multiple time points, testing the stability of features over time
þ1
5 Perform feature selection or multiple test setsdreduce the risk of overfitting and consider feature stability when selecting features
3 (if neither item is available) þ 3 (if there is an implementation)
6 Multivariate analysis using nonradiomics features (e.g., EGFR mutation information)dprovides a more comprehensive model of fused radiomics features and nonradiomics features
þ1
7 Detecting the association between discussion features and biological dimensionsddemonstrating the association between subtype differences in radiomics and gene-protein levels, and explaining the association between radiomics and biology
þ1
8 Cutoff analysisduse the median as a cutoff to divide risk groups
þ1
9 Reporting statisticsdreport C-Index, ROC curve, AUC, and other statistical results. You can also use cross-validation and other methods
þ1 (if both statistical and significant values are available) þ 1 (if resampling is used)
84 CHAPTER 2 Key technologies and software platforms for radiomics


2.7 Radiomics software platform 2.7.1 Radiomics software
This software was developed by the radiomics research team of professor Tian Jie, Institute of Automation, Chinese Academy of Sciences (Key Laboratory of Molecular Imaging, Chinese Academy of Sciences) (Fig. 2.20). The major purpose of the
Table 2.1 Radiomics quality assessment system.dcont’d
Evaluation standard Points
10 Correction statisticsdreport correction statistics (such as calibration curves) and their statistical values (P values and confidence intervals, etc.)
þ1 (if the correction statistics and their statistical values are reported) þ 1 (if resampling method is used)
11 Prospective studydprovides the highest level of evidence to support clinical validation
þ7 (proactive verification)
12 Verificationdverification set is not used for training
5 (lack of verification) þ 2 (concentric verification set) þ 3 (verification set for other centers) þ 4 (validation set from two different centers) þ 4 (the study validated previously published labels) þ 5 (provide validation sets for three or more different centers). The dataset must be at least the same order of magnitude as the number of features
13 Comparison with gold standardsdcompare radiomics models to current gold standards or better than gold standards (e.g., survival analysis using TNM staging)
þ2
14 Potential clinical applicationsdreport on current or potential clinical applications of the model (e.g., decision curve analysis)
þ2
15 Cost analysisdanalysis of the cost of reporting clinical problems
þ1
16 Open source datadopen source code and data
þ1 (raw data open source) þ 1 (ROI open source) þ 1 (code open source) þ 1 (if the radiomic feature is calculated on a representative ROI and the ROI is open source) Total points
37
2.7 Radiomics software platform 85


software is to provide doctors with a set of auxiliary diagnostic systems, including the segmentation of lung nodules, the malignancy classification, and N-stage prediction of tumors (Fig. 2.21).
2.7.2 Pyradiomicsdradiomics algorithm library
Pyradiomics (http://pyradiomics.readthedocs.io/en/latest/) was developed by Harvard University’s Hugo team, which provides a set of open source radiomic feature extraction algorithms using Python. The algorithm library mainly includes shape, texture, intensity, wavelet, HOG, and other scale operators for feature extraction. At the same time, the library provides a very convenient unified interface and support for convenient Python function calls; it also provides a 3D Slicer plug-in, which can be imported as a 3D Slicer plug-in for visual feature extraction, suitable for imaging doctors. There are currently four ways to install pyradiomics: 1. via pip; 2. from the source code github; 3. using the 3D Slicer Radiomics extension; and 4. using pyradiomics Docker. Pyradiomics can calculate the following characteristics:
1. Grayscale histogram feature (intensity feature/first-order feature) The gray histogram feature mainly uses statistics to describe the distribution of gray values in an image. Let X and X_all represent the pixel values of the previously outlined ROI, and the entire ROI has a total of N pixels. P is used to represent the probability vector of the gray histogram, and B is the center gray value. Define the following feature calculations: (1) Gray value intensity range:
gray value intensity range 1⁄4 maxX minX
maxXall minXall
(2.71)
FIGURE 2.20
Radiomics Software Interface. The user can directly load the patient DICOM file by clicking “Load Image” and perform automatic segmentation of the lung nodules, benign and malignant classification, and N staging prediction.
86 CHAPTER 2 Key technologies and software platforms for radiomics


FIGURE 2.21
Radiomics work diagram.
2.7 Radiomics software platform 87


(2) Energy:
energy 1⁄4 N Xl
i
ðN PðiÞÞ2 (2.72)
(3) Entropy:
entropy 1⁄4 N Xl
i
PðiÞ log2PðiÞ (2.73)
(4) Kurtosis:
kurtosis 1⁄4
XNl
i PðiÞ BðiÞ X 4
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
XNl
i PðiÞ BðiÞ X
r !4 (2.74)
(5) Maximum Value:
max X (2.75)
(6) Mean:
mean 1⁄4 X :1⁄4 1
N
N X
i
XðiÞ (2.76)
(7) Mean Absolute Deviation (MAD):
MAD 1⁄4 N Xl
i
PðiÞ BðiÞ X (2.77)
(8) Median:
jfx j x  ̨ X and x < xMediangj 1⁄4 jfx j x  ̨ Xand xMedian < xgj (2.78)
(9) Minimum Value:
min X (2.79)
(10) Number of Pixels:
number of pixels 1⁄4 jXj (2.80)
88 CHAPTER 2 Key technologies and software platforms for radiomics


(11) Range:
range 1⁄4 max X min X (2.81)
(12) Root Mean Square (RMS):
RMS 1⁄4 N Xl
i
PðiÞ BðiÞ2 (2.82)
(13) Skewness:
skewness 1⁄4
XNl
i PðiÞ BðiÞ X 3
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
XNl
i PðiÞ BðiÞ X
r !3 (2.83)
(14) Standard Deviation:
standard deviation 1⁄4
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1 N1
N X
i
XðiÞ X 2
v u u t (2.84)
(15) Sum Pixel:
sum pixel 1⁄4 N X
i
XðiÞ (2.85)
(16) Uniformity:
uniformity 1⁄4 N Xl
i
PðiÞ2 (2.86)
(17) Variance:
variance 1⁄4 1
N1
N X
i
XðiÞ X 2 (2.87)
2. Shape feature Shape features are primarily features used to describe the shape and size of the ROI. A is used to indicate the surface area of the ROI region, and V is used to represent the volume.
2.7 Radiomics software platform 89


(1) Compactness1:
compactness1 1⁄4 pffipffiffiV A2
3
(2.88)
(2) Compactness2:
compactness2 1⁄4 36p A2
V3 (2.89)
(3) Maximum 3D diameter: Calculate the largest Euclidean distance between two pixels in the ROI area. (4) Spherical Disproportion:
spherical disproportion 1⁄4 A
4p R2 1⁄4 A
6pffipffiffi V 2
3
(2.90)
(5) Spherical:
spherical 1⁄4 6p2V 2
3
A
(2.91)
(6) Surface Area to Volume ratio:
surface area to volume ratio 1⁄4 A
V (2.92)
(7) ROI Volume:
V 1⁄4 N rlat rcor rax (2.93)
3. Texture feature Texture features are primarily used to describe the characteristics of the pixels of the ROI and its surrounding pixels. It can be calculated by using the Gray-Level Co-occurrence Matrix (GLCM), Gray-Level Size Zone Matrix (GLSZM), Gray-Level Run Length Matrix (GLRLM), Neighborhood Gray-Tone Difference Matrix (NGTDM), and Gray-Level Dependence Matrix (GLDM). The gray values of the image are combined into Ng. Based on these combined gray values, the size of the Gray-Level Co-occurrence Matrix (GLCM) P is Ng Ng. The ði; jÞ th element of the matrix is defined as the pixel of the i thNg multiplied by the distance d, and the direction is a. The following example is a 2D image I and its GLCM, d 1⁄4 1; 2; 3 in the horizontal direction:
90 CHAPTER 2 Key technologies and software platforms for radiomics


24112 12200 13551 10211 I1⁄4 2 3 4 2 3 GLCM1⁄4 1 0 0 1 2 12521 11000 53135 11101
Calculate 3D texture features based on GLCM. Pa;dði; jÞ 1⁄4 Pði; jÞ: The probability of GLCM pairing i; j. m: The mean of Pði; jÞ. s: The standard deviation of
Pði; iÞ.PxðiÞ 1⁄4 NPg
j
Pði; jÞ: The probability of each line. mx: The mean of
PxðiÞ.sx: The standard deviation of PxðiÞ. PxþyðkÞ: 1⁄4 NPg
i
NPg
j
Pði; jÞ; i þ j 1⁄4 k.
Px yðkÞ: 1⁄4 NPg
i
NPg
j
Pði; jÞ; ji jj 1⁄4 k.
(1) Autocorrelation:
autocorrelation 1⁄4 N Xg
i
N Xg
j
i j Pði; jÞ (2.94)
(2) Cluster Prominence:
cluster prominence 1⁄4 N Xg
i
N Xg
j
ði þ j 2mÞ4Pði; jÞ (2.95)
(3) Cluster Shade:
cluster shade 1⁄4 N Xg
i
N Xg
j
ði þ j 2mÞ3Pði; jÞ (2.96)
(4) Cluster Tendency:
cluster tendency 1⁄4 N Xg
i
N Xg
j
ði þ j 2mÞ2Pði; jÞ (2.97)
(5) Contrast:
contrast 1⁄4 N Xg
i
N Xg
j
ði jÞ2Pði; jÞ (2.98)
2.7 Radiomics software platform 91


(6) Correlation:
correlation 1⁄4 1
s
N Xg
i
N Xg
j
ði mÞðj mÞPði; jÞ (2.99)
(7) Difference Average:
difference average 1⁄4 N Xg
i
i Px yðiÞ (2.100)
(8) Differential Entropy:
differential entropy 1⁄4 N Xg
i
Px yðiÞ log2ðPx yðiÞÞ (2.101)
(9) Difference Variance:
difference variance 1⁄4 N Xg
i
i Px y
2 Px yðiÞ (2.102)
(10) Dissimilarity:
dissimilarity 1⁄4 N Xg
i
N Xg
j
ji jjPði; jÞ (2.103)
(11) Joint Energy:
joint energy 1⁄4 N Xg
i
N Xg
j
Pði; jÞ2 (2.104)
(12) Joint Entropy:
joint entropy 1⁄4 N Xg
i
N Xg
j
Pði; jÞlog21⁄2Pði; jÞ (2.105)
(13) Harralick Correlation:
Harralick correlation 1⁄4 1
sx
2
4 N Xg
i
N Xg
j
i j Pði; jÞ mx
3
5 (2.106)
92 CHAPTER 2 Key technologies and software platforms for radiomics