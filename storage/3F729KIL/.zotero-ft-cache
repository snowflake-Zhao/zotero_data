版权信息 
书名:YOLO目标检测 
作者:杨建华 李瑞峰 
出版社:人民邮电出版社有限公司 
出版时间:2023-12-01 
ISBN:9787115627094 
品牌方:人民邮电出版社有限公司 
 
 
 内容提要 
本书主要介绍基于视觉的YOLO框架的技术原理和代码实现,并 
讲解目标检测领域中的诸多基础概念和基本原理,在YOLO框架的基 
础上介绍流行目标检测框架。 
本书分为4个部分,共13章。第1部分介绍目标检测领域的发展简 
史、主流的目标检测框架和该领域常用的数据集。第2部分详细讲解从 
YOLOv1到YOLOv4这四代YOLO框架的网络结构、检测原理和训练策 
略,以及搭建和训练的YOLO框架的代码实现。第3部分介绍两个较新 
的YOLO框架—YOLOX和YOLOv7,着重讲解其设计理念、网络结构 
和检测原理。第4部分介绍DETR、YOLOF和FCOS在内的流行目标检 
测框架和相应的代码实现。 
本书侧重目标检测的基础知识,包含丰富的实践内容,是目标检 
测领域的入门书,适合对目标检测领域感兴趣的初学者、算法工程 
师、软件工程师等人员学习和阅读。 
 
 
 谨以此书献给我已去世的父亲,感谢您在我生命的前二十五年中 
给予的教育、指导和鼓励,是您将我从一个无知的少年引上了人生的 
正途,让我成为一个完整的人。同时,也献给我的母亲,感谢您的陪 
伴和悉心呵护,是您让我能够在这个复杂的世界中健康茁壮地成长, 
始终保持对身边的人和世界的热爱。 
——杨建华 
 
 
 前言 
我本人很喜欢物理学家费曼先生,尤其是他的科学精神,我的治 
学过程受他的启发很大。当我涉足一个新的知识领域时,只有当我能 
够完全推导出这个知识领域的知识框架时,才会认为自己已经学会并 
掌握了这个知识。 
在我第一次投入目标检测领域时,首要建立的就是这一领域的知 
识体系。然而,我发现如此热门的领域竟然连一本较为系统的、理论 
与实践相结合的入门书都没有。虽然能够在网上搜索到很多带有“目标 
检测”字眼的技术图书,可是读上几章就发现前半部分充斥着太多机器 
学习和深度学习的基础概念和公式,等熬过阅读这些基础内容的时 
间,充满期待地去读后半部分时,却又觉得干货太少,大多时候只停 
留在对某个流行的目标检测框架已开源代码的讲解,以大量篇幅介绍 
怎么训练开源代码,怎么测试开源代码,又怎么在自己的数据集上使 
用开源代码。当我想了解设计一个目标检测网络的方法、制作训练所 
需的正样本的原理及其实现方法、损失函数的原理、一个目标检测框 
架的逻辑的时候,我就迷失在了这些文字的汪洋大海里。 
相反,在很多技术大牛的博客文章和一些技术论坛中,我逐步掌 
握了一些工作的技术内核,学习了他们搭建网络的技术路线、制作正 
负样本的数学原理、各种损失函数的效果和目标检测框架的内在逻辑 
等知识。在有了这些技术基础后,通过不断地模仿和思考,我也逐渐 
写出了一套自己的目标检测项目代码,这对我日后去阅读新的目标检 
测论文、开展前沿工作、上手开源代码都带来了很大的帮助。这不仅 
让我掌握了一些微观上的操作细节,也让我对目标检测领域有了宏观 
上的把握。 
 
 
 但是,当我再回顾自己这段学习的心路历程时,还是觉得这样的 
学习方式,诸如选择适合初学者的论文、选择通俗易懂又全面的科普 
文章等,具有太多的偶然性。如果迟迟没有找到合适的文章,那就不 
能理解什么是YOLO检测器,什么又是Detection with Transformers框 
架。同时,大多数开源代码的上手难度较高,“九曲十八弯”的嵌套封 
装往往让初学者刚上手就迷失在了一次又一次的代码跳转里,更不用 
说要将一堆堆零散的知识串联成一个可以印刻在大脑里的认知框架。 
在我写下这段文字时,深度学习仍旧是以实践为主,它的重要分 
支—目标检测也依旧是以实践为主的研究领域,但很多相关图书往往 
只停留在基础知识的讲解上,所使用的代码也是网上现成的开源代 
码,这对于初学者来说通常是不友好的。 
在某个闲暇的傍晚,我仰靠在实验室工位的椅子上,思索着刚看 
完的论文,正被其中云山雾绕般的复杂理论所困扰,那一刻,灿烂的 
夕阳照亮了灰白的天花板,一切都被温暖的橘色所笼罩,焕发出鲜活 
的色彩。我坐直身子,凝视着窗外远处被柔和的夕阳所点缀的大楼, 
心旷神怡。忽然间,我萌生了写一系列我所认可的目标检测科普文章 
的念头,其中既包括对经典论文的解读,又包括原理层面的讲解,最 
重要的是提供一套可复现的代码,让读者能够从编写代码的角度进一 
步加深对目标检测的理解,最终将那些我所认为的偶然性都变成必然 
性。 
于是,我开始在知乎上写相关的文章。那时候,我对YOLO很感 
兴趣,这也是目标检测领域最热门的目标检测架构,因而我选择通过 
写 YOLO 相 关 的 科 普 文 章 来 讲 解 我 所 了 解 的 目 标 检 测 领 域 的 基 础 知 
识。 
 
 
 渐渐地,随着自己对YOLO的认识、对目标检测领域认识的不断 
加深,我写的科普文章越来越多,内容也越来越详细。同时,随着我 
代码功底的提升,与科普文章配套的代码实现也越来越丰富。我对自 
己的科普工作有3点要求:相关论文必须读透、科普内容必须翔实、代 
码实现必须亲自动手。尤其是第三点,在我看来,是很多科普文章所 
缺乏的,这些文章最多就是放上已有的开源代码来补充内容。或许, 
正是因为很多读者能够在我的科普文章中既习得了感兴趣的技术原 
理,又获得了一份可以运行的、可读性较高的代码,理论与实践相结 
合,避免了纸上谈兵,所以读者对我的一些文章给出了积极评价和赞 
赏。 
在这两年时间中,我坚持跟进目标检测领域的技术发展,在业余 
时间里动手实现每一个感兴趣的模块甚至是整个网络架构,配合自己 
的代码做深度的论文讲解,因此我写出的科普文章越来越多,还创建 
了 以 YOLO 为 核 心 的 目 标 检 测 入 门 知 乎 专 栏 。 尽 管 在 如 今 这 个 讲 究 
“快”的时代,一点一点学习基础知识可能不如直接在现有工作的基础 
上做一些“增量式改进”来得实在,但我还是坚持自己的理念,继续进 
行这方面的科普工作。 
如今,在人民邮电出版社编辑的赏识下,我有幸能够将这些年来 
的科普文章汇总成一本技术图书,对我来说,这是对我科普工作的一 
大肯定。我很希望本书能够填补该领域中入门图书的空白,为初学者 
提供一个较好的入门资料。同时,也由衷地希望这本书能够抛砖引 
玉,引来更多的专业人士拨冗探讨,引导后人。 
那么,回到这本书所要涉猎的技术领域:什么是目标检测(object 
detection)? 
 
 
 在计算机视觉领域中,目标检测是一个十分基础的计算机视觉问 
题,是图像分类(image classification)这个相对简单且更基础的任务的 
诸多下游任务中的一个重要分支。在图像分类任务(如图0-1所示) 
中,我们设计一个分类器(classifier)模型,期望这个分类器能够识别出 
给定图像的类别,例如输入一张有关猫的图像,我们希望分类器能够 
判别出输入图像中的目标是一只猫,如图0-1所示。 
图0-1 图像分类任务 
不过,尽管能够识别出“猫”这一类别,但对于其所处的空间位置 
却几乎是不知道的。因此,图像分类任务有着明显的局限性。不同于 
图像分类任务,在目标检测任务中,我们需要设计一个检测器 
(detector)模型,期望这个检测器能够识别出图像中我们所感兴趣的目 
标,这里对于“识别”的定义既包括识别出每个目标的类别,又要定位 
出每个目标在图像中的位置。例如,输入一张图像,如图0-2所示,我 
们希望检测器能够识别出图像中的“猫”和“电视机”,并采用边界框的 
形式来标记目标在图像中所处的空间位置。 
 
 
 图0-2 目标检测任务 
乍一看,这样的任务对人类来说是一件易如反掌的事情,多数人 
几乎不需要经过相关的培训和训练,即可识别和定位出现于我们视野 
中的物体。然而,就是这么一个对人类来说再简单不过的任务,对计 
算机而言,却是十分困难的。 
直 到 21 世 纪 初 , 随 着 深 度 学 习 中 的 卷 积 神 经 网 络 (convolutional 
neural network,CNN)技术的兴起,目标检测才得到了长足的发展。尽 
管在此之前,已经出现了一批基于传统人工视觉特征(如HOG特征) 
的方法,然而目标检测在真正意义上的突破还是从深度学习时代开始 
的。 
目标检测发展至今,可以说是百家争鸣,百花齐放,不同的算法 
有着不同的特色和优势,倘若我们一一讲来,这将会是一本长篇且有 
趣的综述类图书。但同时也会使这本书变得厚重无比,成为长期放于 
书架、与尘土作伴的“大部头”。这并不是我的初衷。 
不论是哪一个科学领域,总会有几个代表性的工作时常被人提 
起。在目标检测领域中,YOLO(You Only Look Once)便是这样的工作 
之一。YOLO是一个具有里程碑意义的存在,以在GPU上的实时检测 
速度和简洁的网络架构两大特点而一鸣惊人,打破了R-CNN系列工作 
的神话,结束了基于two-stage方法的检测框架的统治时代,掀开了基 
 
 
 于深度学习的目标检测领域的新篇章,创建了新的目标检测范式,为 
这一领域注入了新鲜的、更具有潜在研究价值的新模式。在后续许多 
出色的工作中,我们都能够看到YOLO的影子。 
时 至 今 日 , YOLO 网 络 已 从 最 开 始 的 YOLOv1 发 展 出 YOLOv2 、 
YOLOv3和YOLOv4等多个版本。在GitHub上,由非YOLO官方团队实 
现 的 YOLOv5 也 备 受 研 究 者 的 青 睐 , 以 及 由 旷 视 科 技 公 司 发 布 的 
YOLOX再度将YOLO工作推向了新的高峰。2022年,美团公司发布的 
工业部署友好型的YOLOv6和YOLOv4的作者团队新推出了YOLOv7, 
再一次刷新了YOLO系列的性能上限。随着这些优秀的研究者们不断 
致力于优化和改善YOLO框架,YOLO几乎成了目标检测任务的代名 
词,是当前目标检测社区较活跃,也是较受欢迎的工作。或许终有一 
天,YOLO将被这个时代所抛弃,但在目标检测发展史中,YOLO所筑 
下的里程碑将永远屹立。 
正因如此,我斗胆选择以YOLO为核心,写下这本以“入门目标检 
测”为宗旨的技术图书。本书可能是第一本以实践为出发点来讲解 
YOLO网络的教程类图书,也是一本对初学者较友好的目标检测入门 
书。同时,请允许我以这么一本基础书来为各位读者抛砖引玉。 
本书的组织结构 
本书包含四大部分,共13章。以下是本书各章内容的简要介绍。 
第1部分是“背景知识”,涉及第1章、第2章的内容。 
● 
第1章,“目标检测架构浅析”。详略得当地介绍了自深度学习 
时代以来的目标检测的发展简史,以简略的笔墨向读者铺开这一技术 
发展的画卷。在这一章中,作者列出了若干经典的目标检测框架,如 
 
 
 R-CNN 系 列 和 YOLO 系 列 , 讲 述 了 当 前 目 标 检 测 领 域 的 两 大 技 术 流 
派:两阶段和单阶段。同时,介绍了当前流行的目标检测架构,包含 
主干网络、颈部网络和检测头三大部分,这为以后的改进和优化工作 
提供了较为清晰的路线和准则。目标检测发展得已较为成熟,由于篇 
幅有限,作者无法将每一部分的所有工作都罗列出来,因此只能挑选 
其中极具代表性的工作进行介绍。在了解了相关原理后,建议读者顺 
藤摸瓜地去了解更多的相关工作,丰富知识体系。 
● 
第2章,“常用的数据集”。介绍了目标检测领域常用的两大数 
据集:PASCAL VOC数据集和MS COCO数据集,其中,MS COCO数 
据集是最具挑战性的、当下诸多论文中必不可少的重要数据集之一。 
了解这些数据集的基本情况,是入门目标检测领域的基本功之一,有 
助于读者开展后续工程或学术方面的工作。 
第2部分是“学习YOLO框架”,涉及第3章~第8章的内容。 
● 
第 3章 , “YOLOv1”。 详细讲解经典的YOLOv1工作,包括网络 
结构、检测原理、训练中的标签分配策略、训练模型的策略以及前向 
推理的细节。通过本章的学习,读者将正式迈过目标检测领域的门 
槛,对目标检测任务建立基本的认识,掌握基于YOLO框架的检测技 
术路线,这有助于开展后续的学习和研究工作。 
● 
第 4章 , “搭 建 YOLOv1网 络 ”。 在第3章所学习的YOLO相关知 
识的基础上,通过对YOLOv1的网络结构做适当的改进,着手编写相 
关的网络结构的代码。本章的代码实现环节将有助于提升读者对目标 
检测框架的认识,使其对如何基于现有的深度学习框架搭建目标检测 
网络有一定的基本了解。 
 
 
 ●第5章,“训练YOLOv1网络”。本章进一步编写YOLOv1的项目 
代码,在第4章的基础上,本章主要编写读取数据、预处理数据、搭建 
模型、实现标签匹配、实现训练和测试代码以及可视化检测结果等诸 
多代码实现内容。通过学习本章,读者将对如何搭建一个目标检测框 
架并实现训练和测试等必要的功能有一个较为清晰的认识。这些认识 
也将对读者日后开展深入研究、快速掌握其他开源代码的架构起着很 
大的作用。 
● 
第6章,“YOLOv2”。介绍了自YOLOv1之后的新一代YOLOv2网 
络,着重介绍了YOLOv2所采用的各种改进和优化方式,有助于读者 
了解包括批归一化层、先验框、多尺度训练等在内的关键技术。这些 
技术都是当前主流的目标检测框架中不可或缺的部分。同时,还对 
YOLOv2做了一次复现,有助于读者从代码实现的角度进一步加深对 
YOLOv2的认识,同时巩固搭建目标检测项目的代码能力。 
● 
第 7章 , “YOLOv3”。 介绍了YOLOv3检测框架的技术原理和细 
节。自YOLOv3开始,YOLO系列工作的整体面貌就基本确定下来:强 
大的主干网络和多尺度检测架构。这两点在后续的每一代YOLO检测 
器中都能清晰展现。同时,也讲解了YOLOv3的代码实现,完成对复 
现的YOLOv3的训练和测试。 
● 
第 8章 , “YOLOv4”。 介绍了YOLOv4检测框架的技术原理和细 
节 , 着 重 介 绍 了 相 较 于 YOLOv3 的 诸 多 改 进 。 同 时 , 也 讲 解 了 复 现 
YOLOv4 的 相 关 代 码 实 现 , 进 一 步 引 导 读 者 从 实 现 的 角 度 加 深 对 
YOLOv4的认识和理解,帮助读者巩固和强化对一个完整的目标检测 
项目代码的认知和实现能力。 
第3部分是“较新的YOLO框架”,涉及第9章、第10章的内容。 
 
 
 ● 第 9 章 , “YOLOX” 。 介 绍 了 新 一 代 的 YOLO 框 架 , 讲 解 了 
YOLOX对YOLOv3的改进以及新型的动态标签分配,并动手实现了一 
款较为简单的YOLOX检测器。 
● 
第10章,“YOLOv7”。介绍了YOLOv7检测框架的技术原理,主 
要介绍了YOLOv7所提出的高效网络架构的实现细节,并动手实现了 
一款较为简单的YOLOv7检测器。 
第4部分是“其他流行的目标检测框架”,涉及第11章、第12章和第 
13章的内容。 
● 
第11章,“DETR”。介绍了掀起Transformer在计算机视觉领域中 
的研究浪潮的DETR,讲解了DETR的网络结构,并通过讲解相关的开 
源代码来展现DETR的技术细节。 
● 
第12章,“YOLOF”。介绍了新型的单级目标检测网络,讲解了 
YOLOF独特的网络结构特点和所提出的标签匹配,并通过代码实现的 
方式复现了YOLOF,进一步增强读者的代码能力。 
● 
第 13章 , “FCOS”。 介 绍 了 掀 起 无 先 验 框 检 测 架 构 研 究 浪 潮 的 
FCOS检测器,填补了前文对于无先验框技术框架的空白,加深了读者 
对无先验框检测架构的理解和认识。FCOS是这一架构的经典之作,也 
是常用的基线模型,同时,无先验框技术框架也是当下十分受欢迎的 
框架。 
本书特色 
1.较为全面的YOLO系列内容解读 
 
 
 本书以YOLO系列为核心,围绕这一流行的通用目标检测框架来 
开展本书的技术讲解、代码实现和入门知识科普等工作。本书翔实地 
讲解了自YOLOv1到YOLOv4的发展状况和相关技术细节。尽管在本书 
成稿时,YOLOv4已经算是“古董”了,但即便是这样一个“古董”,最新 
的YOLO检测器也没跳出YOLOv4的技术框架,无非是在每一个模块中 
采用了最新的技术,但其基本架构是一模一样的。因此,在学习 
YOLOv4后,就能在宏观上对YOLO框架的发展有足够清晰的认识,同 
时在微观上了解和掌握相关的技术细节,为日后读者自学更新的 
YOLO检测器做足了相关知识储备,也为后续的改进和优化夯实了基 
础。书中也提供了大量图片以帮助读者更加直观地理解YOLO系列。 
2.作者编写的开源代码 
完整、可复现的开源代码是本书最大的亮点。本书不仅详细地讲 
解了YOLO系列所涉及的理论知识,更是在此基础上编写了大量的相 
关代码。正所谓“纸上得来终觉浅,绝知此事要躬行”,只有通过阅读 
代码、编写代码和调试代码,才能对YOLO具备更加全面的认识,而 
这些认识又会为入门目标检测领域提供大量的正反馈。本书的绝大多 
数代码是由作者亲手编写,部分实现也借鉴了现有的开源代码,而非 
简单地借用或套用已开源的YOLO项目代码作为范例,或者投机取巧 
地解读YOLO开源项目。每一次代码实现环节都对应一份完整的目标 
检测项目代码,而非零散的代码块,其目的是让读者能够一次又一次 
地建立起对完整的目标检测项目的认识。本书的诸多经验和认识都是 
建立在作者编写的大量丰富代码的基础上,使得读者既能够在阅读此 
书时对YOLO建立起一个感性认识,同时又能够通过阅读、模仿、编 
写和调试代码建立起对YOLO的理性认识。 
 
 
 3.经典工作的解读和代码实现 
除YOLO之外,本书还讲解了流行的目标检测框架(如DETR、 
YOLOF和FCOS),同时,也提供了由作者编写的完整的项目代码, 
以便读者阅读、复现和调试。通过学习这些YOLO之外的工作,有助 
于读者将从YOLO项目中学到的知识横向地泛化到其他检测框架中, 
进一步加深对目标检测的认识,同时还能够纵向地摸清、看清目标检 
测领域的发展趋势,掌握更多的技术概念,为后续的“践行”做足准 
备。 
本书读者对象 
本书主要面向具有一定神经网络基础知识、了解深度学习的基本 
概念、想要踏踏实实地夯实目标检测基础知识的初学者。同时,对于 
在 工 作 中 对 YOLO 框 架 有 一 定 涉 猎 , 但 缺 乏 对 相 关 技 术 的 了 解 和 掌 
握,并打算学习相关技术、掌握基础概念的算法工程师和软件工程师 
也同样适用。 
本书采用自底向上、由浅入深、理论与实践相结合的讲解方式, 
帮助读者建立起较为扎实的目标检测知识体系,有助于开展后续的研 
究工作。 
阅读本书需具备的基础知识 
由于本书不会去讲解过多的机器学习和深度学习的基本概念,因 
此希望读者在阅读此书时,已经具备了一些机器学习、神经网络、深 
度学习和计算机视觉领域相关的基础知识。同时,我们也希望读者具 
备Python语言、NumPy库和PyTorch深度学习框架使用基础,以及对流 
行的计算机开源库OpenCV的基本操作有所了解和使用经验。 
 
 
 另外,为了能够顺利调试本书提供的开源代码,还需要读者对 
Ubuntu操作系统具有一些操作经验。尽管本书的大多数代码也能在 
Windows系统下正常运行,但不排除极个别的操作只能在Ubuntu系统下 
运行。同时,读者最好拥有一块性能不低于GTX 1060型号的显卡,其 
显存容量不低于3GB,并且具备安装CUDA、cuDNN和Anaconda3的能 
力,这些硬件和软件是运行本书代码的必要条件。 
致谢 
感谢我的导师李瑞峰对我这些年的博士课题研究的支持和指导, 
感谢我的父亲和母亲的支持,也感谢我所在实验室的师兄、师弟和师 
妹的支持。没有你们的支持,就不会有这本书,是你们给予了我创作 
的动机、勇气和决心,是你们的支持赋予了我工作的最大意义。 
感谢人民邮电出版社的傅道坤编辑在本书的创作过程中提供指 
导、审阅书稿并反馈大量积极的修改建议,感谢人民邮电出版社的单 
瑞婷编辑在本书的审阅和校对阶段所付出的努力和不辞辛劳的帮助, 
也感谢人民邮电出版社的陈聪聪编辑的赏识以及杨海玲编辑提供的帮 
助和支持。 
感谢人民邮电出版社的工作人员为科技图书的普及做出的贡献。 
 
 
 第1部分 背景知识 
第1章 目标检测架构浅析 
通常,在正式迈入一个技术领域之前,往往先从宏观的、感性的 
层面来认识和了解它的发展脉络是很有益处的,因此,在正式开始学 
习YOLO(1)(2)(3)系列工作之前,不妨先从宏观的角度来了解一下什么 
是“目标检测”,了解它的发展简史、主流框架以及部分经典工作。拥 
有这些必要的宏观层面的认识对于开展后续的学习也是极其有益的。 
本章将从目标检测发展简史和当前主流的目标检测网络框架两大方面 
来讲一讲这一领域的发展技术路线。 
1.1 目标检测发展简史 
在深度学习时代到来之前,研究者们对目标检测的研究路线基本 
可以划分为两个阶段,先从图像中提取人工视觉特征(如HOG),再 
将这些视觉特征输入一个分类器(如支持向量机)中,最终输出检测 
结果。 
以现在的技术眼光来看,这种做法十分粗糙,但是已经基本能够 
满足那时实时检测的需求,并且已经在一些实际场景的业务中有所应 
用,但那主要得益于人体结构本身不算太复杂、特点鲜明,尤其是行 
走中的人的模式几乎相同,鲜有“奇行种”。不过,想做出一个可靠的 
通用目标检测器,识别更多、更复杂的物体,则存在很大的困难,在 
作者看来,造成这种困难的最根本的原因是我们难以用一套精准的语 
言或数学方程来定义世间万物。显然,要检测的物体种类越多,模型 
要学会的特征就越多,仅靠人的先验所设计出的特征算子似乎无法满 
足任务需求了。 
 
 
 直到2014年,一道希望之光照射进来,拨开了重重迷雾。 
2014年,著名的R-CNN(4)问世,不仅大幅提升了当时的基准数据 
集PASCAL VOC(5) 的mAP指标,同时也吹响了深度学习进军基于视觉 
的目标检测(object detection)领域的号角。从整体上来看,R-CNN的思 
路是先使用一个搜索算法从图像中提取出若干感兴趣区域(region of 
interest , RoI) , 然 后 使 用 一 个 卷 积 神 经 网 络 (convolutional neural 
network,CNN)分别处理每一个感兴趣区域,提取特征,最后用一个 
支持向量机来完成最终的分类,如图1-1所示。 
图1-1 R-CNN检测流程 
通常,搜索算法会先给出约2000个感兴趣区域,然后交给后续的 
CNN去分别提取每一个感兴趣区域的特征,不难想象,这一过程会十 
分耗时。为了解决这一问题,在R-CNN工作的基础上,先后诞生了 
Fast R-CNN(6)和Faster R-CNN(7)这两个工作,如图1-2所示,迭代改进 
了R-CNN这一检测框架的各种弊端,不断完善R-CNN “先提取,后识 
别”的检测范式。这一检测范式后被称为“两阶段”(two-stage)检测,即 
先提取出可能包含目标的区域,再依次对每个区域进行识别,最后 
经过处理得到最终的检测结果。 
 
 
 图1-2 R-CNN家族图谱 
而在2015年,又一个革命性的工作—YOLO(You Only Look Once)(8) 
问世。不同于R-CNN的两阶段检测范式,YOLO的作者团队认为,提 
取候选区域(定位)和逐一识别(分类)完全可由一个单独的网络来 
同时完成,无须分成两个阶段,不需要对每一个特征区域进行依次分 
类,从而能够减少处理过程中的大量冗余操作,如图1-3所示。 
图1-3 YOLOv1检测流程 
显然,在这一技术理念下,YOLO只需对输入图像处理一次,即 
可获得最终的检测结果,因而YOLO在检测速度上具有天然的优势。 
YOLO所采用的这种端到端的检测方式将定位和分类耦合在一起,同 
步完成,因此,这类工作被称为“单阶段”(one-stage)检测。显然,相较 
 
 
 于以R-CNN为代表的两阶段检测范式,YOLO这类单阶段检测框架理 
应更加高效、简洁。 
在这样的设计理念下,YOLO凭借着其在TITAN X型号的GPU上以 
每秒处理超过40张图像的检测速度(即40 FPS)超越了当时所有的通 
用目标检测器。尽管YOLO的检测性能要略逊于当时最新的Faster R 
CNN检测器,但其显著的速度优势使其成为一个可以满足实时检测需 
求的通用目标检测器,许多研究者看到了这一检测器背后所蕴含的性 
能潜力和研究价值。此后,YOLO以其在检测速度和模型架构上的显 
著优势一鸣惊人,掀开了目标检测领域的新篇章。 
也正是在YOLO框架大火之后,目标检测领域正式诞生了以下两 
大流派: 
●以R-CNN为代表的two-stage流派; 
●以YOLO为首的one-stage流派。 
通 常 情 况 下 , two-stage 框 架 往 往 检 测 精 度 较 高 而 检 测 速 度 却 较 
慢,one-stage框架则恰恰相反,往往检测精度较低但检测速度较快。 
在很多计算机视觉任务中,精度和速度总是矛盾的,因而促使研究者 
尝试从二者的矛盾中寻求一个较为平衡的解决方案。随着后续研究者 
们的不断思考、探索和尝试,如今的one-stage检测框架几乎兼具了性 
能和速度两方面的优势,实现了极为出色的性能上的平衡。 
纵观科学发展史,“大道至简”和“奥卡姆剃刀”原理往往是有效 
的。也许正因如此,广大研究者和工程师才更加青睐one-stage框架, 
投入更多的科研精力和资源去优化这一框架,使其得到了长足的发 
展。这一点从每年发表在计算机视觉顶级会议的目标检测工作中可见 
 
 
 一 斑 , one-stage 框 架 相 关 工 作 占 据 了 很 大 的 比 重 , 如 SSD(9) 、 
RetinaNet(10) 和 FCOS(11) 等 。 近 来 , 这 一 套 框 架 又 由 方 兴 未 艾 的 基 于 
Transformer(12)的DETR系列(13)(14)做了一次大幅度的革新。可以认为, 
目标检测的one-stage框架以其更简洁、更具潜力等优势已经成为这一 
领域的主流框架。 
因此,在入门目标检测领域时,学习one-stage框架相关工作是更 
为契合主流的选择。 
注意: 
尽管基于深度学习的方法成为了这一领域的主流,但我们不难发 
现,基于深度学习的方法仍旧延续着传统方法的工作框架,即先提取 
特征,再进行分类和定位。只不过这两部分现在都被神经网络代替 
了,无须人工设计。因此,虽然传统计算机视觉方法在许多方面被基 
于深度学习的方法所超越,但其思想仍值得我们借鉴和思考。 
1.2 目标检测网络框架概述 
从深度学习时代开始,目标检测网络的框架也逐渐地被确定了下 
来。一个常见的目标检测网络往往可以分为三大部分:主干网络 
(backbone network) 、 颈 部 网 络 (neck network) 和 检 测 头 (detection 
head),如图1-4所示。 
图1-4 目标检测网络的组成 
● 
主干网络。主干网络是目标检测网络中最核心的部分,其关键 
作用就是提取输入图像中的高级特征,减少图像中的冗余信息,以便 
 
 
 于后续的网络去做深入的处理。在大多数情况下,主干网络选择的成 
败对检测性能的影响是十分巨大的。 
● 
颈部网络。颈部网络的主要作用是将由主干网络输出的特征进 
行二次处理。其整合方式有很多,最为常见的就是特征金字塔网络 
(feature pyramid network,FPN)(15),其核心是将不同尺度的特征进行充 
分的融合,以提升检测器的多尺度检测的性能。除此之外,还有很多 
单独的、可即插即用的模块,如RFB(16)、ASPP(17)和YOLOv4(18)所使用 
的SPP模块等,这些模块都可以添加在主干网络之后,以进一步地处 
理和丰富特征信息,扩大模型的感受野。 
● 
检测头。检测头的结构相对简单,其主要作用就是提取类别信 
息和位置信息,输出最终的预测结果。在某些工作里,检测头也被称 
为解码器(decoder),这种称呼不无道理,因为检测头的作用就是从前 
两个部分所输出的特征中提取并预测图像中的目标的空间位置和类 
别,它相当于一个解码器。 
总而言之,从宏观角度来看,几乎任何一个检测器都可以分为以 
上三大部分,如此的模块化思想也有助于我们为其中的每一部分去做 
改进和优化,从而提升网络的性能。接下来,我们再从微观角度来依 
次讲解目标检测网络的这三大部分。 
1.3 目标检测网络框架浅析 
在1.2节中,简单介绍了当前常见的目标检测网络框架的基本部 
分:主干网络、颈部网络和检测头。本节将详细介绍每一个组成部 
分。 
1.3.1 主干网络 
 
 
 为了检测出图像中目标的类别和位置,我们会先从输入的图像中 
提取出必要的特征信息,比如HOG特征。不论是基于传统方法还是深 
度学习方法,提取特征这一步都是至关重要的,区别只在于提取的方 
式。然后利用这些特征去完成后续的定位和分类。在深度学习领域 
中,由于CNN已经在图像分类任务中被证明具有强大的特征提取能 
力,因而选择CNN去处理输入图像,从中提取特征是一个很自然、合 
理的做法。在目标检测框架中,这一部分通常被称为主干网络。很多 
时候,一个通用目标检测器的绝大部分网络参数和计算都包含在了主 
干网络中。 
由于深度学习领域本身的“黑盒子”特性,很多时候我们很难直观 
地去理解CNN究竟提取出了什么样的特征,尽管已经有一些相关的理 
论分析和可视化工作,但对于解开这层面纱还是远远不够的。不过, 
已经有大量的工作证明了这一做法的有效性。 
从某种意义上来说,如何设计主干网络是至关重要的,这不仅因 
为主干网络占据了一个目标检测器的计算量和参数量的大部分,还因 
为提取的特征的好坏对后续的分类和定位有着至关重要的影响。在应 
用于目标检测任务之前,深度学习技术就已经在图像分类任务中大放 
光彩。尤其是在VGG(19)和ResNet(20)工作问世后,图像分类任务几乎达 
到了顶峰—从不再举办ImageNet比赛这一点就可见一斑。虽然这个领 
域还在陆陆续续地出现新的工作,诞生了很多出色的主干网络,但当 
年百花齐放的盛况已成为历史。 
深度学习技术能够如此出色地完成图像分类任务,充分表明了这 
一新技术确实有着不同凡响的特征提取能力。另外,由于ImageNet是 
图像分类领域中最大的数据集,包含百万张自然图像,因而许多研究 
者认为经过该数据集训练后的CNN已经充分学会了如何提取“有用”的 
 
 
 特征,这对于包括目标检测、语义分割、实例分割等在内的下游任务 
是有益的。以目标检测任务为例,尽管图像分类任务和目标检测任务 
有着明显区别,但二者又有着一定的相似性:都需要对图像中的目标 
进行分类。这种相似性为研究者们带来了这样的启发:能否将训练好 
的分类网络(如ResNet等)迁移到目标检测网络中呢?在经过这样 
的思考后,一些研究者们便将在ImageNet数据集上训练好的分类网络 
做一些适当的调整—去掉最后的global avgpooling层和Softmax层后,便 
将其作为目标检测网络中的主干网络,并使用在ImageNet数据集上训 
练好的参数作为主干网络的初始化参数,即“预训练权重”。这一模式 
也就是后来所说的“ImageNet pretrained”。 
大量的工作已经证明,这一模式是十分有效的,可以大大加快目 
标检测网络在训练过程中的收敛速度,也可以提升检测器的检测性 
能。虽然主干网络起初并不具备“定位”的能力,但依靠后续添加的检 
测头等其他网络层在目标检测数据集上的训练后,整体的网络架构便 
兼具了“定位”和“分类”两大重要能力,而主干网络所采用的预训练权 
重参数又大大加快了这一学习过程。自此,许多目标检测模型都采用 
了这样一套十分有效的训练策略。 
然而,2019年的一篇重新思考经过ImageNet预训练的模式的论文 
(21)以大量的实验数据证明了即使不加载预训练权重,而是将主干网 
络的参数随机初始化,也可以达到与之相媲美的性能。但为了达到 
此目的,需要花更多的时间来训练网络,且数据集本身也要包含足够 
多的图像和训练标签,同时,对于数据预处理和训练所采用的超参数 
的调整也带来了一定的挑战。这样的结论似乎是很合理的,正所谓“天 
下没有免费的午餐”(早餐和晚餐也不免费),既然设计了一个主干网 
络,若是不想在ImageNet数据集上预训练,那么自然就要在目标检测 
 
 
 数据集上投入更多的“精力”。因此,目前经过ImageNet预训练的模式 
仍旧是主流,后续的研究者们还是会优先采用这一套训练模式,来降 
低研究的时间成本和计算成本。 
最后,简单介绍5个常用的主干网络模型。 
●VGG 网 络 。 常用的VGG网络(22)是VGG-16网络。由于其结构富 
有规律性,由简单的卷积块堆叠而成,因此备受研究者们的青睐。 
VGG网络也打开了“深度”卷积神经网络的大门,是早期的深度卷积神 
经网络之一。早期的Faster R-CNN和SSD都使用了这一网络作为主干 
网络。 
●Re sNe t 网 络 。 ResNet(23)是当下最主流、最受欢迎的网络之一。 
常用的ResNet是ResNet-50和ResNet-101。ResNet的核心理念是“残差连 
接”(residual connection),正是在这一理念下,此前令许多研究者困扰 
的“无法训练大模型”的问题得到了有效的解决。自ResNet工作之后, 
如何设计一个深度网络已经不再是难题,并且这一系列的工作已在多 
个计算机视觉领域中大放光彩,其残差思想也启发了其他领域的发 
展。 
●DarkNet网络。DarkNet系列主要包含DarkNet-19和DarkNet-53两 
个网络,它们分别来源于YOLOv2(24)和YOLOv3(25)这两个工作。但由 
于 DarkNet 本 身 是 很 小 众 的 深 度 学 习 框 架 , 且 这 两 个 网 络 均 是 由 
DarkNet框架实现的,因此使用这两个主干网络的频率相对较低。 
●Mobile Ne t 网 络 。 MobileNet系 列 的 工 作 由 谷 歌 公 司 团 队 一 手 打 
造,目前已经推出了MobileNet-v1(26)、MobileNet-v2(27) 和 MobileNet 
v3(28) 这 3 个 版 本 。 MobileNet 系 列 的 核 心 技 术 点 是 逐 深 度 卷 积 
 
 
 (depthwise convolution),这一操作也是后来绝大多数轻量型CNN的核 
心操作。相较于前面介绍的以GPU为主要应用平台的大型主干网络, 
MobileNet着眼于低性能的移动端平台,如手机、无人机和其他嵌入式 
设备等。 
●ShuffleNet网络。ShuffleNet系列由旷视科技公司团队一手打造, 
目前已经推出了ShuffleNet-v1(29)和ShuffleNet-v2(30)两个版本,同样是针 
对低性能的移动端平台设计的轻量型网络,其核心思想是通道混合 
(channel shuffle),其目的是通过将每个通道的特征进行混合,弥补逐 
深度卷积无法使不同通道的信息进行交互的缺陷。 
还有很多出色的主干网络,这里就不一一列举了。有关主干网络 
的更多介绍,感兴趣的读者可自行查阅相关资料。 
1.3.2 颈部网络 
1.3.1节已经介绍了目标检测模型中的主干网络,其作用可以用一 
句话来总结:提取图像中有用的信息。当然,“有用的信息”是一种笼 
统的描述,尚不能用精确的数学语言来做定量的解释。另外,由于主 
干网络毕竟是从图像分类任务中迁移过来的,在大多数情况下,这些 
网络的设计很少会考虑到包括目标检测、语义分割等下游任务,它们 
提取的特征也就有可能不太适合目标检测任务。因此,在主干网络处 
理完毕之后,仍有必要去设计一些额外的模块来对其特征做进一步的 
处理,以便适应目标检测任务。因为这一部分是在主干网络之后、检 
测头之前,因此被形象地称为颈部网络。 
相较于主干网络常使用ImageNet预训练参数,颈部网络的参数的 
初始化没有太多需要解释的。既然颈部网络的作用是整合主干网络的 
 
 
 信息,可供研究者们自由发挥的空间也就大得多,很多颈部网络被相 
继提了出来。这里我们介绍两种常见的颈部网络。 
● 
特 征 金 字 塔 网 络 。 特征金字塔网络(feature pyramid network, 
FPN)(31)是目前目标检测领域最有名的结构之一,几乎是当下目标检测 
网络的标准配置,其多尺度特征融合与多级检测思想影响了后续许多 
目标检测网络结构。FPN认为网络中的不同大小的特征图所包含的信 
息是不一样的,即浅层特征图包含更多的位置信息,且分辨率较高, 
感受野较小,便于检测小物体;而深层特征图包含更多的语义信息, 
且分辨率较低,感受野较大,便于检测大物体。因此,FPN设计了一 
种自顶向下的融合方式,将深层的特征不断融合到浅层特征中,通过 
将浅层与深层的信息进行融合,可以有效地提升网络对不同尺度物体 
的检测性能。图1-5展示了特征金字塔网络的结构。 
图1-5 特征金字塔网络的结构 
● 
空 间 金 字 塔 池 化 模 块 。 虽 然 最 早 的 空 间 金 字 塔 池 化 (spatial 
pyramid pooling,SPP)模块是由Kaiming He团队(32)在2015年提出的,但 
在目标检测任务中常用的SPP模块则是由YOLOv3工作的作者团队所设 
 
 
 计的SPP结构,包含4条并行的分支,且每条分支用了不同大小的池化 
核。SPP结构可以有效地聚合不同尺度的显著特征,同时扩大网络的 
感受野,进而提升模型的性能。SPP模块是一个性价比很高的结构, 
被广泛地应用在YOLOv4、YOLOv6和YOLOv7等工作中。图1-6展示了 
SPP模块的网络结构。 
还有很多出色的颈部网络,这里就不一一展开细说了,感兴趣的 
读者可以自行搜索学习。 
图1-6 SPP模块的网络结构 
1.3.3 检测头 
 
 
 当一张输入图像经过主干网络和颈部网络两部分的处理后,得到 
的特征就可以用于后续的检测了。大多数的检测框架所采用的技术路 
线都是在处理好的特征图上,通过部署数层卷积来完成定位和分类。 
对于这一部分,由于其目的十分明确,而前面两部分已经做了充分的 
处理,因此检测头的结构通常十分简单,没有太多可发挥的空间,图 
1-7展示了RetinaNet的网络结构,RetinaNet采用了“解耦”结构的检测 
头,这是当前目标检测网络中常用的一种检测头结构,它由两条并行 
的分支组成,每一条分支都包含若干层普通卷积、非线性激活函数以 
及用于最终预测的线性卷积层。 
图1-7 RetinaNet的网络结构(摘自论文(33)) 
1.4 小结 
本章主要介绍了目标检测领域的发展简史,并着重地讲解了当前 
主流目标检测网络的组成:主干网络、颈部网络和检测头。通过这一 
章的学习,读者可以基本了解什么是目标检测,摸清这一领域发展的 
基本脉络,尤其是能够建立起对当前的主流目标检测网络框架的宏观 
认识。这一点是十分重要的,很多后续的改进工作都是从这三部分出 
发,提出各种修改。同时,这种模块化的思想也有助于我们去学习新 
的目标检测知识和搭建一个完整的目标检测网络框架。 
 
 
 (1) Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object Detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USA: IEEE Press, 2016: 779-788. 
(2) Redmon J, Farhadi A. YOLO9000: Better, Faster, Stronger[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USA: IEEE Press, 2017: 65176525. 
(3) Redmon J, Farhadi A. Yolov3: An Incremental Improvement[J]. arXiv preprint arXiv:1804.02767, 2018. 
(4) Girshick R, Donahue J, Darrell T, et al. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Columbus, OH, USA: IEEE Press, 2014: 580-587. 
(5) Everingham M, Van Gool L, Williams C, et al. Pascal Visual Object Classes Challenge Results[J]. Pascal Network, 2005, 1(6):1-45. 
(6) Dollár P, Appel R, Belongie S, et al. Fast Feature Pyramids for Object Detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36(8): 1532-1545. 
(7) Ren S, He K, Girshick R, et al. Faster R-cnn: Towards Real-time Object Detection with Region Proposal Networks[J]. Advances in Neural Information Processing Systems, 2015, 28. 
(8) Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object Detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USA: IEEE Press, 2016: 779-788. 
(9) Liu W, Anguelov D, Erhan D, et al. SSD: Single Shot Multibox Detector[C]//Proceedings of the European Conference on Computer Vision (ECCV). Springer Cham, 2016: 21-37. 
(10) Lin T Y, Goyal P, Girshick R, et al. Focal Loss for Dense Object Detection[C]//Proceedings of the IEEE International Conference on Computer Vision. Venice, Italy: IEEE Press, 2017: 2980-2988. 
(11) Tian Z, Shen C, Chen H, et al. FCOS: A Simple and Strong Anchor-free Object Detector[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020, 44(4): 1922-1933. 
(12) Vaswani A, Shazeer N, Parmar N, et al. Attention is All You Need[J]. Advances in Neural Information Processing Systems, 2017, 30:1-11. 
(13) Carion N, Massa F, Synnaeve G, et al. End-to-end Object Detection with Transformers[C]//Proceedings of the European Conference on Computer Vision (ECCV). Springer Cham, 2020: 213-229. 
(14) Zhu X, Su W, Lu L, et al. Deformable Detr: Deformable Transformers for End-to-end Object Detection[J]. arXiv preprint arXiv:2010.04159, 2020. 
(15) Lin T Y, Dollár P, Girshick R, et al. Feature Pyramid Networks for Object Detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USA: IEEE Press, 2017: 2117-2125. 
 
 
 (16) Liu S, Huang D. Receptive Field Block Net for Accurate and Fast Object Detection[C]//Proceedings of the European Conference on Computer Vision (ECCV). Springer Cham, 2018: 385-400. 
(17) Chen L C, Papandreou G, Kokkinos I, et al. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 40(4): 834-848. 
(18) Bochkovskiy A, Wang C Y, Liao H Y M. Yolov4: Optimal Speed and Accuracy of Object Detection[J]. arXiv preprint arXiv:2004.10934, 2020. 
(19) Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-scale Image Recognition[J]. arXiv preprint arXiv:1409.1556, 2014. 
(20) He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USA: IEEE Press, 2016: 770-778. 
(21) He K, Girshick R, Dollár P. Rethinking ImageNet Pre-Training[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. Seoul, Korea (South): IEEE Press, 2019: 4917-4926. 
(22) Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-scale Image Recognition[J]. arXiv preprint arXiv:1409.1556, 2014. 
(23) He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USA: IEEE Press, 2016: 770-778. 
(24) Redmon J, Farhadi A. YOLO9000: Better, Faster, Stronger[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USA: IEEE Press, 2017: 65176525. 
(25) Redmon J, Farhadi A. Yolov3: An Incremental Improvement[J]. arXiv preprint arXiv:1804.02767, 2018. 
(26) Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications[J]. arXiv preprint arXiv:1704.04861, 2017. 
(27) Sandler M, Howard A, Zhu M, et al. MobileNetV2: Inverted Residuals and Linear Bottlenecks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USA: IEEE Press, 2018: 4510-4520. 
(28) Howard A, Sandler M, Chu G, et al. Searching for MobileNetV3[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. Seoul, Korea (South): IEEE Press, 2019: 1314-1324. 
(29) Zhang X, Zhou X, Lin M, et al. ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USA: IEEE Press, 2018: 6848-6856. 
 
 
 (30) Ma N, Zhang X, Zheng H T, et al. ShufflenetV2: Practical Guidelines for Efficient CNN Architecture Design[C]//Proceedings of the European Conference on Computer Vision (ECCV). Springer Cham, 2018: 116-131. 
(31) Lin T Y, Dollár P, Girshick R, et al. Feature Pyramid Networks for Object Detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USA: IEEE Press, 2017: 2117-2125. 
(32) He K, Zhang X, Ren S, et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37(9): 19041916. 
(33) Lin T Y, Goyal P, Girshick R, et al. Focal Loss for Dense Object Detection[C]//Proceedings of the IEEE International Conference on Computer Vision. Venice, Italy: IEEE Press, 2017: 2980-2988. 
 
 
 第2章 常用的数据集 
迄今为止,在机器学习和深度学习领域,数据本身对一个算法的 
好坏依旧起着至关重要的作用—数据的有无、数据量的大小以及数据 
的质量都会直接影响一个算法的实际性能。在大多数时候,可能算法 
本身在理论层面是很优秀的,但在处理糟糕的数据时,再优秀的算法 
性能也要大打折扣。 
如今,社会发展已进入大数据时代,这就意味着数据获取会变得 
更加容易,而这在一定程度上也大力推动了深度学习的发展。例如早 
期的图像分类任务,在李飞飞团队公布了庞大的ImageNet数据集并举 
办了相关比赛后,吸引了大量的研究团队,充分利用ImageNet数据集 
所包含的百万级的数据来构建强大的图像分类器,越来越多的优秀算 
法应运而生,为后续诸多的下游任务做足了技术储备。而在目标检测 
领域,正是在MS COCO数据集(1)被公布后,促进了目标检测领域的发 
展,使得越来越多的检测算法被部署到实际场景中,从而解决实际任 
务中的问题。诸如此类的例子还有很多,总结起来,就是深度学习的 
每一条分支的发展都离不开一个庞大的、高质量的、场景复杂的、具 
有挑战性的数据集。因此,在步入目标检测领域之前,了解该领域常 
用的数据集是十分必要的。 
当然,有时一个数据集可能会服务于多个任务,因此会存在不同 
形式的数据标签。为了配合本书,我们只介绍数据集中的部分内容。 
倘若读者对数据集的其他部分也感兴趣,不妨前往数据集的官方网站 
查看更多的信息。 
2.1 PASCAL VOC数据集 
 
 
 PASCAL VOC(PASCAL Visual Object Classes)数据集(2)是目标检测 
领域中的经典数据集,该数据集中共包含20类目标,一万多张图像。 
该 数 据 集 中 最 为 常 用 的 是 VOC2007 和 VOC2012 数 据 集 。 通 常 , 
VOC2007的trainval数据与VOC2012的trainval数据会被组合在一起用于 
训练网络,共包含16551张图像,而VOC2007的test数据则作为测试集 
来验证网络的性能,共包含4952张图像。当然,还有其他数据组合方 
式,但由于这一组合最常见,因此读者只需了解这一种方式。如果读 
者对更多的组合使用方式感兴趣,可自行查阅相关资料。图2-1展示了 
VOC数据集的一些实例。 
图2-1 VOC数据集的一些实例 
读者可以登录PASCAL VOC官方网站下载VOC2007和VOC2012数 
据集,或者使用本书配套源代码的README文件中的下载链接来获得 
该数据集。为了配合后续的代码实现,作者推荐各位读者使用本书所 
提供的下载链接去下载数据集,以便和后续的实践章节相对应,以减 
少一些不必要的麻烦。 
 
 
 以作者提供的数据集下载链接为例,读者会看到一个名为 
VOCdevkit.zip的压缩包,下载后对其进行解压,即可获得VOC2007和 
VOC2012数据集。我们将会在实践章节中使用VOC数据集,因此强烈 
建议读者提前将其下载下来。 
以VOC2007数据集为例,打开VOC2007文件夹,我们会看到几个 
主要文件,如图2-2所示。其中,JPEGImages文件夹下包含数据集图 
像,Annotations文件夹下包含每张图像的标注文件。标注文件的命名 
与图像的命名相同,仅在后缀上有差别。而对于带有“Segmentation”字 
眼的文件,我们暂时不需要考虑,因为它们是用于语义分割和实例分 
割任务的,不在本书的讨论范畴内。 
随后,进入ImageSets文件夹,我们主要关心的是其中的Layout文 
件夹,因为该文件夹下包含4个后面会常用到的txt文档,其作用是用来 
划分数据集,以便分别用于训练和测试。图2-3展示了这4个txt文档。 
图2-2 VOC2007文件夹中的主要文件 
 
 
 图2-3 ImageSets/Layout文件夹下的4个txt文档 
2.2 MS COCO数据集 
MS COCO(3)(Microsoft Common Objects in Context,简称COCO) 
数据集是微软公司于2014年公布的大型图像数据集,包含诸如目标检 
测、图像分割、实例分割和图像标注等丰富的数据标签,是计算机视 
觉领域最受关注、最为重要的大型数据集之一。图2-4展示了COCO数 
据集的实例。 
图2-4 COCO数据集的实例 
相较于VOC数据集,COCO数据集包含更多的图像,场景更加丰 
富和复杂,用于训练的图像数量高达11万余张,包含80个类别,远远 
大于VOC数据集。COCO数据集不仅所包含的图像更多,而且更重要 
的是,COCO数据集的图像具有更加贴近自然生活、场景更加复杂、 
目标尺度多变、光线变化显著等特点,这些特点不仅使得COCO数据 
 
 
 集极具挑战性,同时也赋予了检测算法良好的泛化性能,使得在该数 
据集上训练好的检测器可以被部署到实际场景中。 
在COCO数据集中,目标物体分别被定义为小物体、中物体和大 
物体这3类,其中小物体是指像素面积小于32×32像素的目标;中物体是 
指像素面积介于32×32至96×96像素之间的目标;其余的为大物体。 
迄今为止,COCO数据集的小目标检测依旧是一大难点。图2-5展 
示了一张COCO数据集的实例,读者能否凭借肉眼分辨出处于图像中 
间位置的那条犬和旁边的小孩子呢?不难看出,检测这种外观不够显 
著的小目标是有很大的难度的,因为相较于大目标,小目标包含的像 
素往往很少,所能提供的信息也就很少,同时,大多数检测器的主干 
网络都会存在降采样操作,如步长为2的最大池化层或者卷积,这些降 
采样操作会丢失小目标的特征,这些因素都会导致网络很难充分地学 
习到小目标的信息。另外,也正是由于小目标的像素量远小于大目 
标,因此在训练网络的过程中大目标的信息会占据主导位置,也不利 
于网络学习小目标的信息。因此,针对小物体的目标检测一直以来都 
是该领域的研究热点之一。 
 
 
 图2-5 COCO数据集的实例 
因为COCO数据集具有更大的数据规模,挑战性更高,数据内容 
也更贴近于实际环境,所以在数据公布后,目标检测领域的发展迅 
速,经由COCO数据集训练出来的模型也可以被更好地应用在实际环 
境中,促进了相关部署工作的开展。 
不过,也正是由于COCO数据集包含大量的图像,使得训练网络 
所花费的时间成本也会更高,因此,为了加快训练的进度、缩减时间 
成本,需要研究者具备更多的算力硬件(如GPU),以便使用分布式 
训练来缩短训练时间。倘若不具备足够多的GPU,就只能花费更多的 
时间。因此,读者在本书第3章至第7章中学习YOLOv1至YOLOv3时, 
不要求读者必须使用COCO数据集,仅使用较小的VOC数据集就足够 
了,但在第8章后,希望读者能够准备COCO数据集,通过编写相关的 
代码,即便暂不具备训练的条件,运行由作者提供的模型权重也会有 
 
 
 不菲的收获,且能够加深对COCO数据集的认识,这也便于开展日后 
的相关研究工作。读者可以前往COCO官方网站或者使用本书配套源 
代码中提供的下载文件来下载COCO数据集。 
2.3 小结 
本章介绍了目标检测领域中常见的PASCAL VOC数据集和COCO 
数据集,其中,COCO数据集更具挑战性,是目标检测领域中最为重 
要的数据集之一。就本书的目标而言,了解VOC数据集即可。在后续 
章节的代码实现中,我们将会频繁使用VOC数据集。同时,也希望读 
者在本书之外去了解一些有关COCO数据集的资料,在本书第5章之后 
的章节中,COCO数据集也将被频繁地使用。 
(1) Lin T Y, Maire M, Belongie S, et al. Microsoft Coco: Common Objects in Context[C]//Proceedings of the European Conference on Computer Vision (ECCV). Springer Cham, 2014: 740-755. 
(2) Everingham M, Van Gool L, Williams C, et al. Pascal Visual Object Classes Challenge Results[J]. Pascal Network, 2005, 1(6):1-45. 
(3) Lin T Y, Maire M, Belongie S, et al. Microsoft Coco: Common Objects in Context[C]//Proceedings of the European Conference on Computer Vision (ECCV). Springer Cham, 2014: 740-755. 
 
 
 第2部分 学习YOLO框架 
第3章 YOLOv1 
本章将详细介绍one-stage流派的开山之作YOLOv1。尽管YOLOv1 
已经是2015年的工作了,以现在的技术眼光来看,YOLOv1有着太多 
不完善的地方,但正是这些不完善之处,为后续诸多先进的工作奠定 
了基础。正所谓“温故知新”,虽然YOLOv1已经过时了,但是它为one 
stage架构奠定了重要的基础,其中很多设计理念至今还能够在先进的 
目标检测框架中有所体现。并且,也正是因为YOLOv1略显久远,它 
的架构自然也是简单、易于理解的,尚无复杂的模块,这对于初学者 
来说是极为友好的,就好比在学习高等数学之前,我们也总是要先从 
简单乘法知识学起,而非一上来就面对牛顿—莱布尼茨公式。充分掌 
握 YOLOv1 的 基 本 原 理 对 于 我 们 后 续 学 习 YOLOv2 、 YOLOv3 和 
YOLOv4都会有极大的帮助,所以,希望每一位读者都能够重视本章 
的内容。 
3.1 YOLOv1的网络结构 
作为one-stage流派的开山之祖,YOLOv1以其简洁的网络结构和在 
GPU上的实时检测速度两方面的优势一鸣惊人,开辟了完全不同于R 
CNN的新技术路线,引发了目标检测领域的巨大变革。 
如果以现在的眼光来看待YOLOv1,不难发现其中有诸多设计上 
的缺陷,但YOLOv1是这一派系发展之源头,在它被提出的那一年, 
YOLOv1的特色吸引了诸多研究者的注意,他们看出了其中所蕴含的 
研究潜力和研究价值,自此之后,大批one-stage框架被提出,一代又 
 
 
 一代地、前赴后继地优化这一全新的框架,使其成为了目标检测领域 
发展的主流,对于这一切的发展,YOLOv1功不可没。 
YOLOv1的思想十分简洁:仅使用一个卷积神经网络来端到端地 
检 测 目 标 。 这 一 思 想 是 对 应 当 时 主 流 的 以 R-CNN系 列 为 代 表 的 two 
stage流派。从网络结构上来看,YOLOv1仿照GoogLeNet网络(1)来设计 
主干网络,但没有采用GoogLeNet的Inception模块,而是使用串联的 
卷积和 卷积所组成的模块,所以它的主干网络的结构非常 
简单。图3-1展示了YOLOv1的网络结构。 
图3-1 YOLOv1的网络结构(摘自论文[1]) 
在YOLO所处的年代,图像分类网络都会将特征图展平(flatten), 
得到一个一维特征向量,然后连接全连接层去做预测。YOLOv1继承 
了这个思想,将主干网络最后输出的特征图 调整为一维 
向量 ,其中 。然后,YOLOv1部署若干全连接层 
(fully connected layer)来处理该特征向量。根据图3-1中的网络结构,这 
里的 、 和 分别是7、7和1024。再连接一层包含4096个神经元 
的全连接层做进一步处理,得到特征维度为4096的特征向量。最后, 
 
 
 部署一个包含1470个神经元的全连接层,输出最终的预测。不过,考 
虑到目标检测是一个空间任务,YOLOv1又将这个特征维度为1470的 
输出向量转换为一个三维矩阵 ,其中, 是这个三维 
矩阵的空间尺寸,30是该三维矩阵的通道数。在深度学习中,通常三 
维及三维以上的矩阵都称为张量(tensor)。 
这里需要做一个简单的计算,从特征图被展平,再到连接4096的 
全连接层时,可以用以下算式很容易估算出其中的参数量: 
(3-1) 
显然,仅这一层的参数量就已经达到了十的八次方级,虽然如此 
之多的参数并不意味着模型推理速度一定会很慢,但必然会对内存产 
生巨大的压力。从资源占用的角度来看,YOLOv1的这一缺陷是致命 
的。显然,这一缺陷来自网络结构本身,因此,这一点也为YOLO的 
后续改进埋下了伏笔。当然,这个问题并不是YOLOv1本身的问题, 
而是那个时代做图像分类任务的通病。尽管后来这一操作被全局平均 
池化 (global average pooling)所取代,但全局平均池化并不太适合目标 
检测这一类对空间局部信息较为敏感的任务。即便如此,YOLOv1还 
是以其快速的检测速度与不凡的检测精度而引人注目。 
图3-2展示了YOLOv1在VOC数据集上与其他模型的性能对比。我 
们主要关注其中的两个指标,一个是衡量模型检测性能的平均精度 
(mean average precision,mAP),另一个是衡量模型检测速度的每秒帧 
数(frames per second,FPS)。迄今为止,mAP是目标检测领域常用的性 
能评价指标,其数值越高,意味着模型的检测性能越好。简单地说, 
mAP的计算思路是先计算每个类别的AP,然后把所有类别的AP加和并 
 
 
 求平均值。至于计算AP的具体操作,读者暂不必关注,本书会提供相 
关的计算代码供读者使用,在入门阶段,我们只需关注技术本身,不 
要陷入细节的漩涡之中。FPS是指网络每秒可以处理多少张图像(从 
输入一张图像到输出图像中的目标的检测结果)。FPS的数值越大, 
意味着模型的检测速度越快。 
图3-2 YOLOv1在VOC数据集上与其他模型的性能对比(摘自论文(2)) 
尽管YOLOv1在mAP上略逊于Faster R-CNN,但YOLOv1的速度更 
快。从实用性的角度来说,速度有时是一个重要的指标,它往往会决 
定该算法能否被部署到实际场景中去满足实际的需求。毕竟,不是所 
有情况下都有高算力的GPU来支持超大模型的计算。在作者看来,一 
个良好的科研思路就是辩证地看待每一项技术的优劣点,而非固守 
着“唯SOTA(state-of-the-art)论”的极端理念,那样只会让学术研究误入 
歧途。在那个时候,正是因为有很多研究者看到了YOLOv1所蕴含的 
 
 
 研 究 价 值 和 研 究 潜 力 , 才 有 了 后 来 的 one-stage 框 架 的 蓬 勃 发 展 。 如 
今,one-stage框架的相关工作已经在速度和精度之间取得了良好的平 
衡,成为了目标检测领域的主流技术路线。对此,YOLOv1的作者团 
队功不可没。 
3.2 YOLOv1的检测原理 
现在,我们从网络结构的角度来研究一下YOLOv1到底是如何工 
作的。 
从整体上来看,YOLOv1网络接收一张空间尺寸为 的 
RGB图像,经由主干网络处理后输出一个空间大小被降采样64倍的特 
征图,记作 。该特征图 是一个三维张量, 和 与 
输入图像的一般数学关系如下。 
(3-2) 
其中, 是网络的输出步长,其值通常等于网络的降采样倍 
数 。 依 据 YOLOv1 论 文 所 给 出 的 配 置 , 这 里 的 和 均 为 448 , 
为64,那么就可以很容易地算出来 和 均为7,即主干网络 
输出的特征图的高和宽均为7。而 是输出特征图的通道数,依据 
YOLOv1的设置, 为1024。 
随后,特征图 再由若干全连接层处理以及一些必要的维度转换 
的操作后,得到最终的输出 。我们可以把 想象成一个大 
小为 的立方体, 这个维度可以想象成一个网格 
 
 
 (grid),每一个网格 都是一个特征维度为30的向量,YOLOv1 
的输入与输出如图3-3所示。 
图3-3 YOLOv1的输入与输出 
在每一个网格 处,这个长度为30的向量都包含了两个边 
界框的参数以及总数为20的类别数量(VOC数据集上的类别数量), 
其中每一个边界框都包括一个置信度 (confidence)、边界框位置参 
数 , 表示边界框的中心点相较于网格左上角点的偏移 
量 以及边界框的宽和高 。将这些参数的数量加起来便可以得 
到前面所说的“30”这个数字。更一般地,我们可以用公式 来 
计算输出张量的通道数量,其中, 是每个网格 处预测的边 
界框的数量, 是所要检测的目标类别的总数,对于VOC数据集来 
说, ;对于COCO数据集来说, 。 
总体来看,一张大小为 的RGB图像输入网络,经过一 
系 列 的 卷 积 和 池 化 操 作 后 , 得 到 一 个 经 过 64 倍 降 采 样 的 特 征 图 
,随后,该特征图被展平为一个一维的特征向量,再由 
若干全连接层处理,最后做一些必要的维度转换操作,得到最终的输 
 
 
 出 。之前,我们已经把 想象成一个大小为 
的立方体,而网格 包含了物体的信息,即边界框的位置参数和 
置信度。这里就充分体现了YOLOv1的核心检测思想:逐 网 格 找 物 
体。图3-4展示了YOLOv1的“网格划分”思想的实例。 
具体来说,YOLOv1输出的 的空间维度 相当于 
将输入的 的图像进行了 的网格划分。更进一步地 
说,YOLOv1的主干网络将原图像处理成 的特征图,其 
实就相当于是在划分网格,每一网格处都包含了长度为1024的特征向 
量,该特征向量包含了该网格处的某种高级特征信息。YOLOv1通过 
遍历这些网格、处理其中的特征信息来预测每个网格处是否有目标的 
中心点坐标,以及相应的目标类别。每个网格都会输出 个边界框和 
个类别置信度,而每个边界框包含5个参数(边界框的置信度和边 
界框的位置参数),因此,每个网格都会输出 个预测参数。 
后 来 , 随 着 时 代 的 发 展 , YOLOv1 的 这 种 思 想 逐 渐 演 变 为 后 来 常 说 
的“anchor-based”理念,每个网格就是“anchor”。 
 
 
 图3-4 YOLOv1的“网格划分”思想的实例 
当然,严格来讲,其实YOLOv1的这一检测理念也是从Faster R 
CNN中的区域候选网络(region proposal network,RPN)继承来的,只不 
过,Faster R-CNN只用于确定每个网格里是否有目标,不关心目标类 
别,而YOLOv1则进一步将目标分类也整合进来,使得定位和分类一 
步到位,从而进一步发展了“anchor-based”的思想。 
由于YOLOv1所接受的图像是正方形的,即宽和高是相等的,因 
此输出的网格也是正方形的。为了和论文对应,我们不妨用 表 
示 输 出 的 。 综 上 , YOLOv1 的 最 终 输 出 为 。 
 
 
 YOLOv1的整体处理流程如图3-5所示,以现在的技术视角来看,其网 
络结构十分简洁。 
图3-5 YOLOv1的处理流程 
综上所述,YOLOv1的检测理念就是将输入图像划分成 的网 
格,然后在网格上做预测。理想情况下,依据YOLOv1的设定,包含 
目标中心点的网格输出的边界框置信度很高,而不包含目标中心点的 
网格则输出的边界框置信度很低甚至为0。从这里我们也可以看出来, 
边界框的置信度的本质就是用来判断有无目标的。 
至此,我们介绍了YOLOv1的网络结构。那么,在3.3节,我们就 
要 深 入 且 详 细 地 了 解 YOLOv1 是 如 何 检 测 目 标 的 , 以 及 为 了 使 得 
YOLOv1学习到这一能力,应该如何为其制作训练正样本。 
3.3 YOLOv1的制作训练正样本的方法 
在3.1节和3.2节,我们已经了解了YOLOv1的网络结构和检测思 
想,但如何让网络尽可能充分地领会其检测思想是至关重要的,换言 
之,如何来训练这样的网络是重中之重。因此,本节将深入介绍 
YOLOv1的检测方法和训练正样本的制作方法。 
在3.2节我们已经了解到,对于YOLOv1最后输出的 , 
可以将其理解为一个 的网格,且每个网格包含30个参数—两个 
 
 
 预测边界框的置信度和位置参数(共10个)以及20个VOC数据集的类 
别的置信度。下面,我们详细介绍YOLOv1的这30个参数的定义和学 
习策略,以及如何为每一个参数制作用于训练的正样本。 
3.3.1 边界框的位置参数 、 、 、 
由于YOLOv1是通过检测图像中的目标中心点来达到检测目标的 
目的,因此,只有包含目标中心点的网格才会被认为是有物体的。为 
此,YOLOv1定义了一个“objectness”概念,表示此处是否有物体,即 
表示此网格处有物体,反之, 表示此网 
格处没有物体。 
图3-6展示了正样本候选区域概念的示例,其中,黄颜色网格表示 
这个网格是有物体的,也就是图中的犬的边界框中心点(图中的红 
点)落在了这个网格内。在训练过程中,该网格内所要预测的边界 
框,其置信度会尽可能接近1。而对于其他没有物体的网格,其边界框 
的置信度就会尽可能接近0。有物体的网格会被标记为正样本候选区 
域,也就是说,在训练过程中,训练的正样本(positive sample)只会从 
此网格处的预测中得到,而其他区域的预测都是该目标的负样本。 
 
 
 图3-6 YOLOv1的正样本候选区域概念的示例 
另外,在图3-6中,我们会发现目标的中心点相对于它所在的网格 
的四边是有一定偏移量的,这是网格划分的必然结果。因此,虽然包 
含中心点的网格可以近似代表目标的中心点位置,但仅靠网格的位置 
信息还不足以精准描述目标在图像中的位置。为了解决此问题,我们 
必须要让YOLOv1去学习这个偏移量,从而获得更加精准的目标中心 
点的位置。那么,YOLOv1是如何计算出这个中心点偏移量的呢? 
首先,对于给定的边界框,其左上角点坐标记作 ,右下角 
点坐标记作 ,显然,边界框的宽和高分别是 和 
 
 
 。随后,我们计算边界框的中心点坐标 : 
(3-3) 
通常,中心点坐标不会恰好是一个整数,而网格的坐标又显然是 
离散的整数值。假定用网格的左上角点的坐标来表示该网格的位置, 
那么,就需要对中心点坐标做一个向下取整的操作来得到该中心点所 
在的网格坐标 : 
(3-4) 
其中, 为网络的输出步长或降采样倍数,在YOLOv1中, 
该值为64。于是,这个中心点偏移量就可以被计算出来: 
(3-5) 
 
 
 在YOLOv1的论文中,这一偏移量是用符号 表示的,但 
的含义不清晰,会被误以为某个坐标,无法让人一目了然地理 
解其物理含义,因此,为了避免不必要的歧义,我们将其换成 。 
显然,两个偏移量 的值域都是 。在训练过程中,计算出的 
就将作为此网格处的正样本的学习标签。图3-7直观地展示了 
YOLOv1中的中心点偏移量的概念。 
在推理阶段,YOLOv1先用预测的边界框置信度来找出包含目标 
中心点的网格,再通过这一网格所预测出的中心点偏移量得到最终的 
中心点坐标,计算方法很简单,只需将公式(3-5)做逆运算: 
(3-6) 
 
 
 图3-7 YOLOv1中的中心点偏移量的概念 
除了边界框的中心点,YOLOv1还要输出边界框的宽和高,以确 
定边界框大小。这里,我们可以直接将目标的真实边界框的宽和高作 
为学习目标。但是,这么做的话就会带来一个问题:边界框的宽和高 
的值通常都比较大,数量级普遍为1或2,甚至3,这使得在训练期间很 
容易出现不稳定甚至发散的问题,同时,也会因为这部分所计算出的 
 
 
 损失较大,从而影响其他参数的学习。因此,YOLOv1会先对真实的 
边界框的宽和高做归一化处理: 
(3-7) 
其中, 和 分别是目标的边界框的宽和高,W和H分别是输入 
图像的宽和高。如此一来,由于边界框的尺寸不会超出图像的边界 
(超出的部分通常会被剪裁掉),归一化后的边界框的尺寸就在0~1 
范围内,与边界框的中心点偏移量的值域相同,这样既避免了损失过 
大所导致的训练发散问题,又和其他部分的损失取得了较好的平衡。 
至此,我们确定了YOLOv1中有关边界框的4个位置参数的学习策 
略。 
3.3.2 边界框的置信度 
在3.3.1节中,我们已经了解了YOLOv1如何学习边界框的位置参 
数。但有一个遗留问题:如何让网络确定中心点的位置,即我们应 
如何让网络在训练过程中去学习边界框的置信度,因为置信度直接决 
定一个网格是否包含目标的中心点,如图3-8所示。 
 
 
 图3-8 YOLOv1的边界框置信度概念图示 
只有确定了中心点所在的网格坐标 ,才能去计算边界框 
的中心点坐标 和大小 。我们已经知道,YOLOv1的 网 
格中,只有包含了目标中心点的网格才是正样本候选区域,因此,一 
个训练好的YOLOv1检测器就应该在包含目标中心点的网格所预测的 
个边界框中,其中至少有一个边界框的置信度会很高,接近于1, 
以表明它检测到了此处的目标。 
那么,问题就在于如何给置信度的标签赋值呢?一个很简单的想 
法是,将有目标中心点的网格处的边界框置信度的学习标签设置为1, 
反之为0,这是一个典型的“二分类”思想,正如Faster-RCNN中的RPN 
所做的那样。但是,YOLOv1并没有采取如此简单自然的做法,而是 
将这种二分类做了进一步的发展。 
YOLOv1不仅希望边界框的置信度表征网格是否有目标中心点, 
同时也希望边界框的置信度能表征所预测的边界框的定位精度。因 
为边界框不仅要表征有无物体,它自身也要去定位物体,所以定位得 
是否准确同样是至关重要的。而对于边界框的定位精度,通常使用交 
并比(intersection of union,IoU)来衡量。IoU的计算原理十分简单,分 
别计算出两个矩形框的交集(intersection)和并集(union),它们的比值即 
为IoU。显然,IoU是一个0~1的数,且IoU越接近1,表明两个矩形框 
的重合度越高。图3-9直观地展示了IoU的计算原理。 
 
 
 图3-9 IoU的计算原理 
巧的是,边界框的置信度也是一个0~1的数,并且我们希望预测 
框尽可能地接近真实框,即IoU尽可能地接近于1。同时,我们希望置 
信度尽可能地接近于1。不难想象,一个理想的情况就是有物体的网格 
的 置 信 度 为 1 , 预 测 框 和 真 实 框 的 IoU也 为 1 , 反 之 均 为 0 。 于 是 , 
YOLOv1就把预 测 的 边 界 框 ( 预 测 框 ) 和 目 标 的 边 界 框 ( 目 标 框 ) 
的IoU作为置信度的学习标签。由此,如何学习边界框的置信度这一 
问题也就明确了一些。接下来,详细地介绍一下YOLOv1到底是怎么 
把IoU作为边界框置信度的学习标签的。 
我 们 知 道 , YOLOv1 最 终 输 出 包 含 检 测 目 标 信 息 的 张 量 
,其中共有 个预测的边界框,但我们不必考 
虑所有的边界框,因为很多网格并没有包含目标中心点,只需要关注 
 
 
 那些有目标中心点的网格,即正样本候选区域( )。我们 
以某一个正样本候选区域为例,如图3-10所示,不妨假设 ,其 
中一个是图中的蓝框 ,另一个是绿框 ,红色的边界框则是目标的 
真实边界框。我们计算所有预测框与目标框的IoU,一共得到 个IoU 
值,假设分别是 和 。 
图3-10 正样本候选区域的边界框 (蓝框)和 (绿框)与此处的目标边界框(红框) 
 
 
 由于预测框 和真实边界框的IoU最大,即该预测框更加精确, 
因此我们就理所当然地希望这个预测框能作为正样本去学习目标的信 
息,即只保留IoU最大的蓝框 作为训练的正样本,去参与计算边界 
框的置信度损失、边界框的位置参数损失以及类别损失,并做反向 
传播,同时,它与目标框的 将作为这个边界框的置信度学习 
标签。至于另一个预测框 ,则将其标记为负样本,它不参与类别损 
失和边界框的位置参数损失的计算,只计算边界框的置信度损失, 
且置信度的学习标签为0。而对于正样本候选区域之外的所有预测 
框,都被标记为该目标的负样本,均不参与这个目标的位置参数损失 
和类别损失的计算,而只计算边界框的置信度损失,且边界框置信度 
的学习标签也为0。 
在理想情况下,预测框与目标框的IoU十分接近1,同时网络预测 
的置信度也接近1。因此,YOLOv1预测的置信度在表征此网格处是否 
有目标中心点的同时,也衡量了预测框与目标框的接近程度,即边界 
框定位的准确性。 
可以看到,一个正样本的标记是由预测本身决定的,即我们是直 
接构建预测框与目标框之间的关联,而没有借助某种先验。倘若我们 
以现在的技术视角来看待这一点,会发现YOLOv1一共蕴含了后来被 
着重发展的3个技术点: 
(1)不使用先验框(anchor box)的anchor-free技术。 
(2)将IoU引入类别置信度中的IoU-aware技术(3)(4)。 
(3)动态标签分配(dynamic label assignment)技术。 
 
 
 在以上3个技术点中,尤其是第3点的动态标签分配研究最具有革 
新性,在2022年之后几乎成为了先进的目标检测框架的标准配置之 
一。由此可见,YOLOv1这一早期工作的确蕴含了许多研究潜力和研 
究价值。正如古人所说:“温故而知新”,这也说明我们学习YOLOv1 
的必要性。 
3.3.3 类别置信度 
对于类别置信度,YOLOv1采用了较为简单的处理手段:每一网 
格都只预测一个目标。同边界框的学习一样,类别的学习也只考虑正 
样本网格,而不考虑其他不包含目标中心点的网格。类别的标签是图 
像分类任务中常用的one-hot格式。对于one-hot格式的类别学习,通常 
会使用Softmax函数来处理网络的类别预测,得到每个类别的置信度, 
再配合交叉熵(cross entropy)函数去计算类别损失,这在图像分类任务 
中是再常见不过了。然而,YOLOv1却另辟蹊径,使用线性函数输出 
类别置信度预测,并用L2损失来计算每个类别的损失,所以,在训练 
过 程 中 , YOLOv1 预 测 的 类 别 置 信 度 可 能 会 是 一 个 负 数 , 这 也 使 得 
YOLOv1在训练的早期可能会出现训练不稳定的问题。同样,对于边 
界框的置信度和位置参数,YOLOv1也是采用线性函数来输出的,虽 
然我们知道边界框的置信度的值域应该在0~1范围内,中心点偏移量 
也在0~1范围内,但YOLOv1自身没有对输出做这样的约束,这一点 
也是YOLOv1在后续的工作中被改进的不合理之处之一。 
最后,我们总结一下YOLOv1的制作正样本的流程。对于一个给 
定的目标框,其左上角点坐标为 ,右下角点坐标为 ,我们 
按照以下3个步骤来制作正样本和计算训练损失: 
 
 
 (1)计算目标框的中心点坐标 以及宽和高 ,然后用公式 
(3-4)计算中心点所在的网格坐标,从而确定正样本候选区域的位置; 
(2)使用公式(3-5)计算中心点偏移量 ,并对目标框的宽和高做 
归一化,得到归一化后的坐标 ; 
(3)使用one-hot格式准备类别的学习标签。 
而置信度的学习标签需要在训练过程中确定,步骤如下: 
(1)计算中心点所在的网格的每一个预测框与目标框的IoU; 
(2)保留IoU最大的预测框,标记为正样本,将其设置为置信度的 
学习标签,然后计算边界框的置信度损失、位置参数损失以及类别置 
信度损失; 
(3)对于其他预测框只计算置信度损失,且置信度的学习标签为 
0。 
在了解了制作正样本的流程后,我们就可以着手计算训练损失 
了,下面我们将介绍如何计算训练过程中的损失。 
3.4 YOLOv1的损失函数 
在深度学习领域中,损失函数设计的好坏对模型的性能有着至关 
重要的影响,倘若损失函数设计不当,那么原本一个好的模型结构可 
能会表现得十分糟糕,而一个简单朴素的模型结构在一个好的损失函 
数的训练下,往往会表现出不俗的性能。在前面几节,我们已经了解 
了 YOLOv1 的 工 作 原 理 、 输 出 的 参 数 组 成 以 及 制 作 训 练 正 样 本 的 方 
 
 
 法,掌握了这些必要的知识后,学习YOLOv1的损失函数也就十分容 
易了。 
YOLOv1的损失函数整体如公式(3-8)所示。 
(3-8) 
公式(3-8)中第一行和第二行表示的是边 界 框 的 位 置 参 数 的 损 
失,其中带 的表示学习标签, 和 则是指示函数,分别用于 
标记正样本和负样本。 是位置参数损失的权重,论文中取 
。 
第三行和第四行表示的是边界框的置信度的损失。第三行对应的 
是正样本的置信度损失,其置信度学习标签 为最大的IoU值,第四行 
对应的是负样本的置信度损失,其置信度学习标签就是0。 
 
 
 最后一行就是正 样 本 处 的 类 别 的 损 失 ,每个类别的损失都是L2 
损失,而不是交叉熵。 
整 体 上 看 , YOLOv1 的 损 失 函 数 较 为 简 洁 , 理 解 起 来 也 较 为 容 
易。在掌握了制作正样本的策略和计算损失的方法后,就可以去训练 
YOLOv1,并在训练后去做推理,预测输入图像中的目标。下面我们 
就来了解YOLOv1是如何在测试阶段做前向推理的。 
3.5 YOLOv1的前向推理 
当我们训练完YOLOv1后,对于给定的一张大小为 的 
输入图像,YOLOv1输出 ,其中每个网格位置都包含两个 
边界框的置信度输出 和 、两个边界框的位置参数输出 和 
以及20个类别置信度输出 ~ 。显然,这么多的预 
测不全是我们想要的,我们只关心那些包含目标的网格所给出的预 
测,因此,在得到最终的检测结果之前,我们需要再按照以下4个步骤 
去做滤除和筛选。 
(1)计算所有预测的边界框的得分。在YOLOv1中,每个边界框的 
得分score被定义为该边界框的置信度 与类别的最高置信度 的 
乘积,其中, 。具体来说,对于 
处的网格,边界框 的得分计算公式如下: 
(3-9) 
(2)得分阈值筛选。计算了所有边界框的得分后,我们设定一个阈 
值去滤除那些得分低的边界框。显然,得分低的边界框通常都是背景 
 
 
 框,不包含目标的中心点。比如,我们设置得分阈值为0.3,滤除那些 
得分低于该阈值的低质量的边界框,如图3-11所示。 
图3-11 滤除得分低的边界框 
(3)计算边界框的中心点坐标以及宽和高。筛选完后,我们即可 
计算余下的边界框的中心点坐标以及宽和高。 
(4) 使 用 非 极 大 值 抑 制 进 行 第 二 次 筛 选 。由于YOLOv1可能对同 
一个目标给出多个得分较高的边界框,如图3-12所示,因此,我们需 
要对这种冗余检测进行抑制,以剔除那些不必要的重复检测。为了达 
到 这 一 目 的 , 常 用 的 手 段 之 一 便 是 非 极 大 值 抑 制 (non-maximal 
suppression,NMS)。非极大值抑制的思想很简单,对于某一类别目标 
的所有边界框,先挑选出得分最高的边界框,再依次计算其他边界框 
与这个得分最高的边界框的IoU,超过设定的IoU阈值的边界框则被认 
为是重复检测,将其剔除。对所有类别的边界框都进行上述操作,直 
到无边界框可剔除为止,如图3-12所示。 
 
 
 图3-12 滤除冗余的边界框 
通过上面4个步骤,我们就获得了YOLOv1的最终检测结果。 
3.6 小结 
至此,对于经典的YOLOv1工作,从模型结构到推理方法,再到 
损失函数,我们都进行了详细的讲解。关于YOLOv1是怎么工作的, 
相信读者已经有了较为清晰的认识,而其中的优势和劣势也都在讲解 
的过程中一一体现。然而,老话说得好,纸上得来终觉浅,绝知此事 
要躬行,唯有亲自动手去实践才能更好地加深对YOLOv1的认识,进 
一步地了解如何去搭建一个目标检测的网络,从而构建一个较为完整 
的目标检测项目。通过必要的代码实现环节,将理论与实践结合起 
来,我们才能够真正地掌握所学到的知识。因此,在第4章,我们将会 
在 YOLOv1 工 作 的 基 础 上 去 搭 建 本 书 的 第 一 个 目 标 检 测 网 络 : 
YOLOv1。我们会在YOLOv1的基础上做必要的改进和优化,在不脱离 
YOLOv1框架的范畴的前提下,将会得到一个性能更好的YOLOv1检测 
器。 
(1) Szegedy C, Liu W, Jia Y, et al. Going Deeper with Convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Boston, MA, USA: IEEE Press, 2015: 1-9. 
 
 
 (2) Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object Detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USA: IEEE Press, 2016: 779-788. 
(3) Wu S, Li X, Wang X. IoU-aware Single-stage Object Detector for Accurate Localization[J]. Image and Vision Computing, 2020, 97: 103911. 
(4) Zhang H, Wang Y, Dayoub F, et al. VarifocalNet: An IoU-aware Dense Object Detector[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Nashville, TN, USA: IEEE Press, 2021: 8510-8519. 
 
 
 第4章 搭建YOLOv1网络 
在第3章中,我们讲解了YOLOv1的工作原理,包括网络结构、前 
向推理和损失函数等。在本章中,我们将在YOLOv1的基础上进行改 
进,设计一个结构更好、性能更优的YOLOv1检测器。在正式开始学 
习本章之前,需要强调的是,我们不会以照搬官方代码的方式来搭建 
YOLOv1网络,尽管这从创作的角度来说会带来很大的便利,但从学 
习的角度来说,实为“投机取巧”。本着入门目标检测的宗旨,我们会 
在前文的基础上,优化其中的设计,并结合适当的当前的主流设计理 
念来重新设计一个新的YOLOv1网络,既便于入门,也潜移默化地加 
深我们对于一些当前主流技术理念的认识和理解。 
4.1 改进YOLOv1 
在正式讲解之前,先看一眼我们所要搭建的新的YOLOv1网络是 
什么样的,如图4-1所示,我们要构建的YOLOv1网络是一个全卷积结 
构,其中不包含任何全连接层,这一点可以避免YOLOv1中存在的因 
全连接层而导致的参数过多的问题。尽管YOLO网络是在YOLOv2工作 
才开始转变为全卷积结构,但我们已经了解了全连接层的弊端,因此 
没有必要再循规蹈矩地照搬YOLOv1的原始网络结构,这也符合我们 
设计YOLOv1的初衷。 
 
 
 图4-1 新的YOLOv1的网络结构 
4.1.1 改进主干网络 
首先,我们使用当下流行的ResNet网络代替YOLOv1的GoogLeNet 
风格的主干网络。相较于原本的主干网络,ResNet使用了诸如批 归 一 
化(batch normalization,BN)、残差连接(residual connection)等操作,有 
助于稳定训练更大更深的网络。目前,这两个操作几乎成为了绝大多 
数卷积神经网络的标准结构之一。考虑到这是我们的第一次实践工 
作,并不追求性能上的极致,因此,我们选择很轻量的ResNet-18网络 
作为YOLOv1的主干网络。ResNet-18网络的结构如图4-2所示。 
图4-2 ResNet-18网络的结构 
 
 
 前面已经讲过,将图像分类网络用作目标检测网络的主干网络 
时,通常是不需要最后的平均池化层和分类层的,因此,这里我们去 
除ResNet-18网络中的最后的平均池化层和分类层。 
此 外 , 不 同 于 YOLOv1 原 本 的 GoogLeNet网 络 的 64 倍 降 采 样 , 
ResNet-18网络的最大降采样倍数为32,故而一张 的图像 
输入后,主干网络会输出一个空间大小为 的特征图。相对于 
YOLOv1原本输出的 网格,YOLOv1输出的 网格要更加 
精 细 一 些 。 不 过 , 在 我 们 的 YOLOv1 中 , 默 认 输 入 图 像 的 尺 寸 为 
,而不再是 ,因此,将该尺寸的图像输入 
后,ResNet-18网络会输出张量 。 
关 于 ResNet 网 络 的 代 码 , 读 者 可 以 找 到 项 目 中 的 
models/yolov1/yolov1_backbone.py 文 件 , 在 该 文 件 中 可 以 看 到 由 
PyTorch官方实现的ResNet代码。这里,受篇幅限制,代码4-1只展示 
了ResNet关键部分的代码。 
代码4-1 基于PyTorch框架的ResNet-18 
# YOLO_Tutorial/models/yolov1/yolov1_backbone.py # -------------------------------------------------------... class ResNet(nn.Module): def __init__(self, block, layers, zero_init_residual=False): super(ResNet, self).__init__() self.inplanes=64 self.conv1=nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1=nn.BatchNorm2d(64) self.relu=nn.ReLU(inplace=True) self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1=self._make_layer(block, 64, layers[0]) self.layer2=self._make_layer(block, 128, layers[1], stride=2) 
 
 
 self.layer3=self._make_layer(block, 256, layers[2], stride=2) self.layer4=self._make_layer(block, 512, layers[3], stride=2) 
def forward(self, x): c1=self.conv1(x) # [B, C, H/2, W/2] c1=self.bn1(c1) # [B, C, H/2, W/2] c1=self.relu(c1) # [B, C, H/2, W/2] c2=self.maxpool(c1) # [B, C, H/4, W/4] 
c2=self.layer1(c2) # [B, C, H/4, W/4] c3=self.layer2(c2) # [B, C, H/8, W/8] c4=self.layer3(c3) # [B, C, H/16, W/16] c5=self.layer4(c4) # [B, C, H/32, W/32] 
return c5 
4.1.2 添加一个颈部网络 
为了提升网络的性能,我们不妨选择一个好用的颈部网络,将其 
添加在主干网络后面。在1.3.2节中,我们已经介绍了几种常用的颈部 
网络。这里,出于对参数数量与性能的考虑,我们选择性价比较高的 
空间金字塔池化(SPP)模块。在本次实现中,我们遵循主流的YOLO框 
架的做法,对SPP模块做适当的改进,如图4-3所示。 
图4-3 改进的SPP模块的网络结构 
改进的SPP模块的网络结构设计参考了YOLOv5开源项目中的实现 
方法,让一层5×5的最大池化层等效于先前讲过的5×5、9×9和13×13这 
 
 
 三条并行的最大池化层分支,从而降低计算开销,如代码4-2所示。 
代码4-2 YOLOv5风格的SPP模块 
# YOLO_Tutorial/models/yolov1/yolov1_neck.py # -------------------------------------------------------... class SPPF(nn.Module): def __init__(self, in_dim, out_dim, expand_ratio=0.5, pooling_size=5, act_type='lrelu', norm_type='BN'): super().__init__() inter_dim=int(in_dim * expand_ratio) self.out_dim=out_dim self.cv1=Conv(in_dim, inter_dim, k=1, act_type=act_type, norm_type= norm_type) self.cv2=Conv(inter_dim * 4, out_dim, k=1, act_type=act_type, norm_type= norm_type) self.m=nn.MaxPool2d(kernel_size=pooling_size, stride=1, padding=pooling_ size // 2) 
def forward(self, x): x=self.cv1(x) y1=self.m(x) y2=self.m(y1) 
return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1)) 
在代码4-2中,输入的特征图会先被一层 卷积处理,其通道 
数会被压缩一半,随后再由一层 最大池化层连续处理三次,依 
据感受野的原理,该处理方式等价于分别使用 、 和 
最大池化层并行地处理特征图。最后,将所有处理后的特征 
图沿通道拼接,再由另一层 卷积做一次输出的映射,将其通道映 
射至指定数目的输出通道。 
4.1.3 修改检测头 
 
 
 在YOLOv1中,检测头部分用的是全连接层,关于全连接层的缺 
点我们已经介绍过了,这里,我们抛弃全连接层,改用卷积网络。由 
于当前主流的检测头是解耦检测头,因此,我们也采用解耦检测头作 
为YOLOv1的检测头,由类别分支和回归分支组成,分别提取类别特 
征和位置特征,如图4-4所示。 
图4-4 YOLOv1的解耦检测头 
解耦检测头的结构十分简单,共输出两种不同的特征:类别特征 
和位置特征 。没有过于复杂的结构,因此代码 
编写也较为容易。在本项目的models/yolov1/yolov1_head.py文件中,我 
们实现了相关的代码,如代码4-3所示。 
代码4-3 YOLOv1的解耦检测头 
# YOLO_Tutorial/models/yolov1/yolov1_head.py # ------------------------------------------------------- 
 
 
 ... class DecoupledHead(nn.Module): def __init__(self, cfg, in_dim, out_dim, num_classes=80): super().__init__() print('==============================') print('Head: Decoupled Head') self.in_dim=in_dim self.num_cls_head=cfg['num_cls_head'] self.num_reg_head=cfg['num_reg_head'] self.act_type=cfg['head_act'] self.norm_type=cfg['head_norm'] 
# cls head cls_feats=[] self.cls_out_dim=max(out_dim, num_classes) for i in range(cfg['num_cls_head']): if i==0: cls_feats.append( Conv(in_dim, self.cls_out_dim, k=3, p=1, s=1, act_type=self.act_type, norm_type=self.norm_type, depthwise=cfg['head_depthwise']) ) else: cls_feats.append( Conv(self.cls_out_dim, self.cls_out_dim, k=3, p=1, s=1, act_type=self.act_type, norm_type=self.norm_type, depthwise=cfg['head_depthwise']) ) # reg head reg_feats=[] self.reg_out_dim=max(out_dim, 64) for i in range(cfg['num_reg_head']): if i==0: reg_feats.append( Conv(in_dim, self.reg_out_dim, k=3, p=1, s=1, act_type=self.act_type, norm_type=self.norm_type, depthwise=cfg['head_depthwise']) ) else: reg_feats.append( Conv(self.reg_out_dim, self.reg_out_dim, k=3, p=1, s=1, 
 
 
 act_type=self.act_type, norm_type=self.norm_type, depthwise=cfg['head_depthwise']) ) 
self.cls_feats=nn.Sequential(*cls_feats) self.reg_feats=nn.Sequential(*reg_feats) 
def forward(self, x): cls_feats=self.cls_feats(x) reg_feats=self.reg_feats(x) 
return cls_feats, reg_feats 
4.1.4 修改预测层 
一张 的输入图像经过主干网络、颈部网络和检测头 
三部分处理后,得到特征图 。由于本文要搭建的YOLOv1 
是全卷积网络结构,因此在最后的预测层,采用当下主流的做法,即 
使用 的卷积层在特征图上做预测,如图4-5所示。不难想到,使 
用卷积操作在特征图上做预测,恰好和YOLOv1的“逐网格找物体”这 
一检测思想对应了起来。 
图4-5 YOLOv1使用 卷积层做预测 
在官方的YOLOv1中,每个网格预测两个边界框,而这两个边界 
框的学习完全依赖自身预测的边界框位置的准确性,YOLOv1本身并 
没有对这两个边界框做任何约束。可以认为,这两个边界框是“平 
 
 
 权”的,谁学得好谁学得差完全是随机的,二者之间没有显式的互斥关 
系,且每个网格处最终只会输出置信度最大的边界框,那么可以将这 
两个“平权”的边界框修改为一个边界框,即每个网格处只需要输出一 
个 边 界 框 。 于 是 , 我 们 的 YOLOv1 网 络 最 终 输 出 的 张 量 为 
,其中通道维度上的1表示边界框的置信度, 表示 
类别的总数,4表示边界框的4个位置参数。这里不再有表示每个网格 
的边界框数量的 。 
然而,先前我们已经将检测头替换成了解耦检测头,因此,预测 
层也要做出相应的修改。具体来说,我们采用解耦检测头,分别输出 
两个不同的特征:类别特征 和位置特征 ,Fcls 
用 于 预 测 边 界 框 置 信 度 和 类 别 置 信 度 , Freg 用 于 预 测 边 界 框 位 置 参 
数。图4-6展示了我们设计的YOLOv1所采用的解耦检测头和预测层的 
结构。 
 
 
 图4-6 YOLOv1所采用的解耦检测头的结构 
这里,我们详细介绍一下预测层的处理方式。 
● 
边界框置信度的预测。我们使用类别特征 来完成边 
界框置信度的预测。另外,不同于YOLOv1中使用预测框与目标框的 
IoU作为优化目标,我们暂时采用简单的二分类标签0/1作为置信度的 
学习标签。这样改进并不表示二分类标签比将IoU作为学习标签的方法 
更好,而仅仅是图方便,省去了在训练过程中计算IoU的麻烦,且便于 
读者上手。由于边界框的置信度的值域在0~1范围内,为了确保这一 
点,避免网络输出超出这一值域的“不合理”数值,我们使用Sigmoid函 
数将网络的置信度输出映射到0~1范围内。 
● 
类别置信度的预测。我们使用类别特征 来完成类别 
置信度的预测。因此,类别特征将分别被用于有无目标的检测和类别 
 
 
 分类两个子任务中。类别置信度显然也在0~1范围内,因此我们使用 
Sigmoid函数来输出对每个类别置信度的预测。 
● 
边界框位置参数的预测。自然地,我们使用位置特征 
来完成边界框位置参数的预测。我们已经知道,边界框的 
中心点偏差 的值域是0~1,因此,我们也对网络输出的中心点偏 
差 和 使用Sigmoid函数。另外两个参数 和 是非负数,这也就意 
味着,我们必须保证网络输出的这两个量是非负数,否则没有意义。 
一种办法是用ReLU函数来保证这一点,然而ReLU的负半轴梯度为0, 
无法回传梯度,有“死元”的潜在风险。另一种办法则是仍使用线性输 
出,但添加一个不小于0的不等式约束。但不论是哪一种方法,都存在 
约束问题,这一点往往是不利于训练优化的。为了解决这一问题,我 
们采用指数函数来处理,该方法既能保证输出范围是实数域,又是全 
局可微的,不需要额外的不等式约束。两个参数w和h的计算如公式(4 
1)所示,其中,指数函数外部乘了网络的输出步长 ,这就意味着预 
测的 和 都是相对于网格尺度来表示的。 
(4-1) 
4.1.5 修改损失函数 
经过4.1.4节的修改后,我们同样需要修改YOLO的损失函数,包 
括置信度损失、类别损失和边界框位置参数的损失,以便我们后续能 
够正确地训练模型。接下来,依次介绍每一个损失函数的修改。 
 
 
 置信度损失。首先,修改置信度损失。由于置信度的输出经过 
Sigmoid函数的处理,因此我们采用二元交叉熵(binary cross entropy, 
BCE)函数来计算置信度损失,如公式(4-2)所示,其中, 是正样本 
的数量。 
(4-2) 
类别损失。接着是修改类别置信度的损失函数。由于类别预测中 
的每个类别置信度都经过Sigmoid函数的处理,因此,我们同样采用 
BCE函数来计算类别损失,如公式(4-3)所示。 
(4-3) 
边界框位置参数的损失。对于位置损失,我们采用更主流的办 
法。具体来说,我们首先根据预测的中心点偏差以及宽和高来得到预 
测框 ,然后计算预测框 与目标框 的GIoU(generalized IoU) 
(1),最后,使用线性GIoU损失函数去计算位置参数损失,如公式(4-4) 
所示。 
(4-4) 
 
 
 总的损失。最后,将公式(4-2)、(4-3)、(4-4)加起来便得到完整的 
损失函数,如公式(4-5)所示,其中, 是位置参数损失的权重,默认 
为5。 
(4-5) 
至此,我们设计的YOLOv1的原理部分和相对于官方的YOLOv1所 
作 的 改 进 和 优 化 就 讲 解 完 了 。 在 4.2 节 中 , 我 们 将 着 手 使 用 流 行 的 
PyTorch框架来搭建完整的YOLOv1网络。 
4.2 搭建YOLOv1网络 
在4.1节中,主要讲解了我们设计的YOLOv1网络。从本节开始, 
为了后续的讲解和代码实现,作者默认每一位读者都已熟悉PyTorch框 
架的基本操作,并配置好了Python3等必要的运行环境。后续我们在训 
练网络时,需要使用到独立显卡,若读者没有独立显卡,也不影响学 
习,可在日后有了合适的硬件条件时再去训练模型。在整个讲解的过 
程中,我们会详细讲解每一个实践细节,尽可能地使每一位读者在缺 
少GPU的情况下,仍能够掌握本书的必要知识。并且,我们也在本书 
的项目代码中提供了已经训练好的模型权重文件,以供读者使用(强 
烈建议读者将本书的配套源代码全部下载下来,以便后续的学习)。 
关于PyTorch框架的安装、CUDA环境的配置等,网上已有大量的 
优秀教程,详细介绍了各个软件的安装方式,请读者自行上网查阅这 
些必要的安装教程。 
现在,我们开始动手实践吧。 
 
 
 在正式开始搭建模型之前,我们先构建YOLOv1的代码框架,这 
一过程相当于准备蓝图,以确定好我们后续要编写哪些代码、构建哪 
些模块,从而最终组成YOLOv1模型。我们设计的YOLOv1的整体框架 
如代码4-4所示。 
代码4-4 我们设计的YOLOv1的整体框架 
# YOLO_Tutorial/models/yolov1/yolov1.py # -------------------------------------------------------... 
class YOLOv1(nn.Module): def __init__(self, cfg, device, input_size, num_classes, trainable, conf_thresh, nms_thresh): super(YOLOv1, self).__init__() self.cfg=cfg # 模型配置文 件 
self.device=device # 设备、CUDA 或CPU self.num_classes=num_classes # 类别的数量 self.trainable=trainable # 训练的标记 self.conf_thresh=conf_thresh # 得分阈值 self.nms_thresh=nms_thresh # NMS阈值 self.stride=32 # 网络的最大 步长 
# >>>>>>>>>>>>>>>>>>>> 主干网络 <<<<<<<<<<<<<<<<<<<<< # TODO: 构建我们的backbone网络 # self.backbone=? 
# >>>>>>>>>>>>>>>>>>>> 颈部网络 <<<<<<<<<<<<<<<<<<<<< # TODO: 构建我们的neck网络 # self.neck=? 
# >>>>>>>>>>>>>>>>>>>> 检测头 <<<<<<<<<<<<<<<<<<<<< # TODO: 构建我们的detection head 网络 # self.head=? 
# >>>>>>>>>>>>>>>>>>>> 预测层 <<<<<<<<<<<<<<<<<<<<< # TODO: 构建我们的预测层 # self.pred=? 
def create_grid(self, input_size): # TODO: 用于生成网格坐标矩阵 
 
 
 def decode_boxes(self, pred): # TODO: 解算边界框坐标 
def nms(self, bboxes, scores): # TODO: 非极大值抑制操作 
def postprocess(self, bboxes, scores): 
# TODO: 后处理, 包括得分阈值筛选和NMS操作 
@torch.no_grad() def inference(self, x): # TODO: YOLOv1前向推理 
def forward(self, x, targets=None): # TODO: YOLOv1的主体运算函数 
接下来,我们就可以将根据上面的代码框架来讲解如何搭建 
YOLOv1网络。 
4.2.1 搭建主干网络 
前面已经说到,我们的YOLOv1使用较轻量的ResNet-18作为主干 
网络。由于PyTorch官方已提供了ResNet的源码和相应的预训练模型, 
因此,这里就不需要我们自己去搭建ResNet的网络和训练了。为了方 
便 调 用 和 查 看 , ResNet 的 代 码 文 件 放 在 项 目 中 
models/yolov1/yolov1_backbone.py文件下,感兴趣的读者可以打开该文 
件 来 查 看 ResNet网 络 的 代 码 。 在 确 定 了 主 干 网 络 后 , 我 们 只 需 在 
YOLOv1框架中编写代码即可调用ResNet-18网络,如代码4-5所示。 
代码4-5 构建YOLOv1的主干网络 
# >>>>>>>>>>>>>>>>>>>>主干网络<<<<<<<<<<<<<<<<<<<<< # TODO:构建我们的backbone网络 self.backbone,feat_dim=build_backbone(cfg['backbone'],trainab le&cfg['pretrained']) 
 
 
 在代码4-5中,cfg是模型的配置文件,feat_dim变量是主干网络输 
出的特征图的通道数,这在后续的代码会使用到。我们通过 
trainable&cfg['pretrained']的组合来决定是否加载预训练权重。代码4-6 
展示了模型的配置文件所包含的一些参数,包括网络结构的参数、损 
失函数所需的权重参数、优化器参数以及一些训练配置参数等,每个 
参数的含义都已标注在注释中。 
代码4-6 我们所搭建的YOLOv1的配置参数 
# YOLO_Tutorial/config/mode1_config/yolov1_config.py # -------------------------------------------------------... 
yolov1_cfg={ # input 'trans_type': 'ssd', # 使用SSD风格的数据增强 'multi_scale': [0.5, 1.5], # 多尺度的范围 # model 'backbone': 'resnet18', # 使用ResNet-18作为主干网络 'pretrained': True, # 加载预训练权重 'stride': 32, # P5 # 网络的最大输出步长 # neck 'neck': 'sppf', # 使用SPP作为颈部网络 'expand_ratio': 0.5, # SPP的模型参数 'pooling_size': 5, # SPP的模型参数 'neck_act': 'lrelu', # SPP的模型参数 'neck_norm': 'BN', # SPP的模型参数 'neck_depthwise': False, # SPP的模型参数 # head 'head': 'decoupled_head', # 使用解耦检测头 'head_act': 'lrelu', # 检测头所需的参数 'head_norm': 'BN', # 检测头所需的参数 'num_cls_head': 2, # 解耦检测头的类别分支所包含的卷 积层数 
'num_reg_head': 2, # 解耦检测头的回归分支所包含的卷 积层数 
'head_depthwise': False, # 检测头所需的参数 # loss weight 'loss_obj_weight': 1.0, # obj损失的权重 'loss_cls_weight': 1.0, # cls损失的权重 'loss_box_weight': 5.0, # box损失的权重 
 
 
 # training configuration 'no_aug_epoch': -1, # 关闭马赛克增强和混合增强的节点 # optimizer 'optimizer': 'sgd', # 使用SGD优化器 'momentum': 0.937, # SGD优化器的momentum参数 'weight_decay': 5e-4, # SGD优化器的weight_decay参数 'clip_grad': 10, # 梯度剪裁参数 # model EMA 'ema_decay': 0.9999, # 模型EMA参数 'ema_tau': 2000, # 模型EMA参数 # lr schedule 'scheduler': 'linear', # 使用线性学习率衰减策略 'lr0': 0.01, # 初始学习率 'lrf': 0.01, # 最终的学习率=lr0 * lrf 'warmup_momentum': 0.8, # Warmup阶段, 优化器的momentum 参数的初始值 
'warmup_bias_lr': 0.1, # Warmup阶段, 优化器为模型的 bias参数设置的学习率初始值 } 
4.2.2 搭建颈部网络 
前面已经说到,我们的YOLOv1选择SPP模块作为颈部网络。SPP 
网络的结构非常简单,仅由若干不同尺寸的核的最大池化层所组成, 
实现起来也非常地简单,相关代码我们已经在前面展示了。而在 
YOLOv1中,我们直接调用相关的函数来使用SPP即可,如代码4-7所 
示。 
代码4-7 构建YOLOv1的颈部网络 
# >>>>>>>>>>>>>>>>>>>> 颈部网络 <<<<<<<<<<<<<<<<<<<<< # TODO: 构建我们的颈部网络 
self.neck=build_neck(cfg, feat_dim, out_dim=512) head_dim=self.neck.out_dim 
4.2.3 搭建检测头 
 
 
 有关检测头的代码和预测层相关的代码已经在前面介绍过了,这 
里,我们只需要调用相关的函数来使用解耦检测头,然后再使用 
卷积创建预测层,如代码4-8所示。 
代码4-8 构建YOLOv1的检测头 
# >>>>>>>>>>>>>>>>>>>> 检测头 <<<<<<<<<<<<<<<<<<<<< # TODO: 构建我们的detection head 网络 ## 检测头 self.head=build_head(cfg, head_dim, head_dim, num_classes) 
# >>>>>>>>>>>>>>>>>>>> 预测层 <<<<<<<<<<<<<<<<<<<<< # TODO: 构建我们的预测层 
self.obj_pred=nn.Conv2d(head_dim, 1, kernel_size=1) self.cls_pred=nn.Conv2d(head_dim, num_classes, kernel_size=1) self.reg_pred=nn.Conv2d(head_dim, 4, kernel_size=1) 
4.2.4 YOLOv1前向推理 
确定好了网络结构的代码后,我们就可以按照本章最开始的图4.1 
所展示的结构来编写前向推理的代码,也就是YOLOv1的主框架中的 
forward函数,如代码4-9所示。 
代码4-9 构建YOLOv1在训练阶段所使用的推理函数 
def forward(self, x): if not self.trainable: return self.inference(x) else: 
# 主干网络 
feat=self.backbone(x) 
# 颈部网络 
feat=self.neck(feat) 
# 检测头 
cls_feat, reg_feat=self.head(feat) 
# 预测层 
 
 
 obj_pred=self.obj_pred(cls_feat) cls_pred=self.cls_pred(cls_feat) reg_pred=self.reg_pred(reg_feat) fmp_size=obj_pred.shape[-2:] 
# 对pred 的size做一些调整, 便于后续的处理 
# [B, C, H, W] -> [B, H, W, C] -> [B, H*W, C] obj_pred=obj_pred.permute(0, 2, 3, 1).contiguous().flatten(1, 2) cls_pred=cls_pred.permute(0, 2, 3, 1).contiguous().flatten(1, 2) reg_pred=reg_pred.permute(0, 2, 3, 1).contiguous().flatten(1, 2) 
# 解耦边界框 
box_pred=self.decode_boxes(reg_pred, fmp_size) 
# 网络输出 outputs={ "pred_obj": obj_pred, # (Tensor) [B, M, 1] "pred_cls": cls_pred, # (Tensor) [B, M, C] "pred_box": box_pred, # (Tensor) [B, M, 4] "stride": self.stride, # (Int) "fmp_size": fmp_size # (List) [fmp_h, fmp_w] 
} return outputs 
注意,在上述所展示的推理代码中,我们对变量pred执行了view 
操作,将 和 两个维度合并到一起,由于这之后不会再有任何卷 
积操作了,而仅仅是要计算损失,因此,将输出张量的维度从 
调整为 的目的仅是方便后续的损失计算和后处 
理,而不会造成其他不必要的负面影响。 
另外,在测试阶段,我们只需要推理当前输入图像,无须计算损 
失,所以我们单独实现了一个inference函数,如代码4-10所示。 
代码4-10 YOLOv1在测试阶段的前向推理 
 
 
 @torch.no_grad() def inference(self, x): # 主干网络 
feat=self.backbone(x) 
# 颈部网络 
feat=self.neck(feat) 
# 检测头 
cls_feat, reg_feat=self.head(feat) 
# 预测层 
obj_pred=self.obj_pred(cls_feat) cls_pred=self.cls_pred(cls_feat) reg_pred=self.reg_pred(reg_feat) fmp_size=obj_pred.shape[-2:] 
# 对pred 的size做一些调整, 便于后续的处理 
# [B, C, H, W] -> [B, H, W, C] -> [B, H*W, C] obj_pred=obj_pred.permute(0, 2, 3, 1).contiguous().flatten(1, 2) cls_pred=cls_pred.permute(0, 2, 3, 1).contiguous().flatten(1, 2) reg_pred=reg_pred.permute(0, 2, 3, 1).contiguous().flatten(1, 2) 
# 测试时, 默认batch是1 
# 因此, 我们不需要用batch这个维度, 用[0]将其取走 obj_pred=obj_pred[0] # [H*W, 1] cls_pred=cls_pred[0] # [H*W, NC] reg_pred=reg_pred[0] # [H*W, 4] 
# 每个边界框的得分 
scores=torch.sqrt(obj_pred.sigmoid() * cls_pred.sigmoid()) 
# 解算边界框, 并归一化边界框: [H*W, 4] 
bboxes=self.decode_boxes(reg_pred, fmp_size) # 将预测放在CPU处理上, 以便进行后处理 scores=scores.cpu().numpy() bboxes=bboxes.cpu().numpy() # 后处理 
bboxes, scores, labels=self.postprocess(bboxes, scores) 
return bboxes, scores, labels 
 
 
 在代码4-10中,装饰器@torch.no_grad()表示该inference函数不 
会存在任何梯度,因为推理阶段不会涉及反向传播,无须计算变量的 
梯度。在这段代码中,多了一个后处理postprocess函数的调用,我们 
将会在4.3.2节介绍后处理的实现。 
至此,我们搭建完了YOLOv1的网络,只需将上面的单独实现分 
别对号入座地加入YOLOv1的网络框架里。最后,我们就可以获得网 
络的3个预测分支的输出。但是,这里还遗留下了以下3个问题尚待处 
理。 
(1) 如何有效地计算出边界框的左上角点坐标和右下角点坐标。 
(2) 如何计算3个分支的损失。 
(3) 如何对预测结果进行后处理。 
接下来,我们将在4.3节中一一解决上述的3个问题。 
4.3 YOLOv1的后处理 
在4.2节中,我们已经完成了YOLOv1网络的代码编写,但除了网 
络本身,仍存在一些空白需要我们去填补。在本节中,我们依次解决 
4.2节的节尾所遗留的问题。 
4.3.1 求解预测边界框的坐标 
对于某一处的网格 ,YOLOv1输出的边界框的中心点偏 
移量预测为 和 ,宽和高的对数映射预测为 和 ,我们使用公式 
(4-6)即可解算出边界框的中心点坐标 和 与宽高 和 。 
 
 
 (4-6) 
其中, 是Sigmoid函数。从公式中可以看出,为了计算预测的 
边界框的中心点坐标,我们需要获得网格的坐标 ,因为我们 
的YOLOv1也是在每个网格预测偏移量,从而获得精确的边界框中心 
点坐标。直接的方法就是遍历每一个网格,以获取网格坐标,然后加 
上此处预测的偏移量即可获得此处预测出的边界框中心点坐标,但是 
这种for循环操作的效率不高。在一般情况下,能用矩阵运算来实现的 
操作就尽量避免使用for循环,因为不论是GPU还是CPU,矩阵运算都 
是可以并行处理的,开销更小,因此,这里我们采用一个讨巧的等价 
方法。 
在计算边界框坐标之前,先生成一个保存网格所有坐标的矩阵 
,其中 和 是输出的特征图的空间尺寸,2是网格的 
横纵坐标。 就是输出特征图上 处的网格坐标 
,即 , ,如图4-7所示。 
 
 
 图4-7 YOLOv1的G矩阵,其中保存了所有的网格坐标 
所以,在清楚了G矩阵的含义后,我们便可以编写相应的代码来 
生成G矩阵,如代码4-11所示。 
代码4-11 保存了网格坐标的G矩阵的生成代码 
def create_grid(self, input_size): # 输入图像的宽和高 
w, h=input_size, input_size # 特征图的宽和高 
ws, hs=w // self.stride, h // self.stride # 生成网格的x坐标和y坐标 
grid_y, grid_x=torch.meshgrid([torch.arange(hs), torch.arange(ws)]) 
# 将x和y两部分的坐标拼起来:[H, W, 2] 
grid_xy=torch.stack([grid_x, grid_y], dim=-1).float() # [H, W, 2] -> [HW, 2] 
 
 
 grid_xy=grid_xy.view(-1, 2).to(self.device) return grid_xy 
注 意 , 为 了 后 续 解 算 边 界 框 的 方 便 , 将 grid_xy的 维 度 调 整 成 
的 形 式 , 因 为 在 讲 解 YOLOv1 的 前 向 推 理 的 代 码 时 , 输 出 的 
txtytwth_pred的维度被调整为 的形式,这里我们为了保持维度一 
致,也做了同样的处理。 
在得到了G矩阵之后,我们就可以很容易计算边界框的位置参数 
了,包括边界框的中心点坐标、宽、高、左上角点坐标和右下角点坐 
标,代码4-12展示了这一计算过程。 
代码4-12 解算预测的边界框坐标 
def decode_boxes(self, pred, fmp_size): """ 将txtytwth转换为常用的x1y1x2y2形式 """ 
# 生成网格坐标矩阵 
grid_cell=self.create_grid(fmp_size) 
# 计算预测边界框的中心点坐标、宽和高 
pred_ctr=(torch.sigmoid(pred[..., :2]) + grid_cell) * self.stride pred_wh=torch.exp(pred[..., 2:]) * self.stride 
# 将所有边界框的中心点坐标、宽和高换算成x1y1x2y2形式 pred_x1y1=pred_ctr - pred_wh * 0.5 pred_x2y2=pred_ctr + pred_wh * 0.5 pred_box=torch.cat([pred_x1y1, pred_x2y2], dim=-1) 
return pred_box 
最终,我们会得到边界框的左上角点坐标和右下角点坐标。 
4.3.2 后处理 
 
 
 当我们得到了边界框的位置参数后,我们还需要对预测结果做进 
一步的后处理,滤除那些得分低的边界框和检测到同一目标的冗余 
框。因此,后处理的主要作用可以总结为两点: 
(1)滤除得分低的低质量边界框; 
(2) 滤 除 对 同 一 目 标 的 冗 余 检 测 结 果 , 即 非 极 大 值 抑 制 (NMS) 处 
理。 
在清楚了后处理的逻辑和目的后,我们就可以编写相应的代码 
了,如代码4-13所示。 
代码4-13 YOLOv1的后处理 
def postprocess(self, bboxes, scores): # 将得分最高的类别作为预测的类别标签 
labels=np.argmax(scores, axis=1) # 预测标签所对应的得分 
scores=scores[(np.arange(scores.shape[0]), labels)] # 阈值筛选 
keep=np.where(scores >=self.conf_thresh) bboxes=bboxes[keep] scores=scores[keep] labels=labels[keep] 
# 非极大值抑制 
keep=np.zeros(len(bboxes), dtype=np.int) for i in range(self.num_classes): inds=np.where(labels==i)[0] if len(inds)==0: continue c_bboxes=bboxes[inds] c_scores=scores[inds] c_keep=self.nms(c_bboxes, c_scores) keep[inds[c_keep]]=1 
keep=np.where(keep > 0) bboxes=bboxes[keep] scores=scores[keep] labels=labels[keep] 
 
 
 return bboxes, scores, labels 
我们采用十分经典的基于Python语言实现的代码作为本文的非极 
大值抑制。在入门阶段,希望读者能够将这段代码烂熟于心,这毕竟 
是此领域的必备算法之一。相关代码如代码4-14所示。 
代码4-14 NMS的经典实现 
def nms(self, bboxes, scores): """Pure Python NMS baseline.""" x1=bboxes[:, 0] #xmin y1=bboxes[:, 1] #ymin x2=bboxes[:, 2] #xmax y2=bboxes[:, 3] #ymax areas=(x2 - x1) * (y2 - y1) order=scores.argsort()[::-1] keep=[] while order.size > 0: i=order[0] keep.append(i) 
# 计算交集的左上角点和右下角点的坐标 
xx1=np.maximum(x1[i], x1[order[1:]]) yy1=np.maximum(y1[i], y1[order[1:]]) xx2=np.minimum(x2[i], x2[order[1:]]) yy2=np.minimum(y2[i], y2[order[1:]]) # 计算交集的宽和高 
w=np.maximum(1e-10, xx2 - xx1) h=np.maximum(1e-10, yy2 - yy1) # 计算交集的面积 
inter=w * h # 计算交并比 
iou=inter / (areas[i] + areas[order[1:]] - inter) # 滤除超过NMS阈值的边界框 
inds=np.where(iou <=self.nms_thresh)[0] order=order[inds + 1] 
return keep 
经过后处理后,我们得到了最终的3个输出变量: 
(1)变量bboxes,包含每一个边界框的左上角坐标和右下角坐标; 
 
 
 (2)变量scores,包含每一个边界框的得分; 
(3)变量labels,包含每一个边界框的类别预测。 
至此,我们填补了之前留下来的空白,只需要将上面实现的每一 
个函数放置到YOLOv1的代码框架中,即可组成最终的模型代码。读 
者 可 以 打 开 项 目 中 的 models/yolov1/yolov1.py 文 件 来 查 看 完 整 的 
YOLOv1的模型代码。 
4.4 小结 
至此,在4.2节末尾所提出的3个问题就都得到了解决。现在,我 
们通过相关的代码实现,已经搭建起了完整的YOLOv1的模型,并在 
这一过程中,逐一地清除了原版YOLOv1的一些弊端。通过这样的代 
码实现,相信读者已经了解到了如何使用PyTorch深度学习框架来搭建 
一个目标检测网络。尽管本文所采用的代码风格简约浅显,没有过多 
的封装和嵌套,但如此简约的风格主要是为了便于读者阅读和理解, 
至于今后阅读一些流行的开源代码,还需读者慢慢摸索和积累。既然 
有了模型,接下来,我们就准备讲解如何训练YOLOv1网络,并着重 
讲解制作正样本和计算损失这两个关键的技术要点。 
(1) Rezatofighi H, Tsoi N, Gwak J Y, et al. Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Long Beach, CA, USA: IEEE Press, 2019: 658-666. 
 
 
 第6章 YOLOv2 
在前面的章节中,我们先后了解和学习了目标检测技术的发展概 
况、常用的数据集,以及one-stage 框架的开山之作YOLOv1,并在 
YOLOv1的学习基础上实现了第一个由我们自己动手搭建的单尺度目 
标检测网络:YOLOv1。从检测的原理上来看,我们的YOLOv1和 
YOLOv1一样,区别仅在于实现上改用和加入了更贴近当下主流的做 
法,例如使用ResNet作为主干网络、在卷积层后加入批归一化,以及 
采用全卷积网络结构设计YOLOv1等。相较于官方的YOLOv1,我们的 
YOLOv1具有更加简洁的网络结构和出色的检测性能,同时,端到端 
的全卷积网络架构也更加便于读者学习、理解和实践。 
当然,这里并不是吹捧我们的YOLOv1,毕竟我们的YOLOv1也是 
在借鉴了许多研究成果的基础上得以实现的。YOLOv1的影响是深远 
的,是开启one-stage通用目标检测模型时代的先锋之作,其里程碑地 
位不可撼动。不过,既然称为YOLOv1,就表明YOLO这个工作并未就 
此止步。 
在2016年IEEE主办的计算机视觉与模式识别(CVPR)会议上,继在 
YOLOv1工作获得了瞩目的成功之后,YOLO的作者团队立即推出了第 
二代YOLO检测器:YOLOv2(1)。新提出的YOLOv2在YOLOv1基础之上 
做 了 大 量 的 改 进 和 优 化 , 如 使 用 新 的 网 络 结 构 、 引 入 由 Faster R 
CNN(2)工作提出的先验框机制、提出基于k均值聚类算法的先验框聚类 
算法,以及采用新的边界框回归方法等。在VOC2007数据集上,凭借 
着这一系列的改进,YOLOv2不仅大幅度超越了上一代的YOLOv1,同 
时也超越了同年发表在欧洲计算机视觉国际会议(ECCV)上的新型one 
 
 
 stage检测器:SSD(3),成为了那个年代当之无愧的最强目标检测器之 
一。接下来我们来看看YOLOv2究竟做了哪些改进。 
6.1 YOLOv2详解 
由于YOLOv2的工作是建立在YOLOv1基础上的,因此读者理解 
YOLOv2也会更容易,只要理解它相对于YOLOv1所做出的各种改进和 
优化,就可以在这一过程中掌握YOLOv2的原理和精髓。接下来,就 
让我们来一一认识和了解YOLOv2的诸多改进方面。 
6.1.1 引入批归一化层 
在上一代的YOLOv1中,每一层卷积结构都是由普通的线性卷积 
和非线性激活函数构成,其中并没有使用到后来十分流行的归一化 
层 , 如 批 归 一 化 (batch normalization , BN) 、 层 归 一 化 (layer 
normalization, LN) 和 实 例 归 一 化 (instance normalization, IN) 等 。 后 
来,随着BN层逐渐得到越来越多工作的认可,它几乎成为了许多卷积 
神经网络的标配之一,因此,YOLO的作者团队便在这一次的改进中 
也引入了被广泛验证的BN层。具体来说,卷积层的结构从原先的线性 
卷积与非线性激活函数的组合变为后来YOLO系列常用的“卷积三件 
套”:线性卷积、BN层和非线性激活函数,如图6-1所示。 
图6-1 YOLOv2中的“卷积+BN+激活函数”模块 
 
 
 在 加 入 了 BN层 之 后 , YOLOv1 的 性 能 得 到 了 第 一 次 提 升 。 在 
VOC2007测试集上,其mAP性能从原本的63.4%提升到65.8%。 
6.1.2 高分辨率主干网络 
在上一代的YOLOv1中,作者团队先基于GoogLeNet的网络结构设 
计 了 合 适 的 主 干 网 络 , 并 将 其 放 到 ImageNet 数 据 集 上 进 行 一 次 预 训 
练,随后,再将这一预训练的权重作为YOLOv1的主干网络的初始参 
数,这就是我们前文提到过的“ImageNet pretrained”技术。但是,作者 
团队认为这当中存在一个细节上的缺陷。在预训练阶段,主干网络接 
受的图像的空间尺寸是 ,而在执行目标检测任务时, 
YOLOv1接受的图像的空间尺寸则是 ,不难想象,不同尺 
寸的图像所包含的信息量是完全不同的,那么在训练YOLOv1时,由 
预训练权重初始化的主干网络就必须先解决由图像分辨率的变化所带 
来的某些问题。 
为了解决这一问题,作者团队采用了“二次微调”的策略,即当主 
干网络在完成常规的预训练之后,再使用 的图像继续训练 
主干网络,对其参数进行一次微调(fine-tune),使之适应高分辨率的图 
像。由于第二次训练建立在第一次训练的基础上,因此不需要太多的 
训练。依据论文的设定,第二次训练仅设置10个epoch。当微调完毕 
后,用这一次训练的权重去初始化YOLOv1的主干网络。在这样的策 
略 之 下 , YOLOv1 的 性 能 又 一 次 得 到 了 提 升 : mAP从 65.8% 提 升 到 
69.5%。由此可见,这一技巧确实有明显的作用。不过,似乎这一技 
巧只有YOLO工作在用,并未成为主流训练技巧,鲜在其他工作中用 
到 , 而 随 着 YOLO 系 列 的 发 展 , “ 从 头 训 练 ” 的 策 略 逐 渐 取 代 
了“ImageNet pretrained”技术,这一技巧不再在YOLO中被使用。 
 
 
 6.1.3 先验框机制 
在讲解这一改进之前,我们先来了解一下什么是先验框。 
先验框的意思其实就是在每个网格处都固定放置一些大小不同的 
边界框,通常情况下,所有网格处所放置的先验框都是相同的,以便 
后续的处理。图6-2展示了先验框的实例,注意,为了便于观察,图中 
的先验框是每两个网格才绘制一次,避免绘制出的先验框过于密集, 
导致不够直观。 
 
 
 图6-2 先验框的实例 
这一机制最早是在Faster R-CNN工作(4)中提出的,用在RPN中,其 
目的是希望通过预设不同尺寸的先验框来帮助RPN更好地定位有物体 
的区域,从而生成更高质量的感兴趣区域(region of interest,RoI),以 
提升RPN的召回率。事实上,RPN的检测思想其实和YOLOv1是相似 
的,都是“逐网格找物体”,区别在于,RPN只是找哪些网格有物体 
 
 
 (只定位物体),不关注物体的类别,因为分类的任务属于第二阶 
段;而YOLOv1则是“既找也分类”,即找到物体的时候,也把它的类 
别确定下来。从时间线上来看,YOLOv1继承了RPN的这种思想,从 
发展的角度来看,YOLOv1则是将这一思想进一步发展了。 
在Faster R-CNN中,每个网格都预先被放置了 个具有不同尺寸 
和不同宽高比的先验框(这些尺寸和大小依赖人工设计)。在推理阶 
段,Faster R-CNN的RPN会为每一个先验框预测若干偏移量,包括中 
心 点 的 偏 移 量 、宽 和 高 的 偏 移 量 ,并用这些偏移量去调整每一个先 
验框,得到最终的边界框。由此可见,先验框的本质是提供边界框的 
尺寸先验,使用网络预测出来的偏移量在这些先验值上进行调整,从 
而得到最终的边界框尺寸。后来,使用先验框的目标检测网络被统一 
称为“anchor box based”方法,简称“anchor-based”方法。 
既然有anchor-based,那么自然也会有anchor-free,也就是不使用 
先验框的目标检测器。事实上,YOLOv1就是一种anchor-free检测器, 
只不过这一特性在当时并没有被广泛关注,直到后来的FCOS工作(5)被 
提出后,才引起了广泛的关注,使其成为了主流的设计理念。 
设计先验框的一个难点在于设计多少个先验框,且每个先验框的 
尺寸(宽高比和面积)又该是多少。对于宽高比,研究者们通常采用 
的配置是 、 和 ;对于面积,常用的配置是 、 、 
、 和 。依据这个配置,每个面积都可以计算出3个具有 
相同面积但不同宽和高的先验框,于是,遵循这套配置,就可以得到15 
个先验框,即 。但若不采用这一配置,我们能不能更改先验框 
的尺寸和宽高比呢?如果更改,又应该遵循什么样的原则呢?对于这 
一点,Faster R-CNN并没有给出一个较好的设计准则,而这一点也是 
YOLO作者团队在引入先验框时所面对的一个重要问题。 
 
 
 有关先验框的插曲就介绍至此,我们了解了其基本概念、基本作 
用和相关的问题。接下来,我们来介绍YOLO作者团队如何引入先验 
框并解决其中的问题。 
6.1.4 全卷积网络与先验框机制 
在上一代的YOLOv1中,有一个明显问题是网络在最后阶段使用 
了全连接层,这不仅破坏了先前的特征图所包含的空间信息结构,同 
时也引入了过多的参数。为了解决这一问题,YOLO作者团队在这一 
次改进中将其改成了全卷积结构,如图6-3所示。 
图6-3 YOLOv1的全卷积结构 
具体来说,首先移除了YOLOv1最后一个池化层和所有的全连接 
层 , 使 得 网 络 的 最 后 输 出 步 长 从 原 先 的 64 变 为 32 。 以 一 张 
的输入图像为例,经主干网络处理后,网络输出一个空 
间维度 的特征图,即相当于 的网格。随后,在这个网 
格上,作者团队又放置了 个先验框,正如Faster R-CNN所操作的那 
 
 
 样。在推理阶段,网络只需要学习能够将先验框映射到目标框的尺寸 
的偏移量,无须再学习整个目标框的尺寸信息,这使得训练变得更加 
容易。 
注意,在YOLOv1中,每个网格处会预测 个边界框,每个边界 
框都附带一个置信度预测,但是类别置信度则是共享的,即每个网格 
只会预测1个类别的物体,而不是 个。这显然有一个弊病,倘若一 
个网格包含了两个类别以上的物体,必然会出现漏检问题。而在加入 
先验框后,YOLOv1则让每一个预测的边界框都附带表示有无物体的 
置信度和类别的置信度,即网络的最终输出是 ,每个 
边界框的预测都包含1个置信度、边界框的4个位置参数和 个类别置 
信度。经过这种改进后,每个网格就最多可以检测 个类别的物体。 
尽管预测的方式略有变化,即每个网格的预测边界框都有各自的 
表示有无物体的置信度和类别置信度,但训练策略没有改变,依旧是 
从 个预测的边界框中选择出与目标框的IoU最大的边界框作为正样 
本,其表示有无物体的置信度标签还是最大的IoU,其余的边界框则是 
负样本。 
令人意外的是,在完成了以上改进后,YOLOv1的性能却并未得 
到提升,反而略有下降:mAP从69.5%降为69.2%。不过,作者团队注 
意到此时YOLOv1的召回率却从81%提升到了88%,召回率的提升意味 
着YOLOv1可以检测出更多的目标,尽管精度略有下降,但是作者团 
队并没有因精度的微小损失而放弃这一改进。 
6.1.5 使用新的主干网络